compsci 514: algorithms for data science  Andrew McGregor  Lecture 12  0  
distributional jl proof  Distributional JL Lemma: Let M ∈ Rm×d have independent N (0, 1/m) entries. , then for any y ∈ Rd , with probability at least 1 − δ  If we set m = O  2  (cid:16) log(1/δ)  (cid:17)  (1 − )(cid:107)y(cid:107)2 ≤ (cid:107)My(cid:107)2 ≤ (1 + )(cid:107)y(cid:107)2.  1  
distributional jl proof  Distributional JL Lemma: Let M ∈ Rm×d have independent N (0, 1/m) entries. , then for any y ∈ Rd , with probability at least 1 − δ  If we set m = O  2  (cid:16) log(1/δ)  (cid:17)  (1 − )(cid:107)y(cid:107)2 ≤ (cid:107)My(cid:107)2 ≤ (1 + )(cid:107)y(cid:107)2.  • Let ˜y = My and M j be the j th row of M  1  
distributional jl proof  Distributional JL Lemma: Let M ∈ Rm×d have independent N (0, 1/m) entries. , then for any y ∈ Rd , with probability at least 1 − δ  If we set m = O  2  (cid:16) log(1/δ)  (cid:17)  (1 − )(cid:107)y(cid:107)2 ≤ (cid:107)My(cid:107)2 ≤ (1 + )(cid:107)y(cid:107)2.  • Let ˜y = My and M j be the j th row of M • For any j, ˜yj = (cid:104)M j , y(cid:105).  1  
distributional jl proof  Distributional JL Lemma: Let M ∈ Rm×d have independent N (0, 1/m) entries. , then for any y ∈ Rd , with probability at least 1 − δ  If we set m = O  2  (cid:16) log(1/δ)  (cid:17)  (1 − )(cid:107)y(cid:107)2 ≤ (cid:107)My(cid:107)2 ≤ (1 + )(cid:107)y(cid:107)2.  • Let ˜y = My and M j be the j th row of M • For any j, ˜yj = (cid:104)M j , y(cid:105). Last time, we deduced that  ˜y 2 j = (y 2  1 + . . . + y 2  d )/m = (cid:107)y(cid:107)2  2/m and so E[(cid:107)˜y(cid:107)2  2] = (cid:107)y(cid:107)2  2  1  
distributional jl proof  Distributional JL Lemma: Let M ∈ Rm×d have independent N (0, 1/m) entries. , then for any y ∈ Rd , with probability at least 1 − δ  If we set m = O  2  (cid:16) log(1/δ)  (cid:17)  (1 − )(cid:107)y(cid:107)2 ≤ (cid:107)My(cid:107)2 ≤ (1 + )(cid:107)y(cid:107)2.  • Let ˜y = My and M j be the j th row of M • For any j, ˜yj = (cid:104)M j , y(cid:105). Last time, we deduced that  ˜y 2 j = (y 2  1 + . . . + y 2  d )/m = (cid:107)y(cid:107)2  2] = (cid:107)y(cid:107)2 • Main idea: ˜yj = (cid:104)M j , y(cid:105) is a weighted sum of independent random variables each with mean 0 and variance 1/m. Haven’t yet used the fact we were using Gaussians and that ˜y 2  2/m and so E[(cid:107)˜y(cid:107)2  j ∼ N (0,(cid:107)y(cid:107)2  2/m).  2  1  
distributional jl proof  Distributional JL Lemma: Let M ∈ Rm×d have independent N (0, 1/m) entries. , then for any y ∈ Rd , with probability at least 1 − δ  If we set m = O  2  (cid:16) log(1/δ)  (cid:17)  (1 − )(cid:107)y(cid:107)2 ≤ (cid:107)My(cid:107)2 ≤ (1 + )(cid:107)y(cid:107)2.  • Let ˜y = My and M j be the j th row of M • For any j, ˜yj = (cid:104)M j , y(cid:105). Last time, we deduced that  ˜y 2 j = (y 2  1 + . . . + y 2  d )/m = (cid:107)y(cid:107)2  2] = (cid:107)y(cid:107)2 • Main idea: ˜yj = (cid:104)M j , y(cid:105) is a weighted sum of independent random variables each with mean 0 and variance 1/m. Haven’t yet used the fact we were using Gaussians and that ˜y 2  2/m and so E[(cid:107)˜y(cid:107)2  j ∼ N (0,(cid:107)y(cid:107)2  2/m).  2  • Remains to show that (cid:107)˜y(cid:107)2  2 is tightly concentrated around (cid:107)y(cid:107)2 2.  1  
distributional jl proof  So Far: Each entry of our compressed vector ˜y is Gaussian with :  ˜yj ∼ N (0,(cid:107)y(cid:107)2  2  2/m) and E[(cid:107)˜y(cid:107)2  2] = (cid:107)y(cid:107)2  2  
distributional jl proof  So Far: Each entry of our compressed vector ˜y is Gaussian with :  ˜yj ∼ N (0,(cid:107)y(cid:107)2  2/m) and E[(cid:107)˜y(cid:107)2  2] = (cid:107)y(cid:107)2  2  2 =(cid:80)m  (cid:107)˜y(cid:107)2 freedom (a sum of m squared independent Gaussians)  j a Chi-Squared random variable with m degrees of  i=1 ˜y 2  2  
distributional jl proof  So Far: Each entry of our compressed vector ˜y is Gaussian with :  ˜yj ∼ N (0,(cid:107)y(cid:107)2  2/m) and E[(cid:107)˜y(cid:107)2  2] = (cid:107)y(cid:107)2  2  2 =(cid:80)m  (cid:107)˜y(cid:107)2 freedom (a sum of m squared independent Gaussians)  j a Chi-Squared random variable with m degrees of  i=1 ˜y 2  2  
distributional jl proof  So Far: Each entry of our compressed vector ˜y is Gaussian with :  ˜yj ∼ N (0,(cid:107)y(cid:107)2  2/m) and E[(cid:107)˜y(cid:107)2  2] = (cid:107)y(cid:107)2  2  2 =(cid:80)m  (cid:107)˜y(cid:107)2 freedom (a sum of m squared independent Gaussians)  j a Chi-Squared random variable with m degrees of  i=1 ˜y 2  Lemma: (Chi-Squared Concentration) Letting Z be a Chi-Squared random variable with m degrees of freedom,  Pr [|Z − EZ| ≥ EZ] ≤ 2e−m2/8.  2  
distributional jl proof  So Far: Each entry of our compressed vector ˜y is Gaussian with :  2 =(cid:80)m  ˜yj ∼ N (0,(cid:107)y(cid:107)2  2/m) and E[(cid:107)˜y(cid:107)2  2] = (cid:107)y(cid:107)2  2  (cid:107)˜y(cid:107)2 freedom (a sum of m squared independent Gaussians)  j a Chi-Squared random variable with m degrees of  i=1 ˜y 2  Lemma: (Chi-Squared Concentration) Letting Z be a Chi-Squared random variable with m degrees of freedom,  Pr [|Z − EZ| ≥ EZ] ≤ 2e−m2/8.  If we set m = O  , with probability 1 − O(e− log(1/δ)) ≥ 1 − δ:  (cid:16) log(1/δ)  (cid:17)  2  (1 − )(cid:107)y(cid:107)2  2 ≤ (cid:107)˜y(cid:107)2  2 ≤ (1 + )(cid:107)y(cid:107)2 2.  2  
distributional jl proof  So Far: Each entry of our compressed vector ˜y is Gaussian with :  2 =(cid:80)m  ˜yj ∼ N (0,(cid:107)y(cid:107)2 j a Chi-Squared random variable with m degrees of  (cid:107)˜y(cid:107)2 freedom (a sum of m squared independent Gaussians)  i=1 ˜y 2  2/m) and E[(cid:107)˜y(cid:107)2  2] = (cid:107)y(cid:107)2  2  Lemma: (Chi-Squared Concentration) Letting Z be a Chi-Squared random variable with m degrees of freedom,  Pr [|Z − EZ| ≥ EZ] ≤ 2e−m2/8.  (cid:16) log(1/δ)  (cid:17)  2  If we set m = O  , with probability 1 − O(e− log(1/δ)) ≥ 1 − δ:  (1 − )(cid:107)y(cid:107)2  2 ≤ (cid:107)˜y(cid:107)2  2 ≤ (1 + )(cid:107)y(cid:107)2 2.  Gives the distributional JL Lemma and thus the classic JL Lemma!  2  
jl lemma is essentially optimal  
orthogonal vectors  • Recall that we say two vectors x, y are orthogonal if (cid:104)x, y(cid:105) = 0.  4  
orthogonal vectors  • Recall that we say two vectors x, y are orthogonal if (cid:104)x, y(cid:105) = 0. • What is the largest set of mutually orthogonal unit vectors in  d-dimensional space?  4  
orthogonal vectors  • Recall that we say two vectors x, y are orthogonal if (cid:104)x, y(cid:105) = 0. • What is the largest set of mutually orthogonal unit vectors in  d-dimensional space? Answer: d.  4  
orthogonal vectors  • Recall that we say two vectors x, y are orthogonal if (cid:104)x, y(cid:105) = 0. • What is the largest set of mutually orthogonal unit vectors in  d-dimensional space? Answer: d.  • How large can a set of unit vectors in d-dimensional space be  that have all pairwise dot products |(cid:104)x, y(cid:105)| ≤ ?  4  
orthogonal vectors  • Recall that we say two vectors x, y are orthogonal if (cid:104)x, y(cid:105) = 0. • What is the largest set of mutually orthogonal unit vectors in  d-dimensional space? Answer: d.  • How large can a set of unit vectors in d-dimensional space be  that have all pairwise dot products |(cid:104)x, y(cid:105)| ≤ ? Answer: 2Ω(2d).  An exponentially large set of random vectors will be nearly pairwise orthogonal with high probability!  4  
orthogonal vectors proof  Claim: 2O(2d) random d-dimensional unit vectors will have all pairwise dot products |(cid:104)x, y(cid:105)| ≤  (be nearly orthogonal).  5  
orthogonal vectors proof  Claim: 2O(2d) random d-dimensional unit vectors will have all pairwise dot products |(cid:104)x, y(cid:105)| ≤  (be nearly orthogonal). Proof: Let x1, . . . , xt have independent random entries ± 1√  .  d  5  
orthogonal vectors proof  Claim: 2O(2d) random d-dimensional unit vectors will have all pairwise dot products |(cid:104)x, y(cid:105)| ≤  (be nearly orthogonal). Proof: Let x1, . . . , xt have independent random entries ± 1√ • What is (cid:107)xi(cid:107)2? Every xi is always a unit vector.  d  .  5  
orthogonal vectors proof  Claim: 2O(2d) random d-dimensional unit vectors will have all pairwise dot products |(cid:104)x, y(cid:105)| ≤  (be nearly orthogonal). Proof: Let x1, . . . , xt have independent random entries ± 1√ • What is (cid:107)xi(cid:107)2? Every xi is always a unit vector. • What is E[(cid:104)xi , xj(cid:105)]?  d  .  5  
orthogonal vectors proof  Claim: 2O(2d) random d-dimensional unit vectors will have all pairwise dot products |(cid:104)x, y(cid:105)| ≤  (be nearly orthogonal). Proof: Let x1, . . . , xt have independent random entries ± 1√ • What is (cid:107)xi(cid:107)2? Every xi is always a unit vector. • What is E[(cid:104)xi , xj(cid:105)]? E[(cid:104)xi , xj(cid:105)] = 0  d  .  5  
orthogonal vectors proof  Claim: 2O(2d) random d-dimensional unit vectors will have all pairwise dot products |(cid:104)x, y(cid:105)| ≤  (be nearly orthogonal). Proof: Let x1, . . . , xt have independent random entries ± 1√ • What is (cid:107)xi(cid:107)2? Every xi is always a unit vector. • What is E[(cid:104)xi , xj(cid:105)]? E[(cid:104)xi , xj(cid:105)] = 0 • By a Bernstein bound, Pr[|(cid:104)xi , xj(cid:105)| ≥ ] ≤ 2e−2d/6.  d  .  5  
orthogonal vectors proof  Claim: 2O(2d) random d-dimensional unit vectors will have all pairwise dot products |(cid:104)x, y(cid:105)| ≤  (be nearly orthogonal). Proof: Let x1, . . . , xt have independent random entries ± 1√ • What is (cid:107)xi(cid:107)2? Every xi is always a unit vector. • What is E[(cid:104)xi , xj(cid:105)]? E[(cid:104)xi , xj(cid:105)] = 0 • By a Bernstein bound, Pr[|(cid:104)xi , xj(cid:105)| ≥ ] ≤ 2e−2d/6. • If t = 1  2 e2d/12, using a union bound over(cid:0)t  pairs, with probability ≥ 3/4 all will be nearly orthogonal.  .  d  (cid:1) ≤ 1  2  8 e2d/6 possible  We won’t prove it but this is essentially optimal: In d dimensions, there can be at most 2O(2d) nearly orthogonal unit vectors.  5  
connection to dimensionality reduction  Recall: The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a random matrix (linear map) with m = O for x1, . . . , xn ∈ Rd with high probability, for all i, j:  (cid:16) log n  (cid:17)  2  ,  (1 − )(cid:107)xi − xj(cid:107)2  2 ≤ (cid:107)Mxi − Mxj(cid:107)2  2 ≤ (1 + )(cid:107)xi − xj(cid:107)2 2.  6  
connection to dimensionality reduction  Recall: The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a random matrix (linear map) with m = O for x1, . . . , xn ∈ Rd with high probability, for all i, j:  (cid:16) log n  (cid:17)  2  ,  (1 − )(cid:107)xi − xj(cid:107)2  2 ≤ (cid:107)Mxi − Mxj(cid:107)2  2 ≤ (1 + )(cid:107)xi − xj(cid:107)2 2.  Implies: If x1, . . . , xn are nearly orthogonal unit vectors in d-dimensions (with pairwise dot products bounded by /8), then  Mx1 (cid:107)Mx1(cid:107)2  , . . . ,  Mxn (cid:107)Mxn(cid:107)2  are nearly orthogonal unit vectors in m-dimensions (with pairwise dot products bounded by ).  6  
connection to dimensionality reduction  Recall: The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a random matrix (linear map) with m = O for x1, . . . , xn ∈ Rd with high probability, for all i, j:  (cid:16) log n  (cid:17)  2  ,  (1 − )(cid:107)xi − xj(cid:107)2  2 ≤ (cid:107)Mxi − Mxj(cid:107)2  2 ≤ (1 + )(cid:107)xi − xj(cid:107)2 2.  Implies: If x1, . . . , xn are nearly orthogonal unit vectors in d-dimensions (with pairwise dot products bounded by /8), then  Mx1 (cid:107)Mx1(cid:107)2  , . . . ,  Mxn (cid:107)Mxn(cid:107)2  are nearly orthogonal unit vectors in m-dimensions (with pairwise dot products bounded by ). Algebra is a bit messy but a good exercise to partially work through. Proof uses the fact that  (cid:107)xi − xj(cid:107)2  2 = (cid:107)xi(cid:107)2  2 + (cid:107)xj(cid:107)2  2 − 2(cid:104)xi , xj(cid:105) .  6  
connection to dimensionality reduction  (cid:16) log n  (cid:17)  Claim 1: n nearly orthogonal unit vectors can be projected to m = O  dimensions and still be nearly orthogonal.  2  Claim 2: In m dimensions, there can be at most 2O(2m) nearly orthogonal unit vectors.  7  
connection to dimensionality reduction  (cid:16) log n  (cid:17)  Claim 1: n nearly orthogonal unit vectors can be projected to m = O  dimensions and still be nearly orthogonal.  2  Claim 2: In m dimensions, there can be at most 2O(2m) nearly orthogonal unit vectors. • For both of these to hold it must be that n ≤ 2O(2m).  7  
connection to dimensionality reduction  (cid:16) log n  (cid:17)  2  Claim 1: n nearly orthogonal unit vectors can be projected to m = O  dimensions and still be nearly orthogonal.  Claim 2: In m dimensions, there can be at most 2O(2m) nearly orthogonal unit vectors. • For both of these to hold it must be that n ≤ 2O(2m). • I.e., n = 2log n ≤ 2O(2m) and so m = Ω  (cid:16) log n  (cid:17)  .  2  7  
connection to dimensionality reduction  (cid:16) log n  (cid:17)  2  Claim 1: n nearly orthogonal unit vectors can be projected to m = O  dimensions and still be nearly orthogonal.  Claim 2: In m dimensions, there can be at most 2O(2m) nearly orthogonal unit vectors. • For both of these to hold it must be that n ≤ 2O(2m). • I.e., n = 2log n ≤ 2O(2m) and so m = Ω • Tells us that the JL lemma is optimal up to constants.  (cid:16) log n  (cid:17)  2  .  7  
central limit theorem  
interpretation as a central limit theorem  Bernstein Inequality (Simpliﬁed): Consider independent random  variables X1, . . . , Xn falling in [-1,1]. Let µ = E[(cid:80) Xi ], σ2 = Var[(cid:80) Xi ], and s ≤ σ. Then: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ  ≤ 2 exp  (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  i=1  (cid:19)  .  Pr  Xi − µ  (cid:33)  (cid:18)  − s 2 4  9  
interpretation as a central limit theorem  Bernstein Inequality (Simpliﬁed): Consider independent random  variables X1, . . . , Xn falling in [-1,1]. Let µ = E[(cid:80) Xi ], σ2 = Var[(cid:80) Xi ], and s ≤ σ. Then: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ  ≤ 2 exp  (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  i=1  (cid:19)  .  Pr  Xi − µ  (cid:33)  (cid:18)  − s 2 4  Can plot this bound for diﬀerent s:  9  
interpretation as a central limit theorem  Bernstein Inequality (Simpliﬁed): Consider independent random  variables X1, . . . , Xn falling in [-1,1]. Let µ = E[(cid:80) Xi ], σ2 = Var[(cid:80) Xi ], and s ≤ σ. Then: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ  ≤ 2 exp  (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  i=1  (cid:19)  .  Pr  Xi − µ  (cid:33)  (cid:18)  − s 2 4  Can plot this bound for diﬀerent s:  9  
interpretation as a central limit theorem  Bernstein Inequality (Simpliﬁed): Consider independent random  variables X1, . . . , Xn falling in [-1,1]. Let µ = E[(cid:80) Xi ], σ2 = Var[(cid:80) Xi ], and s ≤ σ. Then: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ  ≤ 2 exp  (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  i=1  (cid:19)  .  Pr  Xi − µ  (cid:33)  (cid:18)  − s 2 4  Can plot this bound for diﬀerent s:  Looks a lot like a Gaussian (normal) distribution.  9  
interpretation as a central limit theorem  Bernstein Inequality (Simpliﬁed): Consider independent random  variables X1, . . . , Xn falling in [-1,1]. Let µ = E[(cid:80) Xi ], σ2 = Var[(cid:80) Xi ], and s ≤ σ. Then: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ  ≤ 2 exp  (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  i=1  (cid:19)  .  Pr  Xi − µ  (cid:33)  (cid:18)  − s 2 4  Can plot this bound for diﬀerent s:  Looks a lot like a Gaussian (normal) distribution.  N (0, σ2) has density p(x) = 1√  2πσ2 · e  − x2  2σ2 .  9  
interpretation as a central limit theorem  Bernstein Inequality (Simpliﬁed): Consider independent random  variables X1, . . . , Xn falling in [-1,1]. Let µ = E[(cid:80) Xi ], σ2 = Var[(cid:80) Xi ], and s ≤ σ. Then: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ  ≤ 2 exp  (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  i=1  (cid:19)  .  Pr  Xi − µ  (cid:33)  (cid:18)  − s 2 4  Can plot this bound for diﬀerent s:  Looks a lot like a Gaussian (normal) distribution.  N (0, σ2) has density p(x) = 1√  2πσ2 · e  − x2  2σ2 .  9  
gaussian tails  N (0, σ2) has density p(x) = 1√  2πσ2 · e  − x2  2σ2 .  10  
gaussian tails  N (0, σ2) has density p(x) = 1√  2πσ2 · e  − x2  2σ2 .  Exercise: Using this can show that for X ∼ N (0, σ2): for any s ≥ 0,  Pr (|X| ≥ s · σ) ≤ O(1) · e− s2 2 .  10  
gaussian tails  N (0, σ2) has density p(x) = 1√  2πσ2 · e  − x2  2σ2 .  Exercise: Using this can show that for X ∼ N (0, σ2): for any s ≥ 0,  Pr (|X| ≥ s · σ) ≤ O(1) · e− s2 2 .  Essentially the same bound that Bernstein’s inequality gives!  10  
gaussian tails  N (0, σ2) has density p(x) = 1√  2πσ2 · e  − x2  2σ2 .  Exercise: Using this can show that for X ∼ N (0, σ2): for any s ≥ 0,  Pr (|X| ≥ s · σ) ≤ O(1) · e− s2 2 .  Essentially the same bound that Bernstein’s inequality gives!  Central Limit Theorem Interpretation: Bernstein’s inequality gives a quantitative version of the CLT. The distribution of the sum of bounded independent random variables can be upper bounded with a Gaussian (normal) distribution.  10  
central limit theorem  Stronger Central Limit Theorem: The distribution of the sum of n bounded independent random variables converges to a Gaussian (normal) distribution as n goes to inﬁnity.  11  
central limit theorem  Stronger Central Limit Theorem: The distribution of the sum of n bounded independent random variables converges to a Gaussian (normal) distribution as n goes to inﬁnity.  • Why is the Gaussian distribution is so important in statistics,  science, ML, etc.?  11  
central limit theorem  Stronger Central Limit Theorem: The distribution of the sum of n bounded independent random variables converges to a Gaussian (normal) distribution as n goes to inﬁnity.  • Why is the Gaussian distribution is so important in statistics,  science, ML, etc.?  • Many random variables can be approximated as the sum of a large number of small and roughly independent random eﬀects. Thus, their distribution looks Gaussian by CLT.  11  
