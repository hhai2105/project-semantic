compsci 514: algorithms for data science  Andrew McGregor  Lecture 6  0  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, output the number of distinct elements in the stream.  1  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, output the number of distinct elements in the stream. E.g.,  1, 5, 7, 5, 2, 1 → 4 distinct elements  1  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements in the stream. E.g.,  1, 5, 7, 5, 2, 1 → 4 distinct elements  1  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements in the stream. E.g.,  1, 5, 7, 5, 2, 1 → 4 distinct elements  Applications: • Distinct IP addresses clicking on an ad or visiting a site. • Number of distinct search engine queries. • Counting distinct motifs in large DNA sequences.  1  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements in the stream. E.g.,  1, 5, 7, 5, 2, 1 → 4 distinct elements  Applications: • Distinct IP addresses clicking on an ad or visiting a site. • Number of distinct search engine queries. • Counting distinct motifs in large DNA sequences.  Google Sawzall, Facebook Presto, Apache Drill, Twitter Algebird  1  
distinct elements ideas  2  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements.  Min-Hashing for Distinct Elements (variant of Flajolet-Martin): • Let h : U → [0, 1] be a random hash function (with a real valued  output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˜d = 1  s − 1  3  
hashing for distinct elements  Min-Hashing for Distinct Elements: • Let h : U → [0, 1] be a random hash function (with a real valued output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˆd = 1  s − 1  4  
hashing for distinct elements  Min-Hashing for Distinct Elements: • Let h : U → [0, 1] be a random hash function (with a real valued output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˆd = 1  s − 1  • After all items are processed, s is the minimum of d points chosen  uniformly at random on [0, 1]. Where d = # distinct elements.  4  
hashing for distinct elements  Min-Hashing for Distinct Elements: • Let h : U → [0, 1] be a random hash function (with a real valued output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˆd = 1  s − 1  • After all items are processed, s is the minimum of d points chosen  uniformly at random on [0, 1]. Where d = # distinct elements.  • Intuition: The larger d is, the smaller we expect s to be.  4  
hashing for distinct elements  Min-Hashing for Distinct Elements: • Let h : U → [0, 1] be a random hash function (with a real valued output) • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi ))  • Return ˆd = 1  s − 1  • After all items are processed, s is the minimum of d points chosen  uniformly at random on [0, 1]. Where d = # distinct elements.  • Intuition: The larger d is, the smaller we expect s to be. • Same idea as Flajolet-Martin algorithm and HyperLogLog, except they  use discrete hash functions.  4  
performance in expectation  s is the minimum of d points chosen uniformly at random on [0, 1]. Where d = # distinct elements.  5  
performance in expectation  s is the minimum of d points chosen uniformly at random on [0, 1]. Where d = # distinct elements.  E[s] =  5  
performance in expectation  s is the minimum of d points chosen uniformly at random on [0, 1]. Where d = # distinct elements.  E[s] =  1  d + 1  (using E(s) =  (cid:90) ∞  0  Pr(s > x)dx) + calculus)  5  
performance in expectation  s is the minimum of d points chosen uniformly at random on [0, 1]. Where d = # distinct elements.  E[s] =  (using E(s) =  1  d + 1  • So estimate of(cid:98)d = 1  (cid:90) ∞  0  Pr(s > x)dx) + calculus)  s − 1 output by the algorithm is correct if s exactly  equals its expectation.  5  
performance in expectation  s is the minimum of d points chosen uniformly at random on [0, 1]. Where d = # distinct elements.  E[s] =  1  d + 1  (cid:90) ∞  (using E(s) =  Pr(s > x)dx) + calculus)  0  • So estimate of(cid:98)d = 1 equals its expectation. Does this mean E[(cid:98)d] = d?  s − 1 output by the algorithm is correct if s exactly  5  
performance in expectation  s is the minimum of d points chosen uniformly at random on [0, 1]. Where d = # distinct elements.  E[s] =  1  d + 1  (cid:90) ∞  (using E(s) =  Pr(s > x)dx) + calculus)  0  • So estimate of(cid:98)d = 1 equals its expectation. Does this mean E[(cid:98)d] = d? No, but:  s − 1 output by the algorithm is correct if s exactly  5  
performance in expectation  s is the minimum of d points chosen uniformly at random on [0, 1]. Where d = # distinct elements.  (cid:90) ∞  (using E(s) =  Pr(s > x)dx) + calculus)  E[s] =  1  d + 1  0  • So estimate of(cid:98)d = 1 equals its expectation. Does this mean E[(cid:98)d] = d? No, but:  s − 1 output by the algorithm is correct if s exactly (1 − 4)d ≤(cid:98)d ≤ (1 + 4)d  • Approximation is robust: if |s − E[s]| ≤  · E[s] for any  ∈ (0, 1/2),  .  5  
6  
initial concentration bound  So question is how well s concentrates around its mean.  E[s] =  1  d + 1  .  .  hashing algorithm. (cid:98)d = 1  s: minimum of d distinct hashes chosen randomly over [0, 1], computed by  s − 1: estimate of # distinct elements d.  7  
initial concentration bound  So question is how well s concentrates around its mean.  E[s] =  1  d + 1  and Var[s] ≤  1  (d + 1)2 (also via calculus).  .  hashing algorithm. (cid:98)d = 1  s: minimum of d distinct hashes chosen randomly over [0, 1], computed by  s − 1: estimate of # distinct elements d.  7  
initial concentration bound  So question is how well s concentrates around its mean.  E[s] =  1  d + 1  and Var[s] ≤  1  (d + 1)2 (also via calculus).  Chebyshev’s Inequality:  Pr [|s − E[s]| ≥ E[s]] ≤ Var[s] (E[s])2  .  hashing algorithm. (cid:98)d = 1  s: minimum of d distinct hashes chosen randomly over [0, 1], computed by  s − 1: estimate of # distinct elements d.  7  
initial concentration bound  So question is how well s concentrates around its mean.  E[s] =  1  d + 1  and Var[s] ≤  1  (d + 1)2 (also via calculus).  Chebyshev’s Inequality:  Pr [|s − E[s]| ≥ E[s]] ≤ Var[s]  (E[s])2 ≤ 1 2 .  hashing algorithm. (cid:98)d = 1  s: minimum of d distinct hashes chosen randomly over [0, 1], computed by  s − 1: estimate of # distinct elements d.  7  
initial concentration bound  So question is how well s concentrates around its mean.  E[s] =  1  d + 1  and Var[s] ≤  1  (d + 1)2 (also via calculus).  Chebyshev’s Inequality:  Pr [|s − E[s]| ≥ E[s]] ≤ Var[s]  (E[s])2 ≤ 1 2 .  Bound is vacuous for any  < 1.  hashing algorithm. (cid:98)d = 1  s: minimum of d distinct hashes chosen randomly over [0, 1], computed by  s − 1: estimate of # distinct elements d.  7  
initial concentration bound  So question is how well s concentrates around its mean.  E[s] =  1  d + 1  and Var[s] ≤  1  (d + 1)2 (also via calculus).  Chebyshev’s Inequality:  Pr [|s − E[s]| ≥ E[s]] ≤ Var[s]  (E[s])2 ≤ 1 2 .  Bound is vacuous for any  < 1. How can we improve accuracy?  hashing algorithm. (cid:98)d = 1  s: minimum of d distinct hashes chosen randomly over [0, 1], computed by  s − 1: estimate of # distinct elements d.  7  
improving performance  Leverage the law of large numbers: improve accuracy via repeated independent trials.  8  
improving performance  Leverage the law of large numbers: improve accuracy via repeated independent trials.  Hashing for Distinct Elements (Improved): • Let h : U → [0, 1] be a random hash function • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi )) • Return(cid:98)d = 1 s − 1  8  
improving performance  Leverage the law of large numbers: improve accuracy via repeated independent trials.  Hashing for Distinct Elements (Improved): • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions • s := 1 • For i = 1, . . . , n  • s := min(s, h(xi )) • Return(cid:98)d = 1 s − 1  8  
improving performance  Leverage the law of large numbers: improve accuracy via repeated independent trials.  Hashing for Distinct Elements (Improved): • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions • s1, s2, . . . , sk := 1 • For i = 1, . . . , n  • s := min(s, h(xi )) • Return(cid:98)d = 1 s − 1  8  
improving performance  Leverage the law of large numbers: improve accuracy via repeated independent trials.  Hashing for Distinct Elements (Improved): • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions • s1, s2, . . . , sk := 1 • For i = 1, . . . , n  • For j=1,. . . ,k, sj := min(sj , hj (xi )) • Return(cid:98)d = 1 s − 1  8  
improving performance  Leverage the law of large numbers: improve accuracy via repeated independent trials.  Hashing for Distinct Elements (Improved): • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions • s1, s2, . . . , sk := 1 • For i = 1, . . . , n  • s := 1  (cid:80)k • For j=1,. . . ,k, sj := min(sj , hj (xi )) • Return(cid:98)d = 1 s − 1  j=1 sj  k  8  
improving performance  Leverage the law of large numbers: improve accuracy via repeated independent trials.  Hashing for Distinct Elements (Improved): • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions • s1, s2, . . . , sk := 1 • For i = 1, . . . , n  • s := 1  (cid:80)k • For j=1,. . . ,k, sj := min(sj , hj (xi )) • Return(cid:98)d = 1 s − 1  j=1 sj  k  8  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  1  d + 1 1  (d + 1)2  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  =⇒ E[s]  1  d + 1 1  (d + 1)2  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  =⇒ E[s] =  1  d + 1  1  d + 1 1  (d + 1)2  (linearity of expectation)  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  1  =⇒ E[s] = (d + 1)2 =⇒ Var[s]  d + 1 1  1  d + 1  (linearity of expectation)  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  1  d + 1 1  =⇒ E[s] =  1  d + 1  (linearity of expectation)  (d + 1)2 =⇒ Var[s] ≤  1  k · (d + 1)2 (linearity of variance)  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  1  d + 1 1  =⇒ E[s] =  1  d + 1  (linearity of expectation)  (d + 1)2 =⇒ Var[s] ≤  1  k · (d + 1)2 (linearity of variance)  Chebyshev Inequality:  Pr [|s − E[s]| ≥ E[s]] ≤ Var[s] (E[s])2  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  1  d + 1 1  =⇒ E[s] =  1  d + 1  (linearity of expectation)  (d + 1)2 =⇒ Var[s] ≤  1  k · (d + 1)2 (linearity of variance)  Chebyshev Inequality:  Pr [|s − E[s]| ≥ E[s]] ≤ Var[s]  (E[s])2 =  E[s]2/k 2E[s]2 =  1 k · 2  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  1  d + 1 1  =⇒ E[s] =  1  d + 1  (linearity of expectation)  1  k · (d + 1)2 (linearity of variance)  Chebyshev Inequality:  (d + 1)2 =⇒ Var[s] ≤ (cid:105) ≤ Var[s]  (cid:12)(cid:12)(cid:12) ≥ 4 · d  (cid:104)(cid:12)(cid:12)(cid:12)d −(cid:98)d  (E[s])2 =  Pr  E[s]2/k 2E[s]2 =  1 k · 2  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  E[sj ] = Var[sj ] ≤  1  d + 1 1  =⇒ E[s] =  1  d + 1  (linearity of expectation)  1  k · (d + 1)2 (linearity of variance)  Chebyshev Inequality:  (d + 1)2 =⇒ Var[s] ≤ (cid:105) ≤ Var[s]  (cid:12)(cid:12)(cid:12) ≥ 4 · d  (cid:104)(cid:12)(cid:12)(cid:12)d −(cid:98)d  (E[s])2 =  Pr  E[s]2/k 2E[s]2 =  1 k · 2  How should we set k if we want 4 · d error with probability ≥ 1 − δ?  (cid:98)d = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  s − 1: estimate of # distinct elements d.  (cid:80)k  j=1 sj .  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  =⇒ E[s] =  1  d + 1  (linearity of expectation)  1  k · (d + 1)2 (linearity of variance)  E[sj ] = Var[sj ] ≤  1  d + 1 1  Chebyshev Inequality:  (d + 1)2 =⇒ Var[s] ≤ (cid:105) ≤ Var[s]  (cid:12)(cid:12)(cid:12) ≥ 4 · d  (cid:104)(cid:12)(cid:12)(cid:12)d −(cid:98)d  (E[s])2 =  E[s]2/k 2E[s]2 =  1 k · 2  Pr  2·δ .  (cid:98)d = 1  How should we set k if we want 4 · d error with probability ≥ 1 − δ? k = 1  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  j=1 sj .  s − 1: estimate of # distinct elements d.  (cid:80)k  9  
analysis  (cid:80)k  j=1 sj . Have already shown that for j = 1, . . . , k:  s = 1 k  =⇒ E[s] =  1  d + 1  (linearity of expectation)  1  k · (d + 1)2 (linearity of variance)  E[sj ] = Var[sj ] ≤  1  d + 1 1  Chebyshev Inequality:  (d + 1)2 =⇒ Var[s] ≤ (cid:105) ≤ Var[s]  (cid:12)(cid:12)(cid:12) ≥ 4 · d  (cid:104)(cid:12)(cid:12)(cid:12)d −(cid:98)d  (cid:80)k  9  Pr  2 · δ 2 = δ. How should we set k if we want 4 · d error with probability ≥ 1 − δ? k = 1  E[s]2/k 2E[s]2 =  1 k · 2 =  (E[s])2 =  sj : minimum of d distinct hashes chosen randomly over [0, 1]. s = 1 k  j=1 sj .  s − 1: estimate of # distinct elements d.  2·δ .  (cid:98)d = 1  
space complexity  Hashing for Distinct Elements: • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions • s1, s2, . . . , sk := 1 • For i = 1, . . . , n (cid:80)k • For j=1,. . . , k, sj := min(sj , hj (xi )) • s := 1 • Return(cid:98)d = 1  j=1 sj  s − 1  k  2·δ , algorithm returns(cid:98)d with |d −(cid:98)d| ≤ 4 · d with  • Setting k = 1  probability at least 1 − δ.  10  
space complexity  Hashing for Distinct Elements: • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions • s1, s2, . . . , sk := 1 • For i = 1, . . . , n (cid:80)k • For j=1,. . . , k, sj := min(sj , hj (xi )) • s := 1 • Return(cid:98)d = 1  j=1 sj  s − 1  k  2·δ , algorithm returns(cid:98)d with |d −(cid:98)d| ≤ 4 · d with  • Setting k = 1  probability at least 1 − δ. • Space complexity is k = 1  2·δ real numbers s1, . . . , sk .  10  
space complexity  Hashing for Distinct Elements: • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions • s1, s2, . . . , sk := 1 • For i = 1, . . . , n (cid:80)k • For j=1,. . . , k, sj := min(sj , hj (xi )) • s := 1 • Return(cid:98)d = 1  j=1 sj  k  s − 1  • Setting k = 1  2·δ , algorithm returns(cid:98)d with |d −(cid:98)d| ≤ 4 · d with  probability at least 1 − δ. • Space complexity is k = 1 • δ = 5% failure rate gives a factor 20 overhead in space complexity.  2·δ real numbers s1, . . . , sk .  10  
improved failure rate  How can we improve our dependence on the failure rate δ?  11  
improved failure rate  How can we improve our dependence on the failure rate δ?  The median trick: Run t = O(log 1/δ) trials each with failure probability δ(cid:48) = 1/4 – each using k = 1 2 hash functions.  δ(cid:48)2 = 4  11  
improved failure rate  How can we improve our dependence on the failure rate δ?  The median trick: Run t = O(log 1/δ) trials each with failure probability δ(cid:48) = 1/4 – each using k = 1 2 hash functions.  • Letting(cid:98)d1, . . . ,(cid:98)dt be the outcomes of the t trials, return  δ(cid:48)2 = 4  (cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt) .  11  
improved failure rate  How can we improve our dependence on the failure rate δ?  The median trick: Run t = O(log 1/δ) trials each with failure probability δ(cid:48) = 1/4 – each using k = 1 2 hash functions.  • Letting(cid:98)d1, . . . ,(cid:98)dt be the outcomes of the t trials, return  δ(cid:48)2 = 4  (cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt) .  11  
improved failure rate  How can we improve our dependence on the failure rate δ?  The median trick: Run t = O(log 1/δ) trials each with failure probability δ(cid:48) = 1/4 – each using k = 1 2 hash functions.  • Letting(cid:98)d1, . . . ,(cid:98)dt be the outcomes of the t trials, return  δ(cid:48)2 = 4  (cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt) .  • If > 1/2 of trials fall in [(1 − 4)d, (1 + 4)d], then the median will.  11  
the median trick  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]?  [(1 − 4)d, (1 + 4)d]  12  
the median trick  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]?  [(1 − 4)d, (1 + 4)d]  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d].  .  12  
the median trick  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]?  [(1 − 4)d, (1 + 4)d]  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d].  (cid:16)(cid:98)d /∈ [(1 − 4)d, (1 + 4)d]  (cid:17) ≤ Pr  Pr  (cid:19)  (cid:18)  X ≤ 1 2  · t  .  12  
the median trick  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]?  [(1 − 4)d, (1 + 4)d]  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d]. E[X] ≥  .  12  (cid:16)(cid:98)d /∈ [(1 − 4)d, (1 + 4)d]  (cid:17) ≤ Pr  Pr  (cid:19)  (cid:18)  X ≤ 1 2  · t  
the median trick  [(1 − 4)d, (1 + 4)d]  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]? 4 · t.  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d]. E[X] ≥ 3  (cid:16)(cid:98)d /∈ [(1 − 4)d, (1 + 4)d]  (cid:17) ≤ Pr  Pr  (cid:19)  (cid:18)  X ≤ 1 2  · t  12  
the median trick  [(1 − 4)d, (1 + 4)d]  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]? 4 · t.  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d]. E[X] ≥ 3  (cid:16)(cid:98)d /∈ [(1 − 4)d, (1 + 4)d]  (cid:17) ≤ Pr  Pr  (cid:19)  (cid:18)  X ≤ 1 2  · t  12  
the median trick  [(1 − 4)d, (1 + 4)d]  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]? 4 · t. (cid:19)  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d]. E[X] ≥ 3  (cid:16)(cid:98)d /∈ [(1 − 4)d, (1 + 4)d]  (cid:17) ≤ Pr  ≤ Pr  (cid:18)  (cid:19)  (cid:18)  X ≤ 1 2  · t  |X − E[X]| ≥ 1 4  t  Pr  12  
the median trick  [(1 − 4)d, (1 + 4)d]  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]? 4 · t. (cid:19)  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d]. E[X] ≥ 3  (cid:16)(cid:98)d /∈ [(1 − 4)d, (1 + 4)d]  (cid:17) ≤ Pr  ≤ Pr  (cid:18)  (cid:19)  (cid:18)  X ≤ 1 2  · t  |X − E[X]| ≥ 1 4  t  Pr  Apply Chernoﬀ bound:  12  
the median trick  [(1 − 4)d, (1 + 4)d]  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]? 4 · t. (cid:19)  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d]. E[X] ≥ 3  (cid:16)(cid:98)d /∈ [(1 − 4)d, (1 + 4)d]  ≤ Pr  (cid:19)  (cid:18)  |X − E[X]| ≥ 1 4  t  Pr  (cid:18)  Apply Chernoﬀ bound: |X − E[X]| ≥ 1 3  Pr  (cid:18) (cid:17) ≤ Pr (cid:19)  E[X]  X ≤ 1 2  · t  ≤ 2 exp  (cid:33)  (cid:32) 2 · 3 − 1 4 t 3 2 + 1/3  = e−Θ(t).  12  
the median trick  [(1 − 4)d, (1 + 4)d]  • (cid:98)d1, . . . ,(cid:98)dt are the outcomes of the t trials, each falling in with probability at least 3/4. Let(cid:98)d = median((cid:98)d1, . . . ,(cid:98)dt). What is the probability that the median(cid:98)d falls in [(1 − 4)d, (1 + 4)d]? 4 · t. (cid:19)  • Let X be the # of trials falling in [(1 − 4)d, (1 + 4)d]. E[X] ≥ 3  (cid:16)(cid:98)d /∈ [(1 − 4)d, (1 + 4)d]  ≤ Pr  (cid:19)  (cid:18)  |X − E[X]| ≥ 1 4  t  X ≤ 1 2  · t  Pr  (cid:18) (cid:17) ≤ Pr (cid:19)  E[X]  (cid:18)  Apply Chernoﬀ bound: |X − E[X]| ≥ 1 3  Pr  (cid:33)  (cid:32) 2 · 3 − 1 4 t 3 2 + 1/3  ≤ 2 exp  = e−Θ(t).  • Setting t = O(log(1/δ)) gives failure probability e− log(1/δ) = δ.  12  
median trick  Upshot: The median of t = O(log(1/δ)) independent runs of the hashing algorithm for distinct elements returns  (cid:98)d ∈ [(1 − 4)d, (1 + 4)d]  with probability at least 1 − δ.  13  
median trick  Upshot: The median of t = O(log(1/δ)) independent runs of the hashing algorithm for distinct elements returns  (cid:98)d ∈ [(1 − 4)d, (1 + 4)d] (cid:16) log(1/δ)  with probability at least 1 − δ. Total Space Complexity: t trials, each using k = 1 functions, for δ(cid:48) = 1/4. Space is 4t (the minimum value of each hash function).  (cid:17)  2 = O  2  2δ(cid:48) hash real numbers  13  
median trick  Upshot: The median of t = O(log(1/δ)) independent runs of the hashing algorithm for distinct elements returns  (cid:98)d ∈ [(1 − 4)d, (1 + 4)d] (cid:16) log(1/δ)  with probability at least 1 − δ. Total Space Complexity: t trials, each using k = 1 functions, for δ(cid:48) = 1/4. Space is 4t (the minimum value of each hash function).  (cid:17)  2 = O  2  2δ(cid:48) hash real numbers  No dependence on the number of distinct elements d or the number of items in the stream n! Both can be very large.  13  
median trick  Upshot: The median of t = O(log(1/δ)) independent runs of the hashing algorithm for distinct elements returns  (cid:98)d ∈ [(1 − 4)d, (1 + 4)d] (cid:16) log(1/δ)  with probability at least 1 − δ. Total Space Complexity: t trials, each using k = 1 functions, for δ(cid:48) = 1/4. Space is 4t (the minimum value of each hash function).  (cid:17)  2 = O  2  2δ(cid:48) hash real numbers  No dependence on the number of distinct elements d or the number of items in the stream n! Both can be very large.  A note on the median: The median is often used as a robust alternative to the mean, when there are outliers (e.g., heavy tailed distributions, corrupted data).  13  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions.  14  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented...  14  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  14  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  • Flajolet-Martin (LogLog) algorithm and HyperLogLog.  14  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  • Flajolet-Martin (LogLog) algorithm and HyperLogLog.  14  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  • Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  14  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  • Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m. The more distinct hashes we see, the higher we expect this maximum to be.  14  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  a) O(1)  b) O(log d)  √ c) O(  d)  d) O(d)  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  a) O(1)  b) O(log d)  √ c) O(  d)  d) O(d)  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has x trailing zeros) =  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has x trailing zeros) =  1 2x  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1  2log d  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  So with d distinct hashes, expect to see 1 with log d trailing zeros. Expect m ≈ log d.  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  So with d distinct hashes, expect to see 1 with log d trailing zeros. Expect m ≈ log d. m takes log log d bits to store.  15  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  So with d distinct hashes, expect to see 1 with log d trailing zeros. Expect m ≈ log d. m takes log log d bits to store.  for an  approximate count.  15  (cid:16) log log d  (cid:17)  Total Space: O  2 + log d  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  So with d distinct hashes, expect to see 1 with log d trailing zeros. Expect m ≈ log d. m takes log log d bits to store.  (cid:16) log log d  (cid:17)  Total Space: O  2 + log d  for an  approximate count.  Note: Careful averaging of estimates from multiple hash functions.  15  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  space used = O  + log d  (cid:18) log log d  2  (cid:19)  16  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  + log d 1.04 · (cid:100)log2 log2 d(cid:101)  2  =  2  + (cid:100)log2 d(cid:101) bits1  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  16  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  16  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  Mergeable Sketch: Consider the case (essentially always in practice) that the items are processed on diﬀerent machines.  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  16  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  Mergeable Sketch: Consider the case (essentially always in practice) that the items are processed on diﬀerent machines. • Given data structures (sketches) HLL(x1, . . . , xn), HLL(y1, . . . , yn) it is  easy to merge them to give HLL(x1, . . . , xn, y1, . . . , yn).  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  16  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  Mergeable Sketch: Consider the case (essentially always in practice) that the items are processed on diﬀerent machines. • Given data structures (sketches) HLL(x1, . . . , xn), HLL(y1, . . . , yn) it is  easy to merge them to give HLL(x1, . . . , xn, y1, . . . , yn). How?  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  16  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  Mergeable Sketch: Consider the case (essentially always in practice) that the items are processed on diﬀerent machines. • Given data structures (sketches) HLL(x1, . . . , xn), HLL(y1, . . . , yn) it is  easy to merge them to give HLL(x1, . . . , xn, y1, . . . , yn). How?  • Set the maximum # of trailing zeros to the maximum in the two  sketches.  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  16  
