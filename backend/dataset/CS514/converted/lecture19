compsci 514: algorithms for data science  Andrew McGregor  Lecture 19  0  
summary  Last Class: Spectral Clustering • Spectral clustering: ﬁnding good cuts via Laplacian eigenvectors.  1  
summary  Last Class: Spectral Clustering • Spectral clustering: ﬁnding good cuts via Laplacian eigenvectors.  This Class: Stochastic Block Model • Stochastic block model: A simple clustered graph model where we can  prove the eﬀectiveness of spectral clustering.  • Prove that clustering with the Laplacian eigenvectors (spectral  clustering) ﬁnds communities in the stochastic block model.  1  
review  For a graph with adjacency matrix A and degree matrix D, L = D − A is the graph Laplacian.  How smooth any vector (cid:126)v is over the graph can be measured by:  ((cid:126)v (i) − (cid:126)v (j))2 = (cid:126)v T L(cid:126)v .  (cid:88)  (i,j)∈E  • We’ll use eigenvectors of Laplacian to divide the nodes of the graph into roughly equal groups such that the number of cut edges is small.  2  
cutting with the second laplacian eigenvector  Find a good partition of the graph by computing  (cid:126)vn−1 =  (cid:126)v T L(cid:126)v  arg min  v∈Rd with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T(cid:126)1=0  Let S be nodes with (cid:126)vn−1(i) < 0, T be nodes with (cid:126)vn−1(i) ≥ 0.  3  
cutting with the second laplacian eigenvector  Find a good partition of the graph by computing  (cid:126)vn−1 =  (cid:126)v T L(cid:126)v  arg min  v∈Rd with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T(cid:126)1=0  Let S be nodes with (cid:126)vn−1(i) < 0, T be nodes with (cid:126)vn−1(i) ≥ 0.  3  
the laplacian view  For a cut indicator vector (cid:126)v ∈ {−1, 1}n with (cid:126)v (i) = −1 for i ∈ S and (cid:126)v (i) = 1 for i ∈ T :  (i,j)∈E ((cid:126)v (i) − (cid:126)v (j))2 = 4 · cut(S, T ).  1. (cid:126)v T L(cid:126)v =(cid:80)  4  
the laplacian view  For a cut indicator vector (cid:126)v ∈ {−1, 1}n with (cid:126)v (i) = −1 for i ∈ S and (cid:126)v (i) = 1 for i ∈ T :  (i,j)∈E ((cid:126)v (i) − (cid:126)v (j))2 = 4 · cut(S, T ).  1. (cid:126)v T L(cid:126)v =(cid:80)  2. (cid:126)v T(cid:126)1 = |T| − |S|.  4  
the laplacian view  For a cut indicator vector (cid:126)v ∈ {−1, 1}n with (cid:126)v (i) = −1 for i ∈ S and (cid:126)v (i) = 1 for i ∈ T :  (i,j)∈E ((cid:126)v (i) − (cid:126)v (j))2 = 4 · cut(S, T ).  1. (cid:126)v T L(cid:126)v =(cid:80)  2. (cid:126)v T(cid:126)1 = |T| − |S|.  Want to minimize both (cid:126)v T L(cid:126)v (cut size) and (cid:126)v T(cid:126)1 (imbalance).  4  
the laplacian view  For a cut indicator vector (cid:126)v ∈ {−1, 1}n with (cid:126)v (i) = −1 for i ∈ S and (cid:126)v (i) = 1 for i ∈ T :  (i,j)∈E ((cid:126)v (i) − (cid:126)v (j))2 = 4 · cut(S, T ).  1. (cid:126)v T L(cid:126)v =(cid:80)  2. (cid:126)v T(cid:126)1 = |T| − |S|.  Want to minimize both (cid:126)v T L(cid:126)v (cut size) and (cid:126)v T(cid:126)1 (imbalance).  Next Step: See how this dual minimization problem is naturally solved by eigendecomposition.  4  
smallest laplacian eigenvector  The smallest eigenvector of the Laplacian is:  (cid:126)v1 =  · (cid:126)1 =  1√ n  arg min  v∈Rn with (cid:107)(cid:126)v(cid:107)=1  (cid:126)v T L(cid:126)v  with eigenvalue (cid:126)v T  1 L(cid:126)v1 = 0.  n: number of nodes in graph, A ∈ Rn×n: adjacency matrix, D ∈ Rn×n: diagonal degree matrix, L ∈ Rn×n: Laplacian matrix L = A − D.  5  
smallest laplacian eigenvector  The smallest eigenvector of the Laplacian is:  (cid:126)v1 =  · (cid:126)1 =  1√ n  arg min  v∈Rn with (cid:107)(cid:126)v(cid:107)=1  (cid:126)v T L(cid:126)v  with eigenvalue (cid:126)v T  1 L(cid:126)v1 = 0. Why?  n: number of nodes in graph, A ∈ Rn×n: adjacency matrix, D ∈ Rn×n: diagonal degree matrix, L ∈ Rn×n: Laplacian matrix L = A − D.  5  
second smallest laplacian eigenvector  By Courant-Fischer, the second smallest eigenvector is given by:  (cid:126)v2 =  arg min  v∈Rn with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T  1 (cid:126)v =0  (cid:126)v T L(cid:126)v  6  
second smallest laplacian eigenvector  By Courant-Fischer, the second smallest eigenvector is given by:  (cid:126)v2 =  (cid:110)− 1√  n , 1√  n  (cid:111)n  If (cid:126)v2 were in • (cid:126)v T  2 L(cid:126)v2 = 4  arg min  v∈Rn with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T  1 (cid:126)v =0  (cid:126)v T L(cid:126)v  it would have:  n · cut(S, T ) as small as possible subject to  (cid:126)v T 2 (cid:126)v1 =  1√ n  (cid:126)v T 2  (cid:126)1 =  |T| − |S|√  n  = 0  6  
second smallest laplacian eigenvector  By Courant-Fischer, the second smallest eigenvector is given by:  (cid:126)v2 =  (cid:110)− 1√  n , 1√  n  (cid:111)n  arg min  v∈Rn with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T  1 (cid:126)v =0  (cid:126)v T L(cid:126)v  it would have:  If (cid:126)v2 were in • (cid:126)v T  2 L(cid:126)v2 = 4  n · cut(S, T ) as small as possible subject to  (cid:126)v T 2 (cid:126)v1 =  1√ n  (cid:126)v T 2  (cid:126)1 =  |T| − |S|√  n  = 0  • I.e., (cid:126)v2 would indicate the smallest perfectly balanced cut.  6  
second smallest laplacian eigenvector  By Courant-Fischer, the second smallest eigenvector is given by:  (cid:126)v2 =  (cid:110)− 1√  n , 1√  n  (cid:111)n  arg min  v∈Rn with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T  1 (cid:126)v =0  (cid:126)v T L(cid:126)v  it would have:  If (cid:126)v2 were in • (cid:126)v T  2 L(cid:126)v2 = 4  n · cut(S, T ) as small as possible subject to  (cid:126)v T 2 (cid:126)v1 =  1√ n  (cid:126)v T 2  (cid:126)1 =  |T| − |S|√  n  = 0  • I.e., (cid:126)v2 would indicate the smallest perfectly balanced cut. • The eigenvector (cid:126)v2 ∈ Rn is not generally binary, but still satisﬁes  a ‘relaxed’ version of this property.  6  
cutting with the second laplacian eigenvector  Find a good partition of the graph by computing  (cid:126)v2 =  arg min  (cid:126)v T L(cid:126)v  Set S to be all nodes with (cid:126)v2(i) < 0, T to be all with (cid:126)v2(i) ≥ 0.  v∈Rd with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T  (cid:126)1=0  2  7  
cutting with the second laplacian eigenvector  Find a good partition of the graph by computing  (cid:126)v2 =  arg min  (cid:126)v T L(cid:126)v  Set S to be all nodes with (cid:126)v2(i) < 0, T to be all with (cid:126)v2(i) ≥ 0.  v∈Rd with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T  (cid:126)1=0  2  7  
cutting with the second laplacian eigenvector  Find a good partition of the graph by computing  (cid:126)v2 =  arg min  (cid:126)v T L(cid:126)v  Set S to be all nodes with (cid:126)v2(i) < 0, T to be all with (cid:126)v2(i) ≥ 0.  v∈Rd with (cid:107)(cid:126)v(cid:107)=1, (cid:126)v T  (cid:126)1=0  2  7  
spectral clustering  A very common task is to partition or cluster vertices in a graph based on similarity/connectivity.  8  
spectral clustering  A very common task is to partition or cluster vertices in a graph based on similarity/connectivity.  8  
spectral clustering  A very common task is to partition or cluster vertices in a graph based on similarity/connectivity.  8  
spectral clustering  A very common task is to partition or cluster vertices in a graph based on similarity/connectivity.  Linearly separable data.  8  
spectral clustering  A very common task is to partition or cluster vertices in a graph based on similarity/connectivity.  Non-linearly separable data k-nearest neighbor graph.  8  
spectral clustering  A very common task is to partition or cluster vertices in a graph based on similarity/connectivity.  Non-linearly separable data k-nearest neighbor graph.  8  
spectral clustering  A very common task is to partition or cluster vertices in a graph based on similarity/connectivity.  Non-linearly separable data k-nearest neighbor graph.  8  
spectral clustering  A very common task is to partition or cluster vertices in a graph based on similarity/connectivity.  Non-linearly separable data k-nearest neighbor graph.  Can ﬁnd this cut using eigendecomposition!  8  
spectral partitioning in practice  The Shi-Malik normalized cuts algorithm is one of the most commonly used variants of this approach, using the normalized Laplacian L = D−1/2LD−1/2.  n: number of nodes in graph, A ∈ Rn×n: adjacency matrix, D ∈ Rn×n: diagonal degree matrix, L ∈ Rn×n: Laplacian matrix L = A − D.  9  
spectral partitioning in practice  The Shi-Malik normalized cuts algorithm is one of the most commonly used variants of this approach, using the normalized Laplacian L = D−1/2LD−1/2.  Important Consideration: What to do when we want to split the graph into more than two parts?  n: number of nodes in graph, A ∈ Rn×n: adjacency matrix, D ∈ Rn×n: diagonal degree matrix, L ∈ Rn×n: Laplacian matrix L = A − D.  9  
spectral partitioning in practice  The Shi-Malik normalized cuts algorithm is one of the most commonly used variants of this approach, using the normalized Laplacian L = D−1/2LD−1/2.  Important Consideration: What to do when we want to split the graph into more than two parts?  Spectral Clustering:  n: number of nodes in graph, A ∈ Rn×n: adjacency matrix, D ∈ Rn×n: diagonal degree matrix, L ∈ Rn×n: Laplacian matrix L = A − D.  9  
spectral partitioning in practice  The Shi-Malik normalized cuts algorithm is one of the most commonly used variants of this approach, using the normalized Laplacian L = D−1/2LD−1/2.  Important Consideration: What to do when we want to split the graph into more than two parts?  Spectral Clustering: • Compute smallest t nonzero eigenvectors (cid:126)v2, . . . , (cid:126)vt+1 of L.  n: number of nodes in graph, A ∈ Rn×n: adjacency matrix, D ∈ Rn×n: diagonal degree matrix, L ∈ Rn×n: Laplacian matrix L = A − D.  9  
spectral partitioning in practice  The Shi-Malik normalized cuts algorithm is one of the most commonly used variants of this approach, using the normalized Laplacian L = D−1/2LD−1/2.  Important Consideration: What to do when we want to split the graph into more than two parts?  Spectral Clustering: • Compute smallest t nonzero eigenvectors (cid:126)v2, . . . , (cid:126)vt+1 of L. • Represent each node by its corresponding row in V ∈ Rn×t  whose columns are (cid:126)v2, . . . (cid:126)vt+1.  n: number of nodes in graph, A ∈ Rn×n: adjacency matrix, D ∈ Rn×n: diagonal degree matrix, L ∈ Rn×n: Laplacian matrix L = A − D.  9  
spectral partitioning in practice  The Shi-Malik normalized cuts algorithm is one of the most commonly used variants of this approach, using the normalized Laplacian L = D−1/2LD−1/2.  Important Consideration: What to do when we want to split the graph into more than two parts?  Spectral Clustering: • Compute smallest t nonzero eigenvectors (cid:126)v2, . . . , (cid:126)vt+1 of L. • Represent each node by its corresponding row in V ∈ Rn×t  whose columns are (cid:126)v2, . . . (cid:126)vt+1.  • Cluster these rows using k-means clustering (or really any  clustering method).  n: number of nodes in graph, A ∈ Rn×n: adjacency matrix, D ∈ Rn×n: diagonal degree matrix, L ∈ Rn×n: Laplacian matrix L = A − D.  9  
generative models  So Far: Argued that spectral clustering partitions a graph, along a small cut that separates the graph into large pieces.  10  
generative models  So Far: Argued that spectral clustering partitions a graph, along a small cut that separates the graph into large pieces. • Haven’t given formal guarantees on ‘quality’ of the partitioning.  10  
generative models  So Far: Argued that spectral clustering partitions a graph, along a small cut that separates the graph into large pieces. • Haven’t given formal guarantees on ‘quality’ of the partitioning. • This is diﬃcult to do for general input graphs.  10  
generative models  So Far: Argued that spectral clustering partitions a graph, along a small cut that separates the graph into large pieces. • Haven’t given formal guarantees on ‘quality’ of the partitioning. • This is diﬃcult to do for general input graphs.  Common Approach: Give a natural generative model for random inputs and analyze how the algorithm performs on inputs drawn from this model.  10  
generative models  So Far: Argued that spectral clustering partitions a graph, along a small cut that separates the graph into large pieces. • Haven’t given formal guarantees on ‘quality’ of the partitioning. • This is diﬃcult to do for general input graphs.  Common Approach: Give a natural generative model for random inputs and analyze how the algorithm performs on inputs drawn from this model. • Very common in algorithm design for data analysis/machine learning (can be used to justify (cid:96)2 linear regression, k-means clustering, PCA, etc.)  10  
stochastic block model  Stochastic Block Model (Planted Partition Model): Let Gn(p, q) be a distribution over graphs on n nodes, split randomly into two groups B and C , each with n/2 nodes. • Any two nodes in the same group are connected with probability p  (including self-loops).  • Any two nodes in diﬀerent groups are connected with prob. q < p. • Connections are independent.  11  
linear algebraic view  Let G be a stochastic block model graph drawn from Gn(p, q). • Let A ∈ Rn×n be the adjacency matrix of G , ordered in terms of  group ID.  Gn(p, q): stochastic block model distribution. B, C : groups with n/2 nodes each. Connections are independent with probability p between nodes in the same group, and probability q between nodes not in the same group.  12  
expected adjacency spectrum  Letting G be a stochastic block model graph drawn from Gn(p, q) and A ∈ Rn×n be its adjacency matrix. (E[A])i,j = p for i, j in same group, (E[A])i,j = q otherwise.  Gn(p, q): stochastic block model distribution. B, C : groups with n/2 nodes each. Connections are independent with probability p between nodes in the same group, and probability q between nodes not in the same group.  13  
expected adjacency spectrum  Letting G be a stochastic block model graph drawn from Gn(p, q) and A ∈ Rn×n be its adjacency matrix. (E[A])i,j = p for i, j in same group, (E[A])i,j = q otherwise.  What is rank(E[A])? What are the eigenvectors and eigenvalues of E[A]?  Gn(p, q): stochastic block model distribution. B, C : groups with n/2 nodes each. Connections are independent with probability p between nodes in the same group, and probability q between nodes not in the same group.  13  
expected adjacency spectrum  If we compute (cid:126)v2 then we recover the communities B and C !  14  
expected adjacency spectrum  If we compute (cid:126)v2 then we recover the communities B and C ! • Can show that for G ∼ Gn(p, q), A is “close” to E[A] in some • Second eigenvector of A is close to [1, 1, 1, . . . ,−1,−1,−1] and  appropriate sense (matrix concentration inequality).  gives a good estimate of the communities.  When the rows/columns aren’t sorted by community ID, the second eigenvector is something like [1,−1, 1,−1, . . . , 1, 1,−1] and the entries give community ids.  14  
