compsci 514: algorithms for data science  Andrew McGregor  Lecture 15  0  
last class: embedding with assumptions  Set Up: Assume that data points (cid:126)x1, . . . , (cid:126)xn ∈ Rd lie in some k-dimensional subspace V of Rd .  Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns.  (cid:107)VT (cid:126)xi − VT (cid:126)xj(cid:107)2  2 = (cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 2.  Letting ˜xi = VT (cid:126)xi , we have a perfect embedding from V into Rk .  1  
last class: projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = XVVT = CVT  • VVT is a projection matrix, which projects the rows of X (the data  points (cid:126)x1, . . . , (cid:126)xn) onto the subspace V.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  2  
last class: projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = XVVT = CVT  • VVT is a projection matrix, which projects the rows of X (the data  points (cid:126)x1, . . . , (cid:126)xn) onto the subspace V.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  2  
last class: projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = XVVT = CVT  • VVT is a projection matrix, which projects the rows of X (the data  points (cid:126)x1, . . . , (cid:126)xn) onto the subspace V.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  2  
embedding with assumptions  Assume that data points (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd .  3  
embedding with assumptions  Assume that data points (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd .  Will show above in homework. Today’s focus: How do we ﬁnd V and V?  3  
embedding with assumptions  Assume that data points (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd .  Letting (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns, VT (cid:126)xi ∈ Rk is still a good embedding for xi ∈ Rd and XVVT is still a good approximation for X:  XVVT = arg min  B with rows in V  (cid:107)X − B(cid:107)2 F .  Will show above in homework. Today’s focus: How do we ﬁnd V and V?  3  
a step back: why low-rank approximation?  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace?  4  
a step back: why low-rank approximation?  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace? • The rows of X can be approximately reconstructed from a basis  of k vectors.  4  
a step back: why low-rank approximation?  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace? • The rows of X can be approximately reconstructed from a basis  of k vectors.  4  
dual view of low-rank approximation  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace?  5  
dual view of low-rank approximation  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace? • Equivalently, the columns of X are approx. spanned by k vectors.  5  
dual view of low-rank approximation  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace? • Equivalently, the columns of X are approx. spanned by k vectors. Linearly Dependent Variables:  5  
dual view of low-rank approximation  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace? • Equivalently, the columns of X are approx. spanned by k vectors. Linearly Dependent Variables:  5  
dual view of low-rank approximation  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace? • Equivalently, the columns of X are approx. spanned by k vectors. Linearly Dependent Variables:  5  
dual view of low-rank approximation  Question: Why might we expect (cid:126)x1, . . . , (cid:126)xn ∈ Rd to lie close to a k-dimensional subspace? • Equivalently, the columns of X are approx. spanned by k vectors. Linearly Dependent Variables:  5  
properties of projection matrices  Quick Exercise 1 : Show that VVT is idempotent. I.e., (VVT )(VVT )(cid:126)y = (VVT )(cid:126)y for any (cid:126)y ∈ Rd .  6  
properties of projection matrices  Quick Exercise 1 : Show that VVT is idempotent. I.e., (VVT )(VVT )(cid:126)y = (VVT )(cid:126)y for any (cid:126)y ∈ Rd .  Quick Exercise 2: The projection is orthogonal to its complement: For any (cid:126)y ∈ Rd , (cid:104)VVT (cid:126)y , (I − VVT )(cid:126)y(cid:105) = 0  6  
properties of projection matrices  Quick Exercise 1 : Show that VVT is idempotent. I.e., (VVT )(VVT )(cid:126)y = (VVT )(cid:126)y for any (cid:126)y ∈ Rd .  Quick Exercise 2: The projection is orthogonal to its complement: For any (cid:126)y ∈ Rd , (cid:104)VVT (cid:126)y , (I − VVT )(cid:126)y(cid:105) = 0 Implies the Pythagorean Theorem: Show that for any (cid:126)y ∈ Rd ,  (cid:107)(cid:126)y(cid:107)2  2 = (cid:107)(VVT )(cid:126)y(cid:107)2  2 + (cid:107)(cid:126)y − (VVT )(cid:126)y(cid:107)2 2.  6  
best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT and XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  7  
best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT and XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  arg min  orthonormal V∈Rd×k  (cid:107)X − XVVT(cid:107)2  F =  =  =  (Xi,j − (XVVT )i,j )2  (cid:107)(cid:126)xi − VVT (cid:126)xi(cid:107)2  2  (cid:107)(cid:126)xi(cid:107)2  2 − (cid:107)VVT (cid:126)xi(cid:107)2  2  i,j  (cid:88) n(cid:88) n(cid:88)  i=1  i=1  7  
best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT and XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  arg min  orthonormal V∈Rd×k  (cid:107)X − XVVT(cid:107)2  F =  =  So want to maximize(cid:80)  =  2 =(cid:80)  i (cid:126)x T  i (cid:107)VVT (cid:126)xi(cid:107)2  (Xi,j − (XVVT )i,j )2  i,j  (cid:107)(cid:126)xi − VVT (cid:126)xi(cid:107)2  (cid:88) n(cid:88) n(cid:88) i VVT VVT (cid:126)xi =(cid:80)  i=1  i=1  2  (cid:107)(cid:126)xi(cid:107)2  2 − (cid:107)VVT (cid:126)xi(cid:107)2  2  i (cid:107)VT (cid:126)xi(cid:107)2  2  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  7  
solution via eigendecomposition  V minimizing (cid:107)X − XVVT(cid:107)2  arg max  orthonormal V∈Rd×k  i=1  j=1  i=1  n(cid:88)  F is given by: (cid:107)VT (cid:126)xi(cid:107)2  2 =  k(cid:88)  n(cid:88)  (cid:104)(cid:126)vj , (cid:126)xi(cid:105)2  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
solution via eigendecomposition  V minimizing (cid:107)X − XVVT(cid:107)2  arg max  orthonormal V∈Rd×k  n(cid:88)  F is given by: (cid:107)VT (cid:126)xi(cid:107)2  2 =  i=1  j=1  i=1  k(cid:88)  n(cid:88)  (cid:104)(cid:126)vj , (cid:126)xi(cid:105)2 =  k(cid:88)  j=1  (cid:107)X(cid:126)vj(cid:107)2  2  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
solution via eigendecomposition  V minimizing (cid:107)X − XVVT(cid:107)2  arg max  orthonormal V∈Rd×k  n(cid:88)  F is given by: (cid:107)VT (cid:126)xi(cid:107)2  2 =  i=1  j=1  i=1  k(cid:88)  n(cid:88)  (cid:104)(cid:126)vj , (cid:126)xi(cid:105)2 =  k(cid:88)  j=1  (cid:107)X(cid:126)vj(cid:107)2  2  Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:107)X(cid:126)v(cid:107)2 2.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
solution via eigendecomposition  V minimizing (cid:107)X − XVVT(cid:107)2  arg max  orthonormal V∈Rd×k  n(cid:88)  F is given by: (cid:107)VT (cid:126)xi(cid:107)2  2 =  i=1  j=1  i=1  k(cid:88)  n(cid:88)  (cid:104)(cid:126)vj , (cid:126)xi(cid:105)2 =  k(cid:88)  j=1  (cid:107)X(cid:126)vj(cid:107)2  2  Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T XT X(cid:126)v .  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
solution via eigendecomposition  V minimizing (cid:107)X − XVVT(cid:107)2  arg max  orthonormal V∈Rd×k  n(cid:88)  F is given by: (cid:107)VT (cid:126)xi(cid:107)2  2 =  i=1  j=1  i=1  k(cid:88)  n(cid:88)  (cid:104)(cid:126)vj , (cid:126)xi(cid:105)2 =  k(cid:88)  j=1  (cid:107)X(cid:126)vj(cid:107)2  2  Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T XT X(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T XT X(cid:126)v .  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
solution via eigendecomposition  V minimizing (cid:107)X − XVVT(cid:107)2  arg max  orthonormal V∈Rd×k  n(cid:88)  F is given by: (cid:107)VT (cid:126)xi(cid:107)2  2 =  i=1  j=1  i=1  k(cid:88)  n(cid:88)  (cid:104)(cid:126)vj , (cid:126)xi(cid:105)2 =  k(cid:88)  j=1  (cid:107)X(cid:126)vj(cid:107)2  2  Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T XT X(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T XT X(cid:126)v .  . . .  (cid:126)vk =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)vj(cid:105)=0 ∀j<k  (cid:126)v T XT X(cid:126)v .  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
solution via eigendecomposition  V minimizing (cid:107)X − XVVT(cid:107)2  arg max  orthonormal V∈Rd×k  n(cid:88)  F is given by: (cid:107)VT (cid:126)xi(cid:107)2  2 =  i=1  j=1  i=1  k(cid:88)  n(cid:88)  (cid:104)(cid:126)vj , (cid:126)xi(cid:105)2 =  k(cid:88)  j=1  (cid:107)X(cid:126)vj(cid:107)2  2  Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T XT X(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T XT X(cid:126)v .  . . .  (cid:126)vk =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)vj(cid:105)=0 ∀j<k  (cid:126)v T XT X(cid:126)v .  These are exactly the top k eigenvectors of XT X.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
