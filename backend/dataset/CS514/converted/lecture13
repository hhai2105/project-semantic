compsci 514: algorithms for data science  Andrew McGregor  Lecture 13  0  
summary of first section  
what we’ve covered  • Probability Tools: Linearity of Expectation, Linear of Variance of Independent Variables, Concentration Bounds (incl. Markov, Chebyshev, Bernstein, Chernoﬀ), Union Bound, Median Trick.  • Hash Tables and Bloom Filters: Analyzing collisions. Building  2-level hash tables. Bloom ﬁlters and false positive rates.  • Locality Sensitive Hashing: MinHash for Jaccard Similarity and SimHash for Cosine Similarity. Nearest Neighbor. All-Pairs Similarity Search.  • Small Space Data Stream Algorithms: a) distinct items, b)  frequent elements, c) frequency moments (homework).  • Johnson Lindenstrauss Lemma: Reducing dimension of  vectors via random projection such that pairwise distances are approximately preserved.  2  
randomized algorithms unit takeaways  • Randomization is an important tool in working with large datasets. • Lets us solve ‘easy’ problems that get really diﬃcult on massive  datasets. Fast/space eﬃcient look up (hash tables and bloom ﬁlters), distinct items counting, frequent items counting, near neighbor search (LSH), etc.  • The analysis of randomized algorithms leads to complex output  distributions, which we can’t compute exactly.  • We’ve covered many of the key ideas used through a small number of  example applications/algorithms.  • We use concentration inequalities to bound these distributions and  behaviors like accuracy, space usage, and runtime.  • Concentration inequalities and probability tools used in randomized  algorithms are also fundamental in statistics, machine learning theory, probabilistic modeling of complex systems, etc.  3  
useful probability facts (1/2)  • Linearity of Expectation: For any random variables X1, . . . , Xn  and constants c1, . . . , cn,  E[c1X1 + . . . + cnXn] = c1E[X1] + . . . + cnE[Xn]  4  
useful probability facts (1/2)  • Linearity of Expectation: For any random variables X1, . . . , Xn  and constants c1, . . . , cn,  E[c1X1 + . . . + cnXn] = c1E[X1] + . . . + cnE[Xn]  • Independent Random Variables: X1, X2, . . . Xn are independent random variables if for any set S ⊂ [n] and values a1, a2, . . . , an  (cid:89)  i∈S  Pr(Xi = ai for all i ∈ S) =  Pr(Xi = ai ) .  They are k-wise independent if this holds for S with |S| ≤ k.  4  
useful probability facts (1/2)  • Linearity of Expectation: For any random variables X1, . . . , Xn  and constants c1, . . . , cn,  E[c1X1 + . . . + cnXn] = c1E[X1] + . . . + cnE[Xn]  • Independent Random Variables: X1, X2, . . . Xn are independent random variables if for any set S ⊂ [n] and values a1, a2, . . . , an  (cid:89)  i∈S  Pr(Xi = ai for all i ∈ S) =  Pr(Xi = ai ) .  They are k-wise independent if this holds for S with |S| ≤ k. • Linearity of Variance: IfX1, . . . , Xn are independent (in fact 2-wise independent suﬃces) then for any constants c1, . . . , cn  Var[c1X1 + . . . + cnXn] = c 2  1 Var[X1] + . . . + c 2  n Var[Xn]  4  
useful probability facts (2/2)  • Union Bound: For any events A1, A2, A3, . . .  (cid:104)(cid:91)  (cid:105) ≤(cid:88)  Pr  Ai  Pr[Ai ] .  • An indicator random variable X just takes the values 0 or 1:  i  E[X ] = p  Var[X ] = p(1 − p)  where p = Pr[X = 1]  • If Y = X1 + . . . + Xn where each Xi are independent and p = Pr[X1 = 1] = . . . = Pr[Xn = 1] then Y is a binomial random variable. Using linearity of expectation and variance,  E[X ] = np  Var[X ] = np(1 − p)  5  
balls and bins (1/2)  • Most of the analysis of hash functions that we’ve considered can  be abstracted as “balls and bins” problems: we throw n balls and each ball is equally likely to land in one of m bins. • Let Ri be number of balls bin i. Then Ri ∼ Bin(n, 1  • Union Bound implies Pr[max(R1, . . . , Rm) > t] ≤(cid:80)  m ). Ri and Rj not independent! i Pr[Ri > t]  E[Ri ] = n  m , Var[Ri ] = n • Pr [no collisions] = m−1  m ) and  m · (1 − 1 m . . . m−(n−1) m−2  m  m  Pr[collisions] = Pr[max(R1, . . . , Rm) > 1] ≤ 1/8 if m > 4n2  and more generally  Pr[max(R1, . . . , Rm) ≥ 2n/m] ≤ m2/n  • In the exam, you’ll be expected to do calculations like these.  6  
balls and bins (2/2)  • Let T be the number of bins where Ri = 0. We showed:  E[T ] = m(1 − 1/m)n  • The probability the next k balls thrown all land in non-empty  bins is  (1 − 1/T )k  and this lets us analyze the false positive rate of a Bloom ﬁlter.  7  
hash functions  • Hash function h : U → [n] is two universal if:  Pr[h(x) = h(y )] ≤ 1 n  .  8  
hash functions  • Hash function h : U → [n] is two universal if:  Pr[h(x) = h(y )] ≤ 1 n  .  • Hash function h : U → [n] is k-wise independent if {h(e)}e∈U  are k-wise independent and each h(e) is uniform in [n].  8  
hash functions  • Hash function h : U → [n] is two universal if:  Pr[h(x) = h(y )] ≤ 1 n  .  • Hash function h : U → [n] is k-wise independent if {h(e)}e∈U  are k-wise independent and each h(e) is uniform in [n].  • Hash function h : U → [n] is fully independent if {h(e)}e∈U are  independent and each h(e) is uniform in [n].  8  
three main concentration bounds  • Markov. For any non-negative random variable X and t > 0,  Pr[X ≥ t] ≤ E[X ]/t .  9  
three main concentration bounds  • Markov. For any non-negative random variable X and t > 0,  Pr[X ≥ t] ≤ E[X ]/t .  • Chebyshev. For any random variable X and t > 0, Pr[|X − E[X ]| ≥ t] ≤ Var[X ]/t2 .  9  
three main concentration bounds  • Markov. For any non-negative random variable X and t > 0,  Pr[X ≥ t] ≤ E[X ]/t .  • Chebyshev. For any random variable X and t > 0, Pr[|X − E[X ]| ≥ t] ≤ Var[X ]/t2 .  variables with µ = E[(cid:80)  • Chernoﬀ. Let X1, . . . , Xn be independent {0, 1} random (cid:19)  i Xi ]. Then for any δ > 0, − δ2µ δ + 2  Xi ) − µ| ≥ δµ] ≤ 2 exp  (cid:18)  (cid:88)  i  Pr[|(  .  9  
three main concentration bounds  • Markov. For any non-negative random variable X and t > 0,  Pr[X ≥ t] ≤ E[X ]/t .  • Chebyshev. For any random variable X and t > 0, Pr[|X − E[X ]| ≥ t] ≤ Var[X ]/t2 .  variables with µ = E[(cid:80)  • Chernoﬀ. Let X1, . . . , Xn be independent {0, 1} random (cid:19)  i Xi ]. Then for any δ > 0, − δ2µ δ + 2  Xi ) − µ| ≥ δµ] ≤ 2 exp  (cid:18)  (cid:88)  i  Pr[|(  .  • Generally, Chernoﬀ gives better results then Chebyshev and  Chebyshev gives better results than Markov. So choose bound based on how much you know about X .  9  
three main concentration bounds  • Markov. For any non-negative random variable X and t > 0,  Pr[X ≥ t] ≤ E[X ]/t .  • Chebyshev. For any random variable X and t > 0, Pr[|X − E[X ]| ≥ t] ≤ Var[X ]/t2 .  variables with µ = E[(cid:80)  • Chernoﬀ. Let X1, . . . , Xn be independent {0, 1} random (cid:19)  i Xi ]. Then for any δ > 0, − δ2µ δ + 2  Xi ) − µ| ≥ δµ] ≤ 2 exp  (cid:18)  (cid:88)  i  Pr[|(  .  • Generally, Chernoﬀ gives better results then Chebyshev and  Chebyshev gives better results than Markov. So choose bound based on how much you know about X .  • Bernstein generalizes Chernoﬀ to arbitrary bounded Xi variables.  9  
averaging and the median trick  • Want to learn a quantity q. Suppose you have a randomized  algorithm that returns X that has expectation q and variance σ2.  10  
averaging and the median trick  • Want to learn a quantity q. Suppose you have a randomized  algorithm that returns X that has expectation q and variance σ2.  • To get a good estimate of q, repeat algorithm t times to get X1, . . . , Xt and let A = (X1 + . . . + Xt)/t. Then, if t = σ2 δ2q2  Pr[|A − q| ≥ q] ≤ Var[A] 2q2  10  
averaging and the median trick  • Want to learn a quantity q. Suppose you have a randomized  algorithm that returns X that has expectation q and variance σ2.  • To get a good estimate of q, repeat algorithm t times to get X1, . . . , Xt and let A = (X1 + . . . + Xt)/t. Then, if t = σ2 δ2q2  Pr[|A − q| ≥ q] ≤ Var[A]  2q2 =  σ2/t 2q2  10  
averaging and the median trick  • Want to learn a quantity q. Suppose you have a randomized  algorithm that returns X that has expectation q and variance σ2.  • To get a good estimate of q, repeat algorithm t times to get X1, . . . , Xt and let A = (X1 + . . . + Xt)/t. Then, if t = σ2 δ2q2  Pr[|A − q| ≥ q] ≤ Var[A]  2q2 =  σ2/t 2q2 = δ  10  
averaging and the median trick  • Want to learn a quantity q. Suppose you have a randomized  algorithm that returns X that has expectation q and variance σ2.  • To get a good estimate of q, repeat algorithm t times to get X1, . . . , Xt and let A = (X1 + . . . + Xt)/t. Then, if t = σ2 δ2q2  Pr[|A − q| ≥ q] ≤ Var[A]  2q2 = • Median Trick: Let t = t1t2 where t1 = 4σ2  σ2/t 2q2 = δ  2q2 and t2 = O(log 1  δ ).  Let A1 be average of ﬁrst t1 results, let A2 be average of next t1 results etc. Then,  Pr[|Ai − q| ≥ q] ≤ 1/4 and Pr[|median(A1, . . . , At2) − q| ≥ q] ≤ δ.  10  
2-level hash tables vs. bloom filter  • Input to both is a set of items S and and both support queries  of the form “Is x ∈ S?” in constant time.  11  
2-level hash tables vs. bloom filter  • Input to both is a set of items S and and both support queries  of the form “Is x ∈ S?” in constant time.  • 2-Level Hash Table:  • Space is O(|S|)×“space required to store an element of S”  11  
2-level hash tables vs. bloom filter  • Input to both is a set of items S and and both support queries  of the form “Is x ∈ S?” in constant time.  • Space is O(|S|)×“space required to store an element of S”  • 2-Level Hash Table:  • Bloom Filter:  11  
2-level hash tables vs. bloom filter  • Input to both is a set of items S and and both support queries  of the form “Is x ∈ S?” in constant time.  • 2-Level Hash Table:  • Bloom Filter:  • Space is O(|S|)×“space required to store an element of S”  • Does not actually store the items in S, just a binary array from  which we make various deductions.  11  
2-level hash tables vs. bloom filter  • Input to both is a set of items S and and both support queries  of the form “Is x ∈ S?” in constant time.  • 2-Level Hash Table:  • Bloom Filter:  • Space is O(|S|)×“space required to store an element of S”  • Does not actually store the items in S, just a binary array from • Uses only O(|S|) space but at the cost of sometimes answering  which we make various deductions.  “yes” when answer should be “no” (a false positive)  11  
2-level hash tables vs. bloom filter  • Input to both is a set of items S and and both support queries  of the form “Is x ∈ S?” in constant time.  • 2-Level Hash Table:  • Bloom Filter:  • Space is O(|S|)×“space required to store an element of S”  which we make various deductions.  • Does not actually store the items in S, just a binary array from • Uses only O(|S|) space but at the cost of sometimes answering • If the Bloom Filter array is length m, false positive probability is roughly (1 − e−k|S|/m)k where k is the number of hash functions used. Picking k = ln 2 · m/|S| gives probability 1/2(ln 2)m/|S|  “yes” when answer should be “no” (a false positive)  11  
locality sensitive hashing  • Designed a hash function for hashing sets such that for sets A  and B, Pr[MH(A) = MH(B)] = J(A, B) =  MH(A) = min x∈A  h(x) where h : U → [0, 1] is fully independent  |A∩B| |A∪B| .  12  
locality sensitive hashing  • Designed a hash function for hashing sets such that for sets A  and B, Pr[MH(A) = MH(B)] = J(A, B) =  |A∩B| |A∪B| .  MH(A) = min x∈A  h(x) where h : U → [0, 1] is fully independent • Can form signature of set A using r independent hash functions:  signature(A) = (MH1(A), . . . , MHr (A))  Note Pr[signature(A) = signature(B)] = J(A, B)r .  12  
locality sensitive hashing  • Designed a hash function for hashing sets such that for sets A  and B, Pr[MH(A) = MH(B)] = J(A, B) =  |A∩B| |A∪B| .  MH(A) = min x∈A  h(x) where h : U → [0, 1] is fully independent • Can form signature of set A using r independent hash functions:  signature(A) = (MH1(A), . . . , MHr (A))  Note Pr[signature(A) = signature(B)] = J(A, B)r .  • Given rt independent hash functions, we can form t signatures  signature1(A), . . . signaturet(A). Then if s = J(A, B), Pr[signaturei (A) = signaturei (B) for some i] = 1 − (1 − s r )t .  12  
locality sensitive hashing  • Designed a hash function for hashing sets such that for sets A  and B, Pr[MH(A) = MH(B)] = J(A, B) =  |A∩B| |A∪B| .  MH(A) = min x∈A  h(x) where h : U → [0, 1] is fully independent • Can form signature of set A using r independent hash functions:  signature(A) = (MH1(A), . . . , MHr (A))  Note Pr[signature(A) = signature(B)] = J(A, B)r .  • Given rt independent hash functions, we can form t signatures  signature1(A), . . . signaturet(A). Then if s = J(A, B), Pr[signaturei (A) = signaturei (B) for some i] = 1 − (1 − s r )t .  • To ﬁnd all pairs of similar sets amongst A1, A2, A3, . . . only compare a pair if there exists i, their ith signatures match.  12  
data streams algorithms  • We want to compute something about the stream x1, x2, . . . , xm  with only one pass over the stream and limited space. • Let fi be the number of values in stream that equal i.  • Distinct Items: Can estimate D = |{i : fi > 0} up to a factor 1 +  • Frequently Elements Items: Can return a set S such that:  with probability 1 − δ in O(−2 log 1/δ) space.  fi ≥ m/k implies i ∈ S with probability 1 − δ in O(k/ · log 1/δ) space.  • Sum of Squares: Can estimate(cid:80) f 2  probability 1 − δ in O(−2 log 1/δ) space.  and  i ∈ S implies fi ≥ m(1 − )/k  i up to a factor 1 +  with  13  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  14  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  14  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  14  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  14  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  14  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  14  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  14  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  Use A[h(x)] to estimate f (x), the frequency of x in the stream. • Claim: A[h(x)] ≥ f (x). • Claim: A[h(x)] ≤ f (x) + 2n/m with probability at least 1/2.  14  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  Use A[h(x)] to estimate f (x), the frequency of x in the stream. • Claim: A[h(x)] ≥ f (x). • Claim: A[h(x)] ≤ f (x) + 2n/m with probability at least 1/2. How can we increase this probability to 1 − δ for arbitrary δ > 0?  14  
count-min sketch accuracy  15  
count-min sketch accuracy  15  
count-min sketch accuracy  15  
count-min sketch accuracy  15  
count-min sketch accuracy  15  
count-min sketch accuracy  15  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)].  15  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)]. • Then Pr[f (x) ≤ ˜f (x) ≤ f (x) + 2n/m] ≥ 1 − 1/2t.  15  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)]. • Then Pr[f (x) ≤ ˜f (x) ≤ f (x) + 2n/m] ≥ 1 − 1/2t.  15  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)]. • Then Pr[f (x) ≤ ˜f (x) ≤ f (x) + 2n/m] ≥ 1 − 1/2t. • Setting t = log(1/δ) ensures probability is at least 1 − δ.  15  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)]. • Then Pr[f (x) ≤ ˜f (x) ≤ f (x) + 2n/m] ≥ 1 − 1/2t. • Setting t = log(1/δ) ensures probability is at least 1 − δ. • Setting m = 2k/ ensures 2n/m = n/k and that’s enough to  determine whether we need to output the element.  15  
johnson-lindenstrauss  The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a  random matrix with m = O(cid:0)−2log n(cid:1), for (cid:126)x1, . . . , (cid:126)xn ∈ Rd with  high probability, for all i, j:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)M(cid:126)xi − M(cid:126)xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2  where (cid:107)(cid:126)z(cid:107)2  2 is the sum of squared entries of (cid:126)z.  16  
johnson-lindenstrauss  The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a  random matrix with m = O(cid:0)−2log n(cid:1), for (cid:126)x1, . . . , (cid:126)xn ∈ Rd with  high probability, for all i, j:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)M(cid:126)xi − M(cid:126)xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2  where (cid:107)(cid:126)z(cid:107)2  2 is the sum of squared entries of (cid:126)z.  Proof Idea:  16  
johnson-lindenstrauss  The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a  random matrix with m = O(cid:0)−2log n(cid:1), for (cid:126)x1, . . . , (cid:126)xn ∈ Rd with  high probability, for all i, j:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)M(cid:126)xi − M(cid:126)xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2  where (cid:107)(cid:126)z(cid:107)2  2 is the sum of squared entries of (cid:126)z.  Proof Idea: • Follows from Distributional JL: If M ∈ RO(−2 log(1/δ))×d has N (0, 1/m) entries then for any (cid:126)y ∈ Rd , (cid:107)M(cid:126)y(cid:107)2 ≈ (1 ± )(cid:107)(cid:126)y(cid:107)2 with probability at least 1 − δ.  16  
johnson-lindenstrauss  The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a  random matrix with m = O(cid:0)−2log n(cid:1), for (cid:126)x1, . . . , (cid:126)xn ∈ Rd with  high probability, for all i, j:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)M(cid:126)xi − M(cid:126)xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2  where (cid:107)(cid:126)z(cid:107)2  2 is the sum of squared entries of (cid:126)z.  Proof Idea: • Follows from Distributional JL: If M ∈ RO(−2 log(1/δ))×d has N (0, 1/m) entries then for any (cid:126)y ∈ Rd , (cid:107)M(cid:126)y(cid:107)2 ≈ (1 ± )(cid:107)(cid:126)y(cid:107)2 with probability at least 1 − δ.  • To prove Distributional JL Lemma:  16  
johnson-lindenstrauss  The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a  random matrix with m = O(cid:0)−2log n(cid:1), for (cid:126)x1, . . . , (cid:126)xn ∈ Rd with  high probability, for all i, j:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)M(cid:126)xi − M(cid:126)xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2  where (cid:107)(cid:126)z(cid:107)2  2 is the sum of squared entries of (cid:126)z.  Proof Idea: • Follows from Distributional JL: If M ∈ RO(−2 log(1/δ))×d has N (0, 1/m) entries then for any (cid:126)y ∈ Rd , (cid:107)M(cid:126)y(cid:107)2 ≈ (1 ± )(cid:107)(cid:126)y(cid:107)2 with probability at least 1 − δ. • By linearity of expectation and variance, E[(cid:107)M(cid:126)y(cid:107)2  • To prove Distributional JL Lemma:  2] = (cid:107)(cid:126)y(cid:107)2 2.  16  
johnson-lindenstrauss  The Johnson Lindenstrauss lemma states that if M ∈ Rm×d is a  random matrix with m = O(cid:0)−2log n(cid:1), for (cid:126)x1, . . . , (cid:126)xn ∈ Rd with  high probability, for all i, j:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)M(cid:126)xi − M(cid:126)xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2  where (cid:107)(cid:126)z(cid:107)2  2 is the sum of squared entries of (cid:126)z.  Proof Idea: • Follows from Distributional JL: If M ∈ RO(−2 log(1/δ))×d has N (0, 1/m) entries then for any (cid:126)y ∈ Rd , (cid:107)M(cid:126)y(cid:107)2 ≈ (1 ± )(cid:107)(cid:126)y(cid:107)2 with probability at least 1 − δ. • By linearity of expectation and variance, E[(cid:107)M(cid:126)y(cid:107)2 • (cid:107)M(cid:126)y(cid:107)2  • To prove Distributional JL Lemma:  2] = (cid:107)(cid:126)y(cid:107)2 2.  2 is the sum of m squared independent normal distributions  and is tightly concentrated around the expectation.  16  
sneak peak of next section  
summary  Next Few Classes: Low-rank approximation, the SVD, and principal component analysis (PCA). • Reduce d-dimesional data points to a smaller dimension m. • Like JL, compression is linear – by applying a matrix. • Choose matrix carefully based on structure of the dataset. • Can give even better compression than random projection.  18  
summary  Next Few Classes: Low-rank approximation, the SVD, and principal component analysis (PCA). • Reduce d-dimesional data points to a smaller dimension m. • Like JL, compression is linear – by applying a matrix. • Choose matrix carefully based on structure of the dataset. • Can give even better compression than random projection.  Will be using a fair amount of linear algebra: orthogonal basis, column/row span, eigenvectors, etc,  18  
