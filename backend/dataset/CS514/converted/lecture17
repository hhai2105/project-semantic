compsci 514: algorithms for data science  Andrew McGregor  Lecture 17  0  
summary  Last Class: Low-Rank Approximation, Eigendecomposition, PCA • For any symmetric square matrix A, we can write A = V ΛV T where  columns of V are orthonormal eigenvectors.  • Can approximate data lying close to in a k-dimensional subspace by  projecting data points into that space.  • Can ﬁnd the best k-dimensional subspace via eigendecomposition  applied to XT X (PCA).  • Measuring error in terms of the eigenvalue spectrum.  This Class: SVD and Applications • SVD and connection to eigenvalue value decomposition. • Applications of low-rank approximation beyond compression.  1  
singular value decomposition  The Singular Value Decomposition (SVD) generalizes the eigendecomposition to asymmetric (even rectangular) matrices.  2  
singular value decomposition  The Singular Value Decomposition (SVD) generalizes the eigendecomposition to asymmetric (even rectangular) matrices. Any matrix X ∈ Rn×d with rank(X) = r can be written as X = UΣVT . • U has orthonormal columns (cid:126)u1, . . . , (cid:126)ur ∈ Rn (left singular vectors). • V has orthonormal columns (cid:126)v1, . . . , (cid:126)vr ∈ Rd (right singular vectors). • Σ is diagonal with elements σ1 ≥ σ2 ≥ . . . ≥ σr > 0 (singular values).  2  
singular value decomposition  The Singular Value Decomposition (SVD) generalizes the eigendecomposition to asymmetric (even rectangular) matrices. Any matrix X ∈ Rn×d with rank(X) = r can be written as X = UΣVT . • U has orthonormal columns (cid:126)u1, . . . , (cid:126)ur ∈ Rn (left singular vectors). • V has orthonormal columns (cid:126)v1, . . . , (cid:126)vr ∈ Rd (right singular vectors). • Σ is diagonal with elements σ1 ≥ σ2 ≥ . . . ≥ σr > 0 (singular values).  2  
singular value decomposition  The Singular Value Decomposition (SVD) generalizes the eigendecomposition to asymmetric (even rectangular) matrices. Any matrix X ∈ Rn×d with rank(X) = r can be written as X = UΣVT . • U has orthonormal columns (cid:126)u1, . . . , (cid:126)ur ∈ Rn (left singular vectors). • V has orthonormal columns (cid:126)v1, . . . , (cid:126)vr ∈ Rd (right singular vectors). • Σ is diagonal with elements σ1 ≥ σ2 ≥ . . . ≥ σr > 0 (singular values).  The ‘swiss army knife’ of modern linear algebra.  2  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT :  XT X =  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT :  XT X = VΣUT UΣVT  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT :  XT X = VΣUT UΣVT = VΣ2VT  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT : XT X = VΣUT UΣVT = VΣ2VT (the eigendecomposition)  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT : XT X = VΣUT UΣVT = VΣ2VT (the eigendecomposition)  Similarly: XXT = UΣVT VΣUT = UΣ2UT .  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT : XT X = VΣUT UΣVT = VΣ2VT (the eigendecomposition)  Similarly: XXT = UΣVT VΣUT = UΣ2UT .  The right and left singular vectors are the eigenvectors of the covariance matrix XT X and the gram matrix XXT respectively.  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT : XT X = VΣUT UΣVT = VΣ2VT (the eigendecomposition)  Similarly: XXT = UΣVT VΣUT = UΣ2UT .  The right and left singular vectors are the eigenvectors of the covariance matrix XT X and the gram matrix XXT respectively. So, letting Vk ∈ Rd×k have columns equal to (cid:126)v1, . . . , (cid:126)vk , we know that XVk VT  k is the best rank-k approximation to X (given by PCA).  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT : XT X = VΣUT UΣVT = VΣ2VT (the eigendecomposition)  Similarly: XXT = UΣVT VΣUT = UΣ2UT .  The right and left singular vectors are the eigenvectors of the covariance matrix XT X and the gram matrix XXT respectively. So, letting Vk ∈ Rd×k have columns equal to (cid:126)v1, . . . , (cid:126)vk , we know that XVk VT  k is the best rank-k approximation to X (given by PCA).  What about Uk UT  k X where Uk ∈ Rn×k has columns equal to (cid:126)u1, . . . , (cid:126)uk ?  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
connection of the svd to eigendecomposition  Writing X ∈ Rn×d in its singular value decomposition X = UΣVT : XT X = VΣUT UΣVT = VΣ2VT (the eigendecomposition)  Similarly: XXT = UΣVT VΣUT = UΣ2UT .  The right and left singular vectors are the eigenvectors of the covariance matrix XT X and the gram matrix XXT respectively. So, letting Vk ∈ Rd×k have columns equal to (cid:126)v1, . . . , (cid:126)vk , we know that XVk VT  k is the best rank-k approximation to X (given by PCA).  What about Uk UT  k X where Uk ∈ Rn×k has columns equal to (cid:126)u1, . . . , (cid:126)uk ?  Exercise: Uk UT  k X = XVk VT  k = Uk Σk VT k  X ∈ Rn×d : data matrix, U ∈ Rn×rank(X): matrix with orthonormal columns (cid:126)u1, (cid:126)u2, . . . (left singular vectors), V ∈ Rd×rank(X): matrix with orthonormal columns (cid:126)v1, (cid:126)v2, . . . (right singular vectors), Σ ∈ Rrank(X)×rank(X): positive diag- onal matrix containing singular values of X.  3  
the svd and optimal low-rank approximation  The best low-rank approximation to X: Xk = arg minrank −k B∈Rn×d (cid:107)X − B(cid:107)F is given by:  Xk = XVk VT  k = Uk UT  k X = Uk Σk VT k  4  
the svd and optimal low-rank approximation  The best low-rank approximation to X: Xk = arg minrank −k B∈Rn×d (cid:107)X − B(cid:107)F is given by:  Xk = XVk VT  k = Uk UT  k X = Uk Σk VT k  Correspond to projecting the rows (data points) onto the span of Vk or the columns (features) onto the span of Uk  4  
the svd and optimal low-rank approximation  The best low-rank approximation to X: Xk = arg minrank −k B∈Rn×d (cid:107)X − B(cid:107)F is given by:  Xk = XVk VT  k = Uk UT  k X = Uk Σk VT k  Correspond to projecting the rows (data points) onto the span of Vk or the columns (features) onto the span of Uk  4  
the svd and optimal low-rank approximation  The best low-rank approximation to X: Xk = arg minrank −k B∈Rn×d (cid:107)X − B(cid:107)F is given by:  Xk = XVk VT  k = Uk UT  k X = Uk Σk VT k  Correspond to projecting the rows (data points) onto the span of Vk or the columns (features) onto the span of Uk  4  
the svd and optimal low-rank approximation  The best low-rank approximation to X: Xk = arg minrank −k B∈Rn×d (cid:107)X − B(cid:107)F is given by:  Xk = XVk VT  k = Uk UT  k X = Uk Σk VT k  Correspond to projecting the rows (data points) onto the span of Vk or the columns (features) onto the span of Uk  4  
basic idea to prove existence of svd  • Let (cid:126)v1, (cid:126)v2, . . . ,∈ Rd be orthonormal eigenvectors of XT X.  5  
basic idea to prove existence of svd  • Let (cid:126)v1, (cid:126)v2, . . . ,∈ Rd be orthonormal eigenvectors of XT X. • Let σi = (cid:107)X(cid:126)vi(cid:107)2 and deﬁne unit vector (cid:126)ui = X(cid:126)vi  .  σi  5  
basic idea to prove existence of svd  • Let (cid:126)v1, (cid:126)v2, . . . ,∈ Rd be orthonormal eigenvectors of XT X. • Let σi = (cid:107)X(cid:126)vi(cid:107)2 and deﬁne unit vector (cid:126)ui = X(cid:126)vi • Exercise: Show (cid:126)u1, (cid:126)u2, . . . are orthonormal.  σi  .  5  
basic idea to prove existence of svd  • Let (cid:126)v1, (cid:126)v2, . . . ,∈ Rd be orthonormal eigenvectors of XT X. • Let σi = (cid:107)X(cid:126)vi(cid:107)2 and deﬁne unit vector (cid:126)ui = X(cid:126)vi • Exercise: Show (cid:126)u1, (cid:126)u2, . . . are orthonormal. • This establishes that XV = UΣ and that V and U have the  σi  .  required properties to show X = UΣVT .  5  
basic idea to prove existence of svd  • Let (cid:126)v1, (cid:126)v2, . . . ,∈ Rd be orthonormal eigenvectors of XT X. • Let σi = (cid:107)X(cid:126)vi(cid:107)2 and deﬁne unit vector (cid:126)ui = X(cid:126)vi • Exercise: Show (cid:126)u1, (cid:126)u2, . . . are orthonormal. • This establishes that XV = UΣ and that V and U have the  σi  .  required properties to show X = UΣVT .  • To see rest of the details, see https://math.mit.edu/  classes/18.095/2016IAP/lec2/SVD_Notes.pdf  5  
applications of low-rank approximation  Rest of Class: Examples of how low-rank approximation is applied in a variety of data science applications.  6  
applications of low-rank approximation  Rest of Class: Examples of how low-rank approximation is applied in a variety of data science applications. • Used for many reasons other than dimensionality reduction/data  compression.  6  
matrix completion  Consider a matrix X ∈ Rn×d which we cannot fully observe but believe is close to rank-k (i.e., well approximated by a rank k matrix).  7  
matrix completion  Consider a matrix X ∈ Rn×d which we cannot fully observe but believe is close to rank-k (i.e., well approximated by a rank k matrix). Classic example: the Netﬂix prize problem.  7  
matrix completion  Consider a matrix X ∈ Rn×d which we cannot fully observe but believe is close to rank-k (i.e., well approximated by a rank k matrix). Classic example: the Netﬂix prize problem.  Solve: Y = arg min rank −k B  (cid:88)  observed (j,k)  [Xj,k − Bj,k ]2  7  
matrix completion  Consider a matrix X ∈ Rn×d which we cannot fully observe but believe is close to rank-k (i.e., well approximated by a rank k matrix). Classic example: the Netﬂix prize problem.  Solve: Y = arg min rank −k B  (cid:88)  observed (j,k)  [Xj,k − Bj,k ]2  Under certain assumptions, can show that Y well approximates X on both the observed and (most importantly) unobserved entries.  7  
entity embeddings  Dimensionality reduction embeds d-dimensional vectors into k (cid:28) d dimensions. But what about when you want to embed objects other than vectors?  8  
entity embeddings  Dimensionality reduction embeds d-dimensional vectors into k (cid:28) d dimensions. But what about when you want to embed objects other than vectors? • Documents (for topic-based search and classiﬁcation) • Words (to identify synonyms, translations, etc.) • Nodes in a social network  8  
entity embeddings  Dimensionality reduction embeds d-dimensional vectors into k (cid:28) d dimensions. But what about when you want to embed objects other than vectors? • Documents (for topic-based search and classiﬁcation) • Words (to identify synonyms, translations, etc.) • Nodes in a social network  Usual Approach: Convert each item into a high-dimensional feature vector and then apply low-rank approximation.  8  
example: latent semantic analysis  9  
example: latent semantic analysis  9  
example: latent semantic analysis  10  
example: latent semantic analysis  • If the error (cid:107)X − YZT(cid:107)F is small, then on average,  Xi,a ≈ (YZT )i,a = (cid:104)(cid:126)yi , (cid:126)za(cid:105).  10  
example: latent semantic analysis  • If the error (cid:107)X − YZT(cid:107)F is small, then on average,  Xi,a ≈ (YZT )i,a = (cid:104)(cid:126)yi , (cid:126)za(cid:105).  • I.e., (cid:104)(cid:126)yi , (cid:126)za(cid:105) ≈ 1 when doci contains worda.  10  
example: latent semantic analysis  • If the error (cid:107)X − YZT(cid:107)F is small, then on average,  Xi,a ≈ (YZT )i,a = (cid:104)(cid:126)yi , (cid:126)za(cid:105).  • I.e., (cid:104)(cid:126)yi , (cid:126)za(cid:105) ≈ 1 when doci contains worda. • If doci and docj both contain worda, (cid:104)(cid:126)yi , (cid:126)za(cid:105) ≈ (cid:104)(cid:126)yj , (cid:126)za(cid:105) ≈ 1.  10  
example: latent semantic analysis  If doci and docj both contain worda, (cid:104)(cid:126)yi , (cid:126)za(cid:105) ≈ (cid:104)(cid:126)yj , (cid:126)za(cid:105) ≈ 1  11  
example: latent semantic analysis  If doci and docj both contain worda, (cid:104)(cid:126)yi , (cid:126)za(cid:105) ≈ (cid:104)(cid:126)yj , (cid:126)za(cid:105) ≈ 1  Another View: Each column of Y represents a ‘topic’. (cid:126)yi (j) indicates how much doci belongs to topic j. (cid:126)za(j) indicates how much worda associates with that topic.  11  
example: latent semantic analysis  • Just like with documents, (cid:126)za and (cid:126)zb will tend to have high dot product  if worda and wordb appear in many of the same documents.  12  
example: latent semantic analysis  • Just like with documents, (cid:126)za and (cid:126)zb will tend to have high dot product  if worda and wordb appear in many of the same documents.  • In an SVD decomposition we set ZT = Σk VT K . • The columns of Vk are equivalently: the top k eigenvectors of XT X.  12  
example: latent semantic analysis  • Just like with documents, (cid:126)za and (cid:126)zb will tend to have high dot product  if worda and wordb appear in many of the same documents.  • In an SVD decomposition we set ZT = Σk VT K . • The columns of Vk are equivalently: the top k eigenvectors of XT X.  The eigendecomposition of XT X is XT X = VΣ2VT .  12  
example: latent semantic analysis  • Just like with documents, (cid:126)za and (cid:126)zb will tend to have high dot product  if worda and wordb appear in many of the same documents.  • In an SVD decomposition we set ZT = Σk VT K . • The columns of Vk are equivalently: the top k eigenvectors of XT X.  The eigendecomposition of XT X is XT X = VΣ2VT .  • The best rank-k approximation of XT X is  (cid:107)XT X − B(cid:107)F = Vk Σ2  k VT  k = ZZT  arg min rank −k B  12  
example: word embedding  LSA gives a way of embedding words into k-dimensional space. • Embedding is via low-rank approximation of XT X: where (XT X)a,b is  the number of documents that both worda and wordb appear in.  13  
example: word embedding  LSA gives a way of embedding words into k-dimensional space. • Embedding is via low-rank approximation of XT X: where (XT X)a,b is  the number of documents that both worda and wordb appear in.  • Think about XT X as a similarity matrix (gram matrix, kernel matrix)  with entry (a, b) being the similarity between worda and wordb.  13  
example: word embedding  LSA gives a way of embedding words into k-dimensional space. • Embedding is via low-rank approximation of XT X: where (XT X)a,b is  the number of documents that both worda and wordb appear in.  • Think about XT X as a similarity matrix (gram matrix, kernel matrix)  with entry (a, b) being the similarity between worda and wordb.  • Many ways to measure similarity: number of sentences both occur in,  number of times both appear in the same window of w words, in similar positions of documents in diﬀerent languages, etc.  13  
example: word embedding  LSA gives a way of embedding words into k-dimensional space. • Embedding is via low-rank approximation of XT X: where (XT X)a,b is  the number of documents that both worda and wordb appear in.  • Think about XT X as a similarity matrix (gram matrix, kernel matrix)  with entry (a, b) being the similarity between worda and wordb.  • Many ways to measure similarity: number of sentences both occur in,  number of times both appear in the same window of w words, in similar positions of documents in diﬀerent languages, etc.  • Replacing XT X with these diﬀerent metrics (sometimes appropriately transformed) leads to popular word embedding algorithms: word2vec, GloVe, fastText, etc.  13  
example: word embedding  14  
example: word embedding  Note: word2vec is typically described as a neural-network method, but it is really just low-rank approximation of a speciﬁc similarity matrix. Neural word embedding as implicit matrix factorization, Levy and Goldberg.  14  
