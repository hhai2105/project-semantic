compsci 514: algorithms for data science  Andrew McGregor  Lecture 5  0  
exponential concentration bounds  • Can sometimes get tighter bounds than Markov via:  Pr[|X − E[X ]| ≥ λ] = Pr[|X − E[X ]|k ≥ λk ] ≤ E[|X − E[X ]|k ]  λk • Moment Generating Function: Consider for any t > 0:  Mt(X) = et·(X−E[X])  and note Mt(X) is monotonic for any t > 0  1  
exponential concentration bounds  • Can sometimes get tighter bounds than Markov via:  Pr[|X − E[X ]| ≥ λ] = Pr[|X − E[X ]|k ≥ λk ] ≤ E[|X − E[X ]|k ]  λk • Moment Generating Function: Consider for any t > 0:  tk (X − E[X])k  k=0  k!  Mt(X) = et·(X−E[X]) =  ∞(cid:88)  and note Mt(X) is monotonic for any t > 0 and so  Pr[|X − E[X ]| ≥ λ] = Pr[Mt(X ) ≥ etλ] ≤ E[Mt(X )]  etλ  1  
exponential concentration bounds  • Can sometimes get tighter bounds than Markov via:  Pr[|X − E[X ]| ≥ λ] = Pr[|X − E[X ]|k ≥ λk ] ≤ E[|X − E[X ]|k ]  λk • Moment Generating Function: Consider for any t > 0:  tk (X − E[X])k  k=0  k!  Mt(X) = et·(X−E[X]) =  ∞(cid:88)  and note Mt(X) is monotonic for any t > 0 and so  Pr[|X − E[X ]| ≥ λ] = Pr[Mt(X ) ≥ etλ] ≤ E[Mt(X )] • Weighted sum of all moments (t controls the weights) and choosing t appropriately lets one prove a number of very powerful exponential concentration bounds such as Chernoﬀ, Bernstein, Hoeﬀding, Azuma, Berry-Esseen, etc.  etλ  1  
bernstein inequality  Bernstein Inequality: Consider independent random variables X1, . . . , Xn all falling in [−M, M]. i=1 Xi ] and  i=1 Var[Xi ]. For any t ≥ 0:  i=1 Xi ] =(cid:80)n σ2 = Var[(cid:80)n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ t (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Xi − µ  Pr  i=1  (cid:33)  Let µ = E[(cid:80)n (cid:33) (cid:32)  −  t 2 2σ2 + 4  3 Mt  ≤ 2 exp  .  2  
bernstein inequality  Bernstein Inequality: Consider independent random variables X1, . . . , Xn all falling in [−M, M]. i=1 Xi ] and  i=1 Var[Xi ]. For any t ≥ 0:  i=1 Xi ] =(cid:80)n σ2 = Var[(cid:80)n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ t (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Xi − µ  Pr  i=1  (cid:33)  Assume that M = 1 and plug in t = s · σ for s ≤ σ.  Let µ = E[(cid:80)n (cid:33) (cid:32)  −  t 2 2σ2 + 4  3 Mt  ≤ 2 exp  .  2  
bernstein inequality  Bernstein Inequality: Consider independent random variables i=1 Xi ] and σ2 =  X1, . . . , Xn all falling in [-1,1]. Let µ = E[(cid:80)n Var[(cid:80)n i=1 Var[Xi ]. For any s ≥ 0: (cid:18)  (cid:33)  (cid:19)  i=1 Xi ] =(cid:80)n (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Pr  i=1  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ  Xi − µ  ≤ 2 exp  − s 2 4  Assume that M = 1 and plug in t = s · σ for s ≤ σ.  .  2  
bernstein inequality  Bernstein Inequality: Consider independent random variables i=1 Xi ] and σ2 =  i=1 Xi ] =(cid:80)n (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Pr  i=1  X1, . . . , Xn all falling in [-1,1]. Let µ = E[(cid:80)n Var[(cid:80)n i=1 Var[Xi ]. For any s ≥ 0: (cid:18)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ Compare to Chebyshev’s: Pr(cid:0)(cid:12)(cid:12)(cid:80)n i=1 Xi − µ(cid:12)(cid:12) ≥ sσ(cid:1) ≤ 1  Assume that M = 1 and plug in t = s · σ for s ≤ σ.  ≤ 2 exp  Xi − µ  (cid:33)  (cid:19)  .  − s 2 4  s 2 .  2  
bernstein inequality  Bernstein Inequality: Consider independent random variables i=1 Xi ] and σ2 =  i=1 Xi ] =(cid:80)n (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Pr  i=1  X1, . . . , Xn all falling in [-1,1]. Let µ = E[(cid:80)n Var[(cid:80)n i=1 Var[Xi ]. For any s ≥ 0: (cid:18)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ Compare to Chebyshev’s: Pr(cid:0)(cid:12)(cid:12)(cid:80)n i=1 Xi − µ(cid:12)(cid:12) ≥ sσ(cid:1) ≤ 1  Assume that M = 1 and plug in t = s · σ for s ≤ σ.  ≤ 2 exp  Xi − µ  (cid:33)  (cid:19)  .  − s 2 4  s 2 .  • An exponentially stronger dependence on s!  2  
comparision to chebyshev’s  Consider again bounding the number of heads H in n = 100 independent coin ﬂips.  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  Bernstein:  In Reality:  Pr(H ≥ 60) ≤ .15 Pr(H ≥ 70) ≤ .00086 Pr(H ≥ 80) ≤ 3−7  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 Pr(H ≥ 80) < 10−9  H: total number heads in 100 random coin ﬂips. E[H] = 50.  3  
comparision to chebyshev’s  Consider again bounding the number of heads H in n = 100 independent coin ﬂips.  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  Bernstein:  In Reality:  Pr(H ≥ 60) ≤ .15 Pr(H ≥ 70) ≤ .00086 Pr(H ≥ 80) ≤ 3−7  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 Pr(H ≥ 80) < 10−9  Getting much closer to the true probability.  H: total number heads in 100 random coin ﬂips. E[H] = 50.  3  
approximately maintaining a set  Want to store a set S of items from a massive universe of possible items (e.g., images, text documents, IP addresses).  4  
approximately maintaining a set  Want to store a set S of items from a massive universe of possible items (e.g., images, text documents, IP addresses).  Goal: support insert(x) to add x to the set and query (x) to check if x is in the set. Both in O(1) time.  4  
approximately maintaining a set  Want to store a set S of items from a massive universe of possible items (e.g., images, text documents, IP addresses).  Goal: support insert(x) to add x to the set and query (x) to check if x is in the set. Both in O(1) time.  4  
approximately maintaining a set  Want to store a set S of items from a massive universe of possible items (e.g., images, text documents, IP addresses).  Goal: support insert(x) to add x to the set and query (x) to check if x is in the set. Both in O(1) time. • Allow small probability δ > 0 of false positives. I.e., for any x,  Pr(query (x) = 1 and x /∈ S) ≤ δ.  4  
approximately maintaining a set  Want to store a set S of items from a massive universe of possible items (e.g., images, text documents, IP addresses).  Goal: support insert(x) to add x to the set and query (x) to check if x is in the set. Both in O(1) time. • Allow small probability δ > 0 of false positives. I.e., for any x,  Pr(query (x) = 1 and x /∈ S) ≤ δ.  Solution: Bloom ﬁlters (repeated random hashing). Will use much less space than a hash table.  4  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m].  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  5  
bloom filters  Chose k independent random hash functions h1, . . . , hk mapping the universe of elements U → [m]. • Maintain an array A containing m bits, all initially 0. • insert(x): set all bits A[h1(x)] = . . . = A[hk (x)] := 1. • query (x): return 1 only if A[h1(x)] = . . . = A[hk (x)] = 1.  No false negatives. False positives more likely with more insertions.  5  
applications: caching  Akamai (Boston-based company serving 15 − 30% of all web traﬃc) applies bloom ﬁlters to prevent caching of ‘one-hit-wonders’ – pages only visited once ﬁll over 75% of cache.  6  
applications: caching  Akamai (Boston-based company serving 15 − 30% of all web traﬃc) applies bloom ﬁlters to prevent caching of ‘one-hit-wonders’ – pages only visited once ﬁll over 75% of cache.  • When url x comes in, if query (x) = 1, cache the page at x. If not, run  insert(x) so that if it comes in again, it will be cached.  6  
applications: caching  Akamai (Boston-based company serving 15 − 30% of all web traﬃc) applies bloom ﬁlters to prevent caching of ‘one-hit-wonders’ – pages only visited once ﬁll over 75% of cache.  • When url x comes in, if query (x) = 1, cache the page at x. If not, run  insert(x) so that if it comes in again, it will be cached.  • False positive: A new url (possible one-hit-wonder) is cached. If the bloom ﬁlter has a false positive rate of δ = .05, the number of cached one-hit-wonders will be reduced by at least 95%.  6  
analysis  For a bloom ﬁlter with m bits and k hash functions, the insertion and query time is O(k).  7  
analysis  For a bloom ﬁlter with m bits and k hash functions, the insertion and query time is O(k). How does the false positive rate δ depend on m, k, and the number of items inserted?  7  
analysis  For a bloom ﬁlter with m bits and k hash functions, the insertion and query time is O(k). How does the false positive rate δ depend on m, k, and the number of items inserted?  Step 1: What is the probability that after inserting n elements, the i th bit of the array A is still 0?  7  
analysis  For a bloom ﬁlter with m bits and k hash functions, the insertion and query time is O(k). How does the false positive rate δ depend on m, k, and the number of items inserted?  Step 1: What is the probability that after inserting n elements, the i th bit of the array A is still 0? n × k total hashes must not hit bit i.  Pr(A[i] = 0) = Pr(cid:0)h1(x1) (cid:54)= i ∩ . . . ∩ hk (x1) (cid:54)= i  ∩ h1(x2) (cid:54)= i . . . ∩ hk (x2) (cid:54)= i ∩ . . .(cid:1)  7  
analysis  For a bloom ﬁlter with m bits and k hash functions, the insertion and query time is O(k). How does the false positive rate δ depend on m, k, and the number of items inserted?  Step 1: What is the probability that after inserting n elements, the i th bit of the array A is still 0? n × k total hashes must not hit bit i.  Pr(A[i] = 0) = Pr(cid:0)h1(x1) (cid:54)= i ∩ . . . ∩ hk (x1) (cid:54)= i (cid:123)(cid:122)  = Pr(cid:0)h1(x1) (cid:54)= i) × . . . × Pr(cid:0)hk (x1) (cid:54)= i) × Pr(cid:0)h1(x2) (cid:54)= i) . . . (cid:124) (cid:125)  ∩ h1(x2) (cid:54)= i . . . ∩ hk (x2) (cid:54)= i ∩ . . .(cid:1)  k·n events each occuring with probability 1−1/m  7  
analysis  For a bloom ﬁlter with m bits and k hash functions, the insertion and query time is O(k). How does the false positive rate δ depend on m, k, and the number of items inserted?  Step 1: What is the probability that after inserting n elements, the i th bit of the array A is still 0? n × k total hashes must not hit bit i.  Pr(A[i] = 0) = Pr(cid:0)h1(x1) (cid:54)= i ∩ . . . ∩ hk (x1) (cid:54)= i (cid:123)(cid:122)  ∩ h1(x2) (cid:54)= i . . . ∩ hk (x2) (cid:54)= i ∩ . . .(cid:1) (cid:19)kn  = Pr(cid:0)h1(x1) (cid:54)= i) × . . . × Pr(cid:0)hk (x1) (cid:54)= i) × Pr(cid:0)h1(x2) (cid:54)= i) . . . (cid:124) (cid:125) (cid:18)  k·n events each occuring with probability 1−1/m  =  1 − 1 m  7  
analysis  How does the false positive rate δ depend on m, k, and the number of items inserted?  What is the probability that after inserting n elements, the i th bit of the array A is still 0?  Pr(A[i] = 0) =  (cid:19)kn  (cid:18)  1 − 1 m  n: total number items in ﬁlter, m: number of bits in ﬁlter, k: number of random hash functions, h1, . . . hk : hash functions, A: bit array, δ: false positive rate.  8  
analysis  How does the false positive rate δ depend on m, k, and the number of items inserted?  What is the probability that after inserting n elements, the i th bit of the array A is still 0?  Pr(A[i] = 0) =  (cid:18)  1 − 1 m  (cid:19)kn ≈ e− kn  m  n: total number items in ﬁlter, m: number of bits in ﬁlter, k: number of random hash functions, h1, . . . hk : hash functions, A: bit array, δ: false positive rate.  8  
analysis  How does the false positive rate δ depend on m, k, and the number of items inserted?  What is the probability that after inserting n elements, the i th bit of the array A is still 0?  Let T be the number of zeros in the array after n inserts. Then,  (cid:18)  1 − 1 m  m  (cid:19)kn ≈ e− kn (cid:19)kn ≈ me− kn  m  Pr(A[i] = 0) =  (cid:18)  E [T ] = m  1 − 1 m  n: total number items in ﬁlter, m: number of bits in ﬁlter, k: number of random hash functions, h1, . . . hk : hash functions, A: bit array, δ: false positive rate.  8  
correct analysis sketch  If T is the number of 0 entries, for a non-inserted element w :  Pr(A[h1(w )] = . . . = A[hk (w )] = 1) = Pr(A[h1(w )] = 1) × . . . × Pr(A[hk (w )] = 1) = (1 − T /m) × . . . × (1 − T /m) = (1 − T /m)k  9  
correct analysis sketch  If T is the number of 0 entries, for a non-inserted element w :  Pr(A[h1(w )] = . . . = A[hk (w )] = 1) = Pr(A[h1(w )] = 1) × . . . × Pr(A[hk (w )] = 1) = (1 − T /m) × . . . × (1 − T /m) = (1 − T /m)k  • How small is T /m? Note that T  m ≥ m−nk  m when kn (cid:28) m. More  (cid:16) (cid:17) m ≈ e− kn e− kn  m  generally, it can be shown that T /m = Ω  via Theorem 2 of:  cglab.ca/~morin/publications/ds/bloom-submitted.pdf  9  
false positive rate  items inserted δ ≈(cid:16)  (cid:17)k  False Positive Rate: with m bits of storage, k hash functions, and n  1 − e  −kn m  .  10  05101520253000.050.10.150.20.250.30.350.40.450.5
false positive rate  items inserted δ ≈(cid:16)  (cid:17)k  False Positive Rate: with m bits of storage, k hash functions, and n  1 − e  −kn m  .  10  05101520253000.050.10.150.20.250.30.350.40.450.5
false positive rate  items inserted δ ≈(cid:16)  (cid:17)k  False Positive Rate: with m bits of storage, k hash functions, and n  1 − e  −kn m  .  10  05101520253000.050.10.150.20.250.30.350.40.450.5
false positive rate  items inserted δ ≈(cid:16)  (cid:17)k  False Positive Rate: with m bits of storage, k hash functions, and n  1 − e  −kn m  .  10  05101520253000.050.10.150.20.250.30.350.40.450.5
false positive rate  (cid:17)k  .  items inserted δ ≈(cid:16)  1 − e  −kn m  False Positive Rate: with m bits of storage, k hash functions, and n  • Can diﬀerentiate to show optimal number of hashes is k = ln 2 · m n .  10  05101520253000.050.10.150.20.250.30.350.40.450.5
false positive rate  (cid:17)k  .  items inserted δ ≈(cid:16)  1 − e  −kn m  False Positive Rate: with m bits of storage, k hash functions, and n  • Can diﬀerentiate to show optimal number of hashes is k = ln 2 · m n . • Balances between ﬁlling up the array with too many hashes and having enough hashes so that even when the array is pretty full, a new item is unlikely to have all its bits set (yield a false positive)  10  05101520253000.050.10.150.20.250.30.350.40.450.5
streaming algorithms  Stream Processing: Have a massive dataset X with n items x1, x2, . . . , xn that arrive in a continuous stream. Not nearly enough space to store all the items (in a single location). • Still want to analyze and learn from this data.  11  
streaming algorithms  Stream Processing: Have a massive dataset X with n items x1, x2, . . . , xn that arrive in a continuous stream. Not nearly enough space to store all the items (in a single location). • Still want to analyze and learn from this data. • Typically must compress the data on the ﬂy, storing a data structure from which you can still learn useful information.  11  
streaming algorithms  Stream Processing: Have a massive dataset X with n items x1, x2, . . . , xn that arrive in a continuous stream. Not nearly enough space to store all the items (in a single location). • Still want to analyze and learn from this data. • Typically must compress the data on the ﬂy, storing a data structure from which you can still learn useful information. • Often the compression is randomized. E.g., bloom ﬁlters.  11  
streaming algorithms  Stream Processing: Have a massive dataset X with n items x1, x2, . . . , xn that arrive in a continuous stream. Not nearly enough space to store all the items (in a single location). • Still want to analyze and learn from this data. • Typically must compress the data on the ﬂy, storing a data structure from which you can still learn useful information. • Often the compression is randomized. E.g., bloom ﬁlters. • Compared to traditional algorithm design, which focuses on  minimizing runtime, the big question here is how much space is needed to answer queries of interest.  11  
some examples  • Sensor data: images from telescopes (15 terabytes per night from the  Large Synoptic Survey Telescope), readings from seismometer arrays monitoring and predicting earthquake activity, traﬃc cameras and travel time sensors (Smart Cities), electrical grid monitoring.  12  
some examples  • Sensor data: images from telescopes (15 terabytes per night from the  Large Synoptic Survey Telescope), readings from seismometer arrays monitoring and predicting earthquake activity, traﬃc cameras and travel time sensors (Smart Cities), electrical grid monitoring.  12  
some examples  • Sensor data: images from telescopes (15 terabytes per night from the  Large Synoptic Survey Telescope), readings from seismometer arrays monitoring and predicting earthquake activity, traﬃc cameras and travel time sensors (Smart Cities), electrical grid monitoring.  • Internet Traﬃc: 500 million Tweets per day, 5.6 billion Google searches, billions of ad-clicks and other logs from instrumented webpages, IPs routed by network switches, ...  12  
some examples  • Sensor data: images from telescopes (15 terabytes per night from the  Large Synoptic Survey Telescope), readings from seismometer arrays monitoring and predicting earthquake activity, traﬃc cameras and travel time sensors (Smart Cities), electrical grid monitoring.  • Internet Traﬃc: 500 million Tweets per day, 5.6 billion Google searches, billions of ad-clicks and other logs from instrumented webpages, IPs routed by network switches, ...  • Datasets in Machine Learning: When training e.g. a neural network  on a large dataset (ImageNet with 14 million images), the data is typically processed in a stream due to storage limitations.  12  
some examples  • Sensor data: images from telescopes (15 terabytes per night from the  Large Synoptic Survey Telescope), readings from seismometer arrays monitoring and predicting earthquake activity, traﬃc cameras and travel time sensors (Smart Cities), electrical grid monitoring.  • Internet Traﬃc: 500 million Tweets per day, 5.6 billion Google searches, billions of ad-clicks and other logs from instrumented webpages, IPs routed by network switches, ...  • Datasets in Machine Learning: When training e.g. a neural network  on a large dataset (ImageNet with 14 million images), the data is typically processed in a stream due to storage limitations.  12  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, output the number of distinct elements in the stream.  13  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, output the number of distinct elements in the stream. E.g.,  1, 5, 7, 5, 2, 1 → 4 distinct elements  13  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements in the stream. E.g.,  1, 5, 7, 5, 2, 1 → 4 distinct elements  13  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements in the stream. E.g.,  1, 5, 7, 5, 2, 1 → 4 distinct elements  Applications: • Distinct IP addresses clicking on an ad or visiting a site. • Distinct values in a database column (for estimating sizes of  joins and group bys).  • Number of distinct search engine queries. • Counting distinct motifs in large DNA sequences.  13  
distinct elements  Distinct Elements (Count-Distinct) Problem: Given a stream x1, . . . , xn, estimate the number of distinct elements in the stream. E.g.,  1, 5, 7, 5, 2, 1 → 4 distinct elements  Applications: • Distinct IP addresses clicking on an ad or visiting a site. • Distinct values in a database column (for estimating sizes of  joins and group bys).  • Number of distinct search engine queries. • Counting distinct motifs in large DNA sequences.  Google Sawzall, Facebook Presto, Apache Drill, Twitter Algebird  13  
14  
