compsci 514: algorithms for data science  Andrew McGregor  Lecture 9  0  
1  
locality sensitive hashing  2  
locality sensitive hashing  • MinHash reduces estimating the Jaccard similarity of two sets to  checking equality of a single number. Pr(MinHash(A) = MinHash(B)) = J(A, B) = |A ∩ B|/|A ∪ B|  2  
locality sensitive hashing  • MinHash reduces estimating the Jaccard similarity of two sets to  checking equality of a single number. Pr(MinHash(A) = MinHash(B)) = J(A, B) = |A ∩ B|/|A ∪ B| • An instance of locality sensitive hashing (LSH), i.e., where the collision probability is higher when two inputs are more similar.  2  
locality sensitive hashing  • MinHash reduces estimating the Jaccard similarity of two sets to  checking equality of a single number. Pr(MinHash(A) = MinHash(B)) = J(A, B) = |A ∩ B|/|A ∪ B| • An instance of locality sensitive hashing (LSH), i.e., where the collision probability is higher when two inputs are more similar.  2  
locality sensitive hashing  • MinHash reduces estimating the Jaccard similarity of two sets to  checking equality of a single number. Pr(MinHash(A) = MinHash(B)) = J(A, B) = |A ∩ B|/|A ∪ B| • An instance of locality sensitive hashing (LSH), i.e., where the collision probability is higher when two inputs are more similar.  2  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  3  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach:  3  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function  g : [0, 1] → [m], and insert x into bucket g(MinHash(x)). Search for items similar to y in bucket g(MinHash(y )).  3  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function  g : [0, 1] → [m], and insert x into bucket g(MinHash(x)). Search for items similar to y in bucket g(MinHash(y )). • What is Pr [g(MinHash(x)) = g(MinHash(y ))] assuming  J(x, y ) ≤ 1/2 and g is collision free?  3  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function  g : [0, 1] → [m], and insert x into bucket g(MinHash(x)). Search for items similar to y in bucket g(MinHash(y )). • What is Pr [g(MinHash(x)) = g(MinHash(y ))] assuming  J(x, y ) ≤ 1/2 and g is collision free? At most 1/2  3  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function  g : [0, 1] → [m], and insert x into bucket g(MinHash(x)). Search for items similar to y in bucket g(MinHash(y )). • What is Pr [g(MinHash(x)) = g(MinHash(y ))] assuming  J(x, y ) ≤ 1/2 and g is collision free? At most 1/2  • For each document x in your database with J(x, y ) ≥ 1/2 what  is the probability you will ﬁnd x in bucket g(MinHash(y ))?  3  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function  g : [0, 1] → [m], and insert x into bucket g(MinHash(x)). Search for items similar to y in bucket g(MinHash(y )). • What is Pr [g(MinHash(x)) = g(MinHash(y ))] assuming  J(x, y ) ≤ 1/2 and g is collision free? At most 1/2  • For each document x in your database with J(x, y ) ≥ 1/2 what is the probability you will ﬁnd x in bucket g(MinHash(y ))? At least 1/2  3  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables.  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions?  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions? 1− (probability in no buckets)  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  1− (probability in no buckets) = 1 −(cid:0) 1  (cid:1)t  these buckets, assuming for simplicity g has no collisions?  2  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  1− (probability in no buckets) = 1 −(cid:0) 1  (cid:1)t ≈ .99 for t = 7.  these buckets, assuming for simplicity g has no collisions?  2  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  1− (probability in no buckets) = 1 −(cid:0) 1  (cid:1)t ≈ .99 for t = 7.  these buckets, assuming for simplicity g has no collisions?  • What is the probability that x with J(x, y ) = 1/4 is in at least one of  2  these buckets, assuming for simplicity g has no collisions?  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions?  1− (probability in no buckets) = 1 −(cid:0) 1 1− (probability in no buckets) = 1 −(cid:0) 3  2  (cid:1)t ≈ .99 for t = 7. (cid:1)t  these buckets, assuming for simplicity g has no collisions?  • What is the probability that x with J(x, y ) = 1/4 is in at least one of  4  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions?  1− (probability in no buckets) = 1 −(cid:0) 1 1− (probability in no buckets) = 1 −(cid:0) 3  (cid:1)t ≈ .99 for t = 7. (cid:1)t ≈ .87 for t = 7.  these buckets, assuming for simplicity g has no collisions?  • What is the probability that x with J(x, y ) = 1/4 is in at least one of  2  4  4  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions?  1− (probability in no buckets) = 1 −(cid:0) 1 1− (probability in no buckets) = 1 −(cid:0) 3  (cid:1)t ≈ .99 for t = 7. (cid:1)t ≈ .87 for t = 7.  these buckets, assuming for simplicity g has no collisions?  • What is the probability that x with J(x, y ) = 1/4 is in at least one of  2  4  Potential for a lot of false positives! Slows down search time.  4  
5  
balancing hit rate and query time  We want to balance a small probability of false negatives (a high hit rate) with a small probability of false positives (a small query time.)  6  
balancing hit rate and query time  We want to balance a small probability of false negatives (a high hit rate) with a small probability of false positives (a small query time.)  Create t hash tables. Each is indexed into not with a single MinHash value, but with r values, appended together. A length r signature.  6  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s:  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )]  .  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r .  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r .  • Probability that x and y don’t match in repetition i:  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r .  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r . • Probability that x and y don’t match in all repetitions:  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r . • Probability that x and y don’t match in all repetitions: (1 − s r )t.  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r . • Probability that x and y don’t match in all repetitions: (1 − s r )t. • Probability that x and y match in at least one repetition:  7  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r . • Probability that x and y don’t match in all repetitions: (1 − s r )t. • Probability that x and y match in at least one repetition:  Hit Probability: 1 − (1 − s r )t.  7  
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  8  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 5, t = 10
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  8  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 5, t = 10
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  8  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 10, t = 10
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  8  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 5, t = 30
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  r and t are tuned depending on application. ‘Threshold’ when hit probability is 1/2 is ≈ (1/t)1/r . E.g., ≈ (1/30)1/5 = .51 in this case.  8  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 5, t = 30
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9.  9  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  9  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  With signature length r = 25 and repetitions t = 50, hit probability for J(x, y ) = s is 1 − (1 − s 25)50.  9  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  With signature length r = 25 and repetitions t = 50, hit probability for J(x, y ) = s is 1 − (1 − s 25)50. • Hit probability for J(x, y ) ≥ .9 is ≥ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ∈ [.7, .9] is ≤ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ≤ .7 is ≤ 1 − (1 − .720)40 ≈ .007  9  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  With signature length r = 25 and repetitions t = 50, hit probability for J(x, y ) = s is 1 − (1 − s 25)50. • Hit probability for J(x, y ) ≥ .9 is ≥ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ∈ [.7, .9] is ≤ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ≤ .7 is ≤ 1 − (1 − .720)40 ≈ .007  Expected Number of Items Scanned: (proportional to query time)  ≤ 10 + .98 ∗ 10, 000 + .007 ∗ 9, 989, 990 ≈ 80, 000  9  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  With signature length r = 25 and repetitions t = 50, hit probability for J(x, y ) = s is 1 − (1 − s 25)50. • Hit probability for J(x, y ) ≥ .9 is ≥ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ∈ [.7, .9] is ≤ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ≤ .7 is ≤ 1 − (1 − .720)40 ≈ .007  Expected Number of Items Scanned: (proportional to query time) ≤ 10 + .98 ∗ 10, 000 + .007 ∗ 9, 989, 990 ≈ 80, 000 (cid:28) 10, 000, 000.  9  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with any similarity metric, given a locality sensitive hash function for that metric.  10  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with any similarity metric, given a locality sensitive hash function for that metric. • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  10  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with any similarity metric, given a locality sensitive hash function for that metric. • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  10  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with any similarity metric, given a locality sensitive hash function for that metric. • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  10  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with any similarity metric, given a locality sensitive hash function for that metric. • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  Cosine Similarity: cos(θ(x, y ))  10  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with any similarity metric, given a locality sensitive hash function for that metric. • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  Cosine Similarity: cos(θ(x, y )) • cos(θ(x, y )) = 1 when θ(x, y ) = 0◦ and cos(θ(x, y )) = 0 when  θ(x, y ) = 90◦, and cos(θ(x, y )) = −1 when θ(x, y ) = 180◦  10  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with any similarity metric, given a locality sensitive hash function for that metric. • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  Cosine Similarity: cos(θ(x, y )) = • cos(θ(x, y )) = 1 when θ(x, y ) = 0◦ and cos(θ(x, y )) = 0 when  θ(x, y ) = 90◦, and cos(θ(x, y )) = −1 when θ(x, y ) = 180◦  (cid:104)x,y(cid:105)  (cid:107)x(cid:107)2·(cid:107)y(cid:107)2  .  10  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  11  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  11  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  11  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  11  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  SimHash(x) = sign((cid:104)x, t(cid:105)) for a random vector t.  11  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  SimHash(x) = sign((cid:104)x, t(cid:105)) for a random vector t.  What is Pr [SimHash(x) = SimHash(y )]?  11  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]?  12  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]? SimHash(x) (cid:54)= SimHash(y ) when the plane separates x from y .  12  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]? SimHash(x) (cid:54)= SimHash(y ) when the plane separates x from y .  12  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]? SimHash(x) (cid:54)= SimHash(y ) when the plane separates x from y .  • Pr [SimHash(x) (cid:54)= SimHash(y )] = θ(x,y )  180  12  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]? SimHash(x) (cid:54)= SimHash(y ) when the plane separates x from y .  • Pr [SimHash(x) (cid:54)= SimHash(y )] = θ(x,y ) • Pr [SimHash(x) = SimHash(y )] = 1 − θ(x,y )  180  180  12  
Questions on MinHash and Locality Sensitive Hashing?  13  
