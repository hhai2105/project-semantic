compsci 514: algorithms for data science  Andrew McGregor  Lecture 7  0  
distinct elements recap  • Estimate # distinct elements d in stream x1, . . . , xn ∈ U  1  
distinct elements recap  • Estimate # distinct elements d in stream x1, . . . , xn ∈ U • Basic Algorithm: • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions. • Compute s = 1 • Return ˆd = 1/s − 1  (cid:80) si where si = minj∈[n] hi (xj )  k  1  
distinct elements recap  • Estimate # distinct elements d in stream x1, . . . , xn ∈ U • Basic Algorithm: • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions. • Compute s = 1 • Return ˆd = 1/s − 1 • Analysis: E[s] = 1  (cid:80) si where si = minj∈[n] hi (xj ) d+1 and Var[s] ≤  k = O(−2) ensures (1 − )E[s] ≤ s ≤ (1 + )E[s] with probability at least 3/4 and  k  1  k(d+1)2 . Setting  (1 − )E[s] ≤ s ≤ (1 + )E[s] ⇒ (1 − 4)d ≤ ˆd ≤ (1 + 4)d  1  
distinct elements recap  • Estimate # distinct elements d in stream x1, . . . , xn ∈ U • Basic Algorithm: • Let h1, h2, . . . , hk : U → [0, 1] be random hash functions. • Compute s = 1 • Return ˆd = 1/s − 1 • Analysis: E[s] = 1  (cid:80) si where si = minj∈[n] hi (xj ) d+1 and Var[s] ≤  k = O(−2) ensures (1 − )E[s] ≤ s ≤ (1 + )E[s] with probability at least 3/4 and  k  1  k(d+1)2 . Setting  (1 − )E[s] ≤ s ≤ (1 + )E[s] ⇒ (1 − 4)d ≤ ˆd ≤ (1 + 4)d • Median Trick: If an algorithm returns a suﬃciently accurate  numerical answer with probability at least 3/4, run it O(log(1/δ)) times and take the median answer. This will have the required accuracy with probability at least 1 − δ.  1  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions.  2  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented...  2  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  2  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  • Flajolet-Martin (LogLog) algorithm and HyperLogLog.  2  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  • Flajolet-Martin (LogLog) algorithm and HyperLogLog.  2  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  • Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  2  
distinct elements in practice  Our algorithm uses continuous valued fully random hash functions. Can’t be implemented... • The idea of using the minimum hash value of x1, . . . , xn to  estimate the number of distinct elements naturally extends to when the hash functions map to discrete values.  • Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m. The more distinct hashes we see, the higher we expect this maximum to be.  2  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  a) O(1)  b) O(log d)  √ c) O(  d)  d) O(d)  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  a) O(1)  b) O(log d)  √ c) O(  d)  d) O(d)  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has x trailing zeros) =  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has x trailing zeros) =  1 2x  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1  2log d  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  So with d distinct hashes, expect to see 1 with log d trailing zeros. Expect m ≈ log d.  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  So with d distinct hashes, expect to see 1 with log d trailing zeros. Expect m ≈ log d. m takes log log d bits to store.  3  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  So with d distinct hashes, expect to see 1 with log d trailing zeros. Expect m ≈ log d. m takes log log d bits to store.  for an  approximate count.  3  (cid:16) log log d  (cid:17)  Total Space: O  2 + log d  
loglog counting of distinct elements  Flajolet-Martin (LogLog) algorithm and HyperLogLog.  Estimate # distinct elements based on maximum number of trailing zeros m.  With d distinct elements, roughly what do we expect m to be?  Pr(h(xi ) has log d trailing zeros) =  1 2log d =  1 d  .  So with d distinct hashes, expect to see 1 with log d trailing zeros. Expect m ≈ log d. m takes log log d bits to store.  (cid:16) log log d  (cid:17)  Total Space: O  2 + log d  for an  approximate count.  Note: Careful averaging of estimates from multiple hash functions.  3  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  space used = O  + log d  (cid:18) log log d  2  (cid:19)  4  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  + log d 1.04 · (cid:100)log2 log2 d(cid:101)  2  =  2  + (cid:100)log2 d(cid:101) bits1  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  4  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  4  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  Mergeable Sketch: Consider the case (essentially always in practice) that the items are processed on diﬀerent machines.  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  4  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  Mergeable Sketch: Consider the case (essentially always in practice) that the items are processed on diﬀerent machines. • Given data structures (sketches) HLL(x1, . . . , xn), HLL(y1, . . . , yn) it is  easy to merge them to give HLL(x1, . . . , xn, y1, . . . , yn).  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  4  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  Mergeable Sketch: Consider the case (essentially always in practice) that the items are processed on diﬀerent machines. • Given data structures (sketches) HLL(x1, . . . , xn), HLL(y1, . . . , yn) it is  easy to merge them to give HLL(x1, . . . , xn, y1, . . . , yn). How?  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  4  
loglog space guarantees  Using HyperLogLog to count 1 billion distinct items with 2% accuracy:  (cid:18) log log d  (cid:19)  space used = O  2  + log d 1.04 · (cid:100)log2 log2 d(cid:101) + (cid:100)log2 d(cid:101) bits1 1.04 · 5 .022 + 30 = 13030 bits ≈ 1.6 kB!  2  =  =  Mergeable Sketch: Consider the case (essentially always in practice) that the items are processed on diﬀerent machines. • Given data structures (sketches) HLL(x1, . . . , xn), HLL(y1, . . . , yn) it is  easy to merge them to give HLL(x1, . . . , xn, y1, . . . , yn). How?  • Set the maximum # of trailing zeros to the maximum in the two  sketches.  1. 1.04 is the constant in the HyperLogLog analysis. Not important!  4  
Questions on distinct elements counting?  5  
another fundamental problem  Jaccard Index: A similarity measure between two sets.  J(A, B) =  |A ∩ B| |A ∪ B| =  # shared elements # total elements  .  Natural measure for similarity between bit strings – interpret an n bit string as a set, containing the elements corresponding the positions of its ones. J(x, y ) = # shared ones  .  total ones  6  
search with jaccard similarity  J(A, B) =  |A ∩ B| |A ∪ B| =  # shared elements # total elements  .  Want Fast Implementations For:  7  
search with jaccard similarity  J(A, B) =  |A ∩ B| |A ∪ B| =  # shared elements # total elements  .  Want Fast Implementations For: • Near Neighbor Search: Have a database of n sets/bit strings and given a set A, want to ﬁnd if it has high Jaccard similarity to anything in the database. Ω(n) time with a linear scan.  7  
search with jaccard similarity  J(A, B) =  |A ∩ B| |A ∪ B| =  # shared elements # total elements  .  Want Fast Implementations For: • Near Neighbor Search: Have a database of n sets/bit strings and given a set A, want to ﬁnd if it has high Jaccard similarity to anything in the database. Ω(n) time with a linear scan.  • All-pairs Similarity Search: Have n diﬀerent sets/bit strings and want to ﬁnd all pairs with high Jaccard similarity. Ω(n2) time if we check all pairs explicitly.  Will speed up via randomized locality sensitive hashing.  7  
applications  Document Similarity: • E.g., detecting plagiarism, copyright infringement, spam. • Use Shingling + Jaccard similarity.  8  
applications  Document Similarity: • E.g., detecting plagiarism, copyright infringement, spam. • Use Shingling + Jaccard similarity. (n-grams, k-mers)  8  
application: collaborative filtering  Online recommendation systems are often based on collaborative ﬁltering. Simplest approach: ﬁnd similar users and make recommendations based on those users.  9  
application: collaborative filtering  Online recommendation systems are often based on collaborative ﬁltering. Simplest approach: ﬁnd similar users and make recommendations based on those users.  • Twitter: represent a user as the set of accounts they follow. Match  similar users based on the Jaccard similarity of these sets. Recommend that you follow accounts followed by similar users. Netﬂix: look at sets of movies watched. Amazon: look at products purchased, etc.  9  
application: entity resolution  Entity Resolution Problem: Want to combine records from multiple data sources that refer to the same entities.  10  
application: entity resolution  Entity Resolution Problem: Want to combine records from multiple data sources that refer to the same entities. • E.g. data on individuals from voting registrations, property records, and social media accounts. Names and addresses may not exactly match, due to typos, nicknames, moves, etc.  • Still want to match records that all refer to the same person using all  pairs similarity search.  10  
application: entity resolution  Entity Resolution Problem: Want to combine records from multiple data sources that refer to the same entities. • E.g. data on individuals from voting registrations, property records, and social media accounts. Names and addresses may not exactly match, due to typos, nicknames, moves, etc.  • Still want to match records that all refer to the same person using all  pairs similarity search.  See Section 3.8.2 of Mining Massive Datasets for a discussion of a real world example involving 1 million customers. Naively this would be  (cid:1) ≈ 500 billion pairs of customers to check!  (cid:0)1000000  2  10  
application: spam and fraud detection  Many applications to spam/fraud detection. E.g.  11  
application: spam and fraud detection  Many applications to spam/fraud detection. E.g. • Fake Reviews: Very common on websites like Amazon.  Detection often looks for (near) duplicate reviews on similar products, which have been copied. ‘Near duplicate’ measured with shingles + Jaccard similarity.  11  
application: spam and fraud detection  Many applications to spam/fraud detection. E.g. • Fake Reviews: Very common on websites like Amazon.  Detection often looks for (near) duplicate reviews on similar products, which have been copied. ‘Near duplicate’ measured with shingles + Jaccard similarity.  • Lateral phishing: Phishing emails sent to addresses at a  business coming from a legitimate email address at the same business that has been compromised. • One method of detection looks at the recipient list of an email and checks if it has small Jaccard similarity with any previous recipient lists. If not, the email is ﬂagged as possible spam.  11  
minhashing  Goal: Speed up Jaccard similarity search (near neighbor and all-pairs similarity search).  12  
minhashing  Goal: Speed up Jaccard similarity search (near neighbor and all-pairs similarity search).  Strategy: Use random hashing to map each set to a very compressed representation. Jaccard similarity can be estimated from these representations.  12  
minhashing  Goal: Speed up Jaccard similarity search (near neighbor and all-pairs similarity search).  Strategy: Use random hashing to map each set to a very compressed representation. Jaccard similarity can be estimated from these representations.  MinHash(A): [Andrei Broder, 1997 at Altavista]  • Let h : U → [0, 1] be a random  hash function  • s := 1 • For x1, . . . , x|A| ∈ A • s := min(s, h(xk ))  • Return s  12  
minhashing  Goal: Speed up Jaccard similarity search (near neighbor and all-pairs similarity search).  Strategy: Use random hashing to map each set to a very compressed representation. Jaccard similarity can be estimated from these representations.  MinHash(A): [Andrei Broder, 1997 at Altavista]  • Let h : U → [0, 1] be a random  hash function  • s := 1 • For x1, . . . , x|A| ∈ A • s := min(s, h(xk ))  • Return s  12  
minhashing  Goal: Speed up Jaccard similarity search (near neighbor and all-pairs similarity search).  Strategy: Use random hashing to map each set to a very compressed representation. Jaccard similarity can be estimated from these representations.  MinHash(A): [Andrei Broder, 1997 at Altavista]  • Let h : U → [0, 1] be a random  hash function  • s := 1 • For x1, . . . , x|A| ∈ A • s := min(s, h(xk ))  • Return s  Identical to our distinct elements sketch!  12  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))?  13  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions)  13  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions)  13  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions)  13  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions)  13  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions)  13  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions)  • MinHash(A) = MinHash(B) only if an item in A ∩ B has the  minimum hash value in both sets.  13  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? Claim: MinHash(A) = MinHash(B) only if an item in A ∩ B has the minimum hash value in both sets.  14  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? Claim: MinHash(A) = MinHash(B) only if an item in A ∩ B has the minimum hash value in both sets.  Pr(MinHash(A) = MinHash(B)) =?  14  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? Claim: MinHash(A) = MinHash(B) only if an item in A ∩ B has the minimum hash value in both sets.  Pr(MinHash(A) = MinHash(B)) =  |A ∩ B|  total # items hashed  14  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? Claim: MinHash(A) = MinHash(B) only if an item in A ∩ B has the minimum hash value in both sets.  Pr(MinHash(A) = MinHash(B)) =  =  |A ∩ B|  total # items hashed |A ∩ B| |A ∪ B|  14  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? Claim: MinHash(A) = MinHash(B) only if an item in A ∩ B has the minimum hash value in both sets.  Pr(MinHash(A) = MinHash(B)) =  =  |A ∩ B|  total # items hashed |A ∩ B| |A ∪ B| = J(A, B).  14  
