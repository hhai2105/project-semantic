compsci 514: algorithms for data science  Andrew McGregor  Lecture 8  0  
search with jaccard similarity  Jaccard Index: A similarity measure between two sets.  J(A, B) =  |A ∩ B| |A ∪ B| =  # shared elements # total elements  .  Want Fast Implementations For:  1  
search with jaccard similarity  Jaccard Index: A similarity measure between two sets.  J(A, B) =  |A ∩ B| |A ∪ B| =  # shared elements # total elements  .  Want Fast Implementations For: • Near Neighbor Search: Have a database of n sets and given a set A, want to ﬁnd if it has high Jaccard similarity to anything in the database. Ω(n) time with a linear scan.  1  
search with jaccard similarity  Jaccard Index: A similarity measure between two sets.  J(A, B) =  |A ∩ B| |A ∪ B| =  # shared elements # total elements  .  Want Fast Implementations For: • Near Neighbor Search: Have a database of n sets and given a set A, want to ﬁnd if it has high Jaccard similarity to anything in the database. Ω(n) time with a linear scan.  • All-pairs Similarity Search: Have n diﬀerent sets and want to ﬁnd all pairs with high Jaccard similarity. Ω(n2) time if we check all pairs explicitly.  Will speed up via randomized locality sensitive hashing.  1  
minhashing  Goal: Speed up Jaccard similarity search.  2  
minhashing  Goal: Speed up Jaccard similarity search.  Strategy: Use random hashing to map each set to a very compressed representation. Jaccard similarity can be estimated from these.  2  
minhashing  Goal: Speed up Jaccard similarity search.  Strategy: Use random hashing to map each set to a very compressed representation. Jaccard similarity can be estimated from these.  MinHash(A): [Andrei Broder, 1997 at Altavista]  • Let h : U → [0, 1] be a random  hash function  • s := 1 • For x1, . . . , x|A| ∈ A • s := min(s, h(xk ))  • Return s  2  
minhashing  Goal: Speed up Jaccard similarity search.  Strategy: Use random hashing to map each set to a very compressed representation. Jaccard similarity can be estimated from these.  MinHash(A): [Andrei Broder, 1997 at Altavista]  • Let h : U → [0, 1] be a random  hash function  • s := 1 • For x1, . . . , x|A| ∈ A • s := min(s, h(xk ))  • Return s  2  
minhashing  Goal: Speed up Jaccard similarity search.  Strategy: Use random hashing to map each set to a very compressed representation. Jaccard similarity can be estimated from these.  MinHash(A): [Andrei Broder, 1997 at Altavista]  • Let h : U → [0, 1] be a random  hash function  • s := 1 • For x1, . . . , x|A| ∈ A • s := min(s, h(xk ))  • Return s  Identical to our distinct elements sketch!  2  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))?  3  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions)  3  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions) • MH(A) = MH(B) iﬀ an item in A ∩ B has the minimum hash  value in both sets. Therefore,  3  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions) • MH(A) = MH(B) iﬀ an item in A ∩ B has the minimum hash  value in both sets. Therefore,  Pr(MH(A) = MH(B)) =  (cid:88)  x∈A∩B  Pr(MH(A) = h(x) ∩ MH(B) = h(x))  3  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions) • MH(A) = MH(B) iﬀ an item in A ∩ B has the minimum hash  value in both sets. Therefore,  (cid:88) (cid:88)  x∈A∩B  Pr(MH(A) = MH(B)) =  =  x∈A∩B  Pr(MH(A) = h(x) ∩ MH(B) = h(x))  Pr(x = arg min y∈A∪B  h(y ))  3  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions) • MH(A) = MH(B) iﬀ an item in A ∩ B has the minimum hash  value in both sets. Therefore,  Pr(MH(A) = MH(B)) =  =  =  x∈A∩B  x∈A∩B  (cid:88) (cid:88) (cid:88)  x∈A∩B  Pr(MH(A) = h(x) ∩ MH(B) = h(x))  Pr(x = arg min y∈A∪B  h(y ))  1  |A ∪ B|  3  
minhash  For two sets A and B, what is Pr(MinHash(A) = MinHash(B))? • Since we are hashing into the continuous range [0, 1], we will never have h(x) = h(y ) for x (cid:54)= y (i.e., no spurious collisions) • MH(A) = MH(B) iﬀ an item in A ∩ B has the minimum hash  value in both sets. Therefore,  Pr(MH(A) = h(x) ∩ MH(B) = h(x))  Pr(x = arg min y∈A∪B  h(y ))  Pr(MH(A) = MH(B)) =  =  =  =  (cid:88) (cid:88) (cid:88)  x∈A∩B  x∈A∩B  1  |A ∪ B|  x∈A∩B |A ∩ B| |A ∪ B| = J(A, B)  3  
locality sensitive hashing  Upshot: MinHash reduces estimating the Jaccard similarity to checking equality of a single number.  Pr(MinHash(A) = MinHash(B)) = J(A, B).  4  
locality sensitive hashing  Upshot: MinHash reduces estimating the Jaccard similarity to checking equality of a single number.  Pr(MinHash(A) = MinHash(B)) = J(A, B).  • An instance of locality sensitive hashing (LSH).  4  
locality sensitive hashing  Upshot: MinHash reduces estimating the Jaccard similarity to checking equality of a single number.  Pr(MinHash(A) = MinHash(B)) = J(A, B).  • An instance of locality sensitive hashing (LSH). • A hash function where the collision probability is higher when two inputs are more similar (can design diﬀerent functions for diﬀerent similarity metrics.)  4  
locality sensitive hashing  Upshot: MinHash reduces estimating the Jaccard similarity to checking equality of a single number.  Pr(MinHash(A) = MinHash(B)) = J(A, B).  • An instance of locality sensitive hashing (LSH). • A hash function where the collision probability is higher when two inputs are more similar (can design diﬀerent functions for diﬀerent similarity metrics.)  4  
locality sensitive hashing  Upshot: MinHash reduces estimating the Jaccard similarity to checking equality of a single number.  Pr(MinHash(A) = MinHash(B)) = J(A, B).  • An instance of locality sensitive hashing (LSH). • A hash function where the collision probability is higher when two inputs are more similar (can design diﬀerent functions for diﬀerent similarity metrics.)  4  
lsh for similarity search  How does locality sensitive hashing help for similarity search?  5  
lsh for similarity search  How does locality sensitive hashing help for similarity search?  • Near Neighbor Search: Given item x, compute h(x). Only search for similar items in the h(x) bucket of the hash table.  5  
lsh for similarity search  How does locality sensitive hashing help for similarity search?  • Near Neighbor Search: Given item x, compute h(x). Only search for similar items in the h(x) bucket of the hash table. • All-pairs Similarity Search: Scan through all buckets of the  hash table and look for similar pairs within each bucket.  5  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  6  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach:  6  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function g : [0, 1] → [m], and insert each item x into bucket g(MH(x)). Search for items similar to y in bucket g(MH(y )).  6  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function g : [0, 1] → [m], and insert each item x into bucket g(MH(x)). Search for items similar to y in bucket g(MH(y )).  • What is Pr [g(MH(x)) = g(MH(y ))] assuming J(x, y ) ≤ 1/2  and g is collision free?  6  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function g : [0, 1] → [m], and insert each item x into bucket g(MH(x)). Search for items similar to y in bucket g(MH(y )).  • What is Pr [g(MH(x)) = g(MH(y ))] assuming J(x, y ) ≤ 1/2  and g is collision free? At most 1/2  6  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function g : [0, 1] → [m], and insert each item x into bucket g(MH(x)). Search for items similar to y in bucket g(MH(y )).  • What is Pr [g(MH(x)) = g(MH(y ))] assuming J(x, y ) ≤ 1/2  and g is collision free? At most 1/2  • For each document x in your database with J(x, y ) ≥ 1/2 what  is the probability you will ﬁnd x in bucket g(MH(y ))?  6  
lsh with minhash  Goal: Given a document y , identify all documents x in a database with Jaccard similarity (of their shingle sets) J(x, y ) ≥ 1/2.  Our Approach: • Create a hash table of size m, choose a random hash function g : [0, 1] → [m], and insert each item x into bucket g(MH(x)). Search for items similar to y in bucket g(MH(y )).  • What is Pr [g(MH(x)) = g(MH(y ))] assuming J(x, y ) ≤ 1/2  and g is collision free? At most 1/2  • For each document x in your database with J(x, y ) ≥ 1/2 what is the probability you will ﬁnd x in bucket g(MH(y ))? At least 1/2  6  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables.  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions?  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions? 1− (probability in no buckets)  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  1− (probability in no buckets) = 1 −(cid:0) 1  (cid:1)t  these buckets, assuming for simplicity g has no collisions?  2  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  1− (probability in no buckets) = 1 −(cid:0) 1  (cid:1)t ≈ .99 for t = 7.  these buckets, assuming for simplicity g has no collisions?  2  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  1− (probability in no buckets) = 1 −(cid:0) 1  (cid:1)t ≈ .99 for t = 7.  these buckets, assuming for simplicity g has no collisions?  • What is the probability that x with J(x, y ) = 1/4 is in at least one of  2  these buckets, assuming for simplicity g has no collisions?  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions?  1− (probability in no buckets) = 1 −(cid:0) 1 1− (probability in no buckets) = 1 −(cid:0) 3  2  (cid:1)t ≈ .99 for t = 7. (cid:1)t  these buckets, assuming for simplicity g has no collisions?  • What is the probability that x with J(x, y ) = 1/4 is in at least one of  4  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions?  1− (probability in no buckets) = 1 −(cid:0) 1 1− (probability in no buckets) = 1 −(cid:0) 3  (cid:1)t ≈ .99 for t = 7. (cid:1)t ≈ .87 for t = 7.  these buckets, assuming for simplicity g has no collisions?  • What is the probability that x with J(x, y ) = 1/4 is in at least one of  2  4  7  
reducing false negatives  With a simple use of MinHash, we miss a match x with J(x, y ) = 1/2 with probability 1/2. How can we reduce this false negative rate?  Repetition: Run MinHash t times independently, to produce hash values MH1(x), . . . , MHt(x). Apply random hash function g to map all these values to locations in t hash tables. • To search for items similar to y , look at all items in bucket g(MH1(y ))  of the 1st table, bucket g(MH2(y )) of the 2nd table, etc.  • What is the probability that x with J(x, y ) = 1/2 is in at least one of  these buckets, assuming for simplicity g has no collisions?  1− (probability in no buckets) = 1 −(cid:0) 1 1− (probability in no buckets) = 1 −(cid:0) 3  (cid:1)t ≈ .99 for t = 7. (cid:1)t ≈ .87 for t = 7.  these buckets, assuming for simplicity g has no collisions?  • What is the probability that x with J(x, y ) = 1/4 is in at least one of  2  4  Potential for a lot of false positives! Slows down search time.  7  
balancing hit rate and query time  We want to balance a small probability of false negatives (a high hit rate) with a small probability of false positives (a small query time.)  8  
balancing hit rate and query time  We want to balance a small probability of false negatives (a high hit rate) with a small probability of false positives (a small query time.)  Create t hash tables. Each is indexed into not with a single MinHash value, but with r values, appended together. A length r signature.  8  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s:  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )]  .  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r .  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r .  • Probability that x and y don’t match in repetition i:  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r .  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r . • Probability that x and y don’t match in all repetitions:  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r . • Probability that x and y don’t match in all repetitions: (1 − s r )t.  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r . • Probability that x and y don’t match in all repetitions: (1 − s r )t. • Probability that x and y match in at least one repetition:  9  
balancing hit rate and query time  Consider searching for matches in t hash tables, using MinHash signatures of length r . For x and y with Jaccard similarity J(x, y ) = s: • Probability that a single hash matches. Pr [MHi,j (x) = MHi,j (y )] = J(x, y ) = s.  • Probability that x and y having matching signatures in repetition i.  Pr [MHi,1(x), . . . , MHi,r (x) = MHi,1(y ), . . . , MHi,r (y )] = s r . • Probability that x and y don’t match in repetition i: 1 − s r . • Probability that x and y don’t match in all repetitions: (1 − s r )t. • Probability that x and y match in at least one repetition:  Hit Probability: 1 − (1 − s r )t.  9  
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  10  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 5, t = 10
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  10  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 5, t = 10
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  10  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 10, t = 10
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  10  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 5, t = 30
the s-curve  Using t repetitions each with a signature of r MinHash values, the probability that x and y with Jaccard similarity J(x, y ) = s match in at least one repetition is: 1 − (1 − s r )t.  r and t are tuned depending on application. ‘Threshold’ when hit probability is 1/2 is ≈ (1/t)1/r . E.g., ≈ (1/30)1/5 = .51 in this case.  10  00.20.40.60.81Jaccard Similarity s00.10.20.30.40.50.60.70.80.91Hit Probabilityr = 5, t = 30
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9.  11  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  11  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  With signature length r = 25 and repetitions t = 50, hit probability for J(x, y ) = s is 1 − (1 − s 25)50.  11  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  With signature length r = 25 and repetitions t = 50, hit probability for J(x, y ) = s is 1 − (1 − s 25)50. • Hit probability for J(x, y ) ≥ .9 is ≥ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ∈ [.7, .9] is ≤ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ≤ .7 is ≤ 1 − (1 − .720)40 ≈ .007  11  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  With signature length r = 25 and repetitions t = 50, hit probability for J(x, y ) = s is 1 − (1 − s 25)50. • Hit probability for J(x, y ) ≥ .9 is ≥ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ∈ [.7, .9] is ≤ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ≤ .7 is ≤ 1 − (1 − .720)40 ≈ .007  Expected Number of Items Scanned: (proportional to query time)  ≤ 10 + .98 ∗ 10, 000 + .007 ∗ 9, 989, 990 ≈ 80, 000  11  
s-curve example  For example: Consider a database with 10, 000, 000 audio clips. You are given a clip x and want to ﬁnd any y in the database with J(x, y ) ≥ .9. • There are 10 true matches in the database with J(x, y ) ≥ .9. • There are 10, 000 near matches with J(x, y ) ∈ [.7, .9].  With signature length r = 25 and repetitions t = 50, hit probability for J(x, y ) = s is 1 − (1 − s 25)50. • Hit probability for J(x, y ) ≥ .9 is ≥ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ∈ [.7, .9] is ≤ 1 − (1 − .920)40 ≈ .98 • Hit probability for J(x, y ) ≤ .7 is ≤ 1 − (1 − .720)40 ≈ .007  Expected Number of Items Scanned: (proportional to query time) ≤ 10 + .98 ∗ 10, 000 + .007 ∗ 9, 989, 990 ≈ 80, 000 (cid:28) 10, 000, 000.  11  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with other similarity metrics:  12  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with other similarity metrics: • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  12  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with other similarity metrics: • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  12  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with other similarity metrics: • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  12  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with other similarity metrics: • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  Cosine Similarity: cos(θ(x, y ))  12  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with other similarity metrics: • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  Cosine Similarity: cos(θ(x, y )) • cos(θ(x, y )) = 1 when θ(x, y ) = 0◦ and cos(θ(x, y )) = 0 when  θ(x, y ) = 90◦, and cos(θ(x, y )) = −1 when θ(x, y ) = 180◦  12  
generalizing locality sensitive hashing  Repetition and s-curve tuning can be used for fast similarity search with other similarity metrics: • LSH schemes exist for many similarity/distance measures: hamming  distance, cosine similarity, etc.  Cosine Similarity: cos(θ(x, y )) = • cos(θ(x, y )) = 1 when θ(x, y ) = 0◦ and cos(θ(x, y )) = 0 when  θ(x, y ) = 90◦, and cos(θ(x, y )) = −1 when θ(x, y ) = 180◦  (cid:104)x,y(cid:105)  (cid:107)x(cid:107)2·(cid:107)y(cid:107)2  .  12  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  13  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  13  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  13  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  13  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  SimHash(x) = sign((cid:104)x, t(cid:105)) for a random vector t.  13  
simhash for cosine similarity  SimHash Algorithm: LSH for cosine similarity.  SimHash(x) = sign((cid:104)x, t(cid:105)) for a random vector t.  What is Pr [SimHash(x) = SimHash(y )]?  13  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]?  14  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]? SimHash(x) (cid:54)= SimHash(y ) when the plane separates x from y .  14  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]? SimHash(x) (cid:54)= SimHash(y ) when the plane separates x from y .  14  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]? SimHash(x) (cid:54)= SimHash(y ) when the plane separates x from y .  • Pr [SimHash(x) (cid:54)= SimHash(y )] = θ(x,y )  180  14  
simhash for cosine similarity  What is Pr [SimHash(x) = SimHash(y )]? SimHash(x) (cid:54)= SimHash(y ) when the plane separates x from y .  • Pr [SimHash(x) (cid:54)= SimHash(y )] = θ(x,y ) • Pr [SimHash(x) = SimHash(y )] = 1 − θ(x,y )  180  180  14  
Questions on MinHash and Locality Sensitive Hashing?  15  
