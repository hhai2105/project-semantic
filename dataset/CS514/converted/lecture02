compsci 514: algorithms for data science  Andrew McGregor  Lecture 2  0  
today  Today: • Investigate linearity of expectation and variance. • Algorithmic application of linearity of expectation and variance. • Introduce Markov’s inequality, a fundamental concentration  bound, that let us prove that a random variable lies close to its expectation with good probability.  • Learn about random hash functions, which are a key tool in  randomized methods for data processing. Probabilistic analysis via linearity of expectation.  1  
some probability review  • Expectation:  E[X] =(cid:80)  s∈S Pr(X = s) · s.  2  
some probability review  • Expectation: • Variance:  E[X] =(cid:80)  s∈S Pr(X = s) · s.  Var[X] = E[(X − E[X])2].  2  
some probability review  E[X] =(cid:80)  s∈S Pr(X = s) · s.  • Expectation: • Variance: • Two random variables X, Y are independent if for all s, t,  Var[X] = E[(X − E[X])2].  {X = s} and {Y = t} are independent events. In other words:  Pr({X = s} ∩ {Y = t}) = Pr(X = s) · Pr(Y = t).  2  
linearity of expectation and variance  When are the expectation and variance linear?  I.e., under what conditions on X and Y do we have:  E[X + Y] = E[X] + E[Y]  and  Var[X + Y] = Var[X] + Var[Y].  3  
linearity of expectation and variance  When are the expectation and variance linear?  I.e., under what conditions on X and Y do we have:  E[X + Y] = E[X] + E[Y]  and  Var[X + Y] = Var[X] + Var[Y].  Last time we showed that linearity of expectation is true regardless of whether the random variables were independent.  X, Y: any two random variables.  3  
linearity of variance  Var[X + Y] = Var[X] + Var[Y]  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent.  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation)  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give:  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  (linearity of expectation)  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  = E[X2] + 2E[XY] + E[Y2] − E[X]2 − 2E[X] · E[Y] − E[Y]2  (linearity of expectation)  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  = E[X2] + 2E[XY] + E[Y2] − E[X]2 − 2E[X] · E[Y] − E[Y]2  (linearity of expectation)  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  (linearity of expectation)  = E[X2] + 2E[XY] + E[Y2] − E[X]2 − 2E[X] · E[Y] − E[Y]2 = E[X2] + E[Y2] − E[X]2 − E[Y]2  4  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  (linearity of expectation)  = E[X2] + 2E[XY] + E[Y2] − E[X]2 − 2E[X] · E[Y] − E[Y]2 = E[X2] + E[Y2] − E[X]2 − E[Y]2 = Var[X] + Var[Y].  4  
an algorithmic application  You have contracted with a new company to provide CAPTCHAS for your website.  5  
an algorithmic application  You have contracted with a new company to provide CAPTCHAS for your website.  • They claim that they have a database of 1, 000, 000 unique  CAPTCHAS. A random one is chosen for each security check. • You want to independently verify this claimed database size.  5  
an algorithmic application  You have contracted with a new company to provide CAPTCHAS for your website.  • They claim that they have a database of 1, 000, 000 unique  CAPTCHAS. A random one is chosen for each security check. • You want to independently verify this claimed database size. • You could make test checks until you see 1, 000, 000 unique  CAPTCHAS: would take ≥ 1, 000, 000 checks!  5  
an algorithmic application  An Idea: You run some test security checks and see if any duplicate CAPTCHAS show up. If you’re seeing duplicates after not too many checks, the database size is probably not too big.  6  
an algorithmic application  An Idea: You run some test security checks and see if any duplicate CAPTCHAS show up. If you’re seeing duplicates after not too many checks, the database size is probably not too big.  ‘Mark and recapture’ method in ecology.  6  
an algorithmic application  An Idea: You run some test security checks and see if any duplicate CAPTCHAS show up. If you’re seeing duplicates after not too many checks, the database size is probably not too big.  ‘Mark and recapture’ method in ecology.  6  
an algorithmic application  An Idea: You run some test security checks and see if any duplicate CAPTCHAS show up. If you’re seeing duplicates after not too many checks, the database size is probably not too big.  ‘Mark and recapture’ method in ecology.  Note that if the same CAPTCHA shows up four times this counts as(cid:0)4 (cid:1)  2  duplicates.  6  
linearity of expectation  Let Di,j = 1 if tests i and j give the same CAPTCHA, and 0 otherwise. An indicator random variable.  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS  7  
linearity of expectation  Let Di,j = 1 if tests i and j give the same CAPTCHA, and 0 otherwise. An indicator random variable.  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS  7  
linearity of expectation  Let Di,j = 1 if tests i and j give the same CAPTCHA, and 0 otherwise. An indicator random variable. The number of pairwise duplicates (a random variable) is:  D =  Di,j .  (cid:88)  i,j∈[m],i(cid:54)=j  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS  7  
linearity of expectation  Let Di,j = 1 if tests i and j give the same CAPTCHA, and 0 otherwise. An indicator random variable. The number of pairwise duplicates (a random variable) is:  E[D] =  E[Di,j ].  (cid:88)  i,j∈[m],i(cid:54)=j  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS  7  
linearity of expectation  Let Di,j = 1 if tests i and j give the same CAPTCHA, and 0 otherwise. An indicator random variable. The number of pairwise duplicates (a random variable) is:  E[D] =  E[Di,j ].  (cid:88)  i,j∈[m],i(cid:54)=j  For any pair i, j ∈ [m], i (cid:54)= j:  E[Di,j ] = Pr[Di,j = 1] = 1 n .  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS  7  
linearity of expectation  Let Di,j = 1 if tests i and j give the same CAPTCHA, and 0 otherwise. An indicator random variable. The number of pairwise duplicates (a random variable) is:  (cid:88)  E[D] =  E[Di,j ].  i,j∈[m],i(cid:54)=j  For any pair i, j ∈ [m], i (cid:54)= j:  (cid:88)  E[D] =  i,j∈[m],i(cid:54)=j  (cid:0)m  (cid:1)  E[Di,j ] = Pr[Di,j = 1] = 1 n .  1 n  =  2 n  =  m(m − 1)  .  2n  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS  7  
linearity of expectation  You take m = 1000 samples. If the database size is as claimed (n = 1, 000, 000) then expected number of duplicates is:  m(m − 1)  2n  E[D] =  = .4995  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS.  8  
linearity of expectation  You take m = 1000 samples. If the database size is as claimed (n = 1, 000, 000) then expected number of duplicates is:  m(m − 1)  2n  E[D] =  = .4995  You see 10 pairwise duplicates and suspect that something is up. But how conﬁdent can you be in your test?  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS.  8  
linearity of expectation  You take m = 1000 samples. If the database size is as claimed (n = 1, 000, 000) then expected number of duplicates is:  m(m − 1)  2n  E[D] =  = .4995  You see 10 pairwise duplicates and suspect that something is up. But how conﬁdent can you be in your test?  Concentration Inequalities: Bounds on the probability that a random variable deviates a certain distance from its mean.  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS.  8  
linearity of expectation  You take m = 1000 samples. If the database size is as claimed (n = 1, 000, 000) then expected number of duplicates is:  m(m − 1)  2n  E[D] =  = .4995  You see 10 pairwise duplicates and suspect that something is up. But how conﬁdent can you be in your test?  Concentration Inequalities: Bounds on the probability that a random variable deviates a certain distance from its mean. • Useful in understanding how statistical tests perform, the  behavior of randomized algorithms, the behavior of data drawn from diﬀerent distributions, etc.  n: number of CAPTCHAS in database, m: number of random CAPTCHAS drawn to check database size, D: number of pairwise duplicates in m random CAPTCHAS.  8  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  9  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  For any non-negative random variable X and any t > 0:  Pr[X ≥ t] ≤ E[X]  t  .  9  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  For any non-negative random variable X and any t > 0:  Pr[X ≥ t] ≤ E[X]  t  .  Proof:  9  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  For any non-negative random variable X and any t > 0:  Pr[X ≥ t] ≤ E[X]  t  .  Proof:  (cid:88)  s  E[X] =  Pr(X = s) · s  9  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  For any non-negative random variable X and any t > 0:  t  Pr[X ≥ t] ≤ E[X] Pr(X = s) · s ≥(cid:88)  Proof:  (cid:88)  s  E[X] =  .  s≥t  Pr(X = s) · s  9  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  For any non-negative random variable X and any t > 0:  .  t  Pr(X = s) · s  Pr(X = s) · t  9  Proof:  (cid:88)  s  E[X] =  Pr[X ≥ t] ≤ E[X] Pr(X = s) · s ≥(cid:88) ≥(cid:88)  s≥t  s≥t  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  For any non-negative random variable X and any t > 0:  Proof:  (cid:88)  s  E[X] =  .  t  Pr[X ≥ t] ≤ E[X] Pr(X = s) · s ≥(cid:88) ≥(cid:88)  s≥t  Pr(X = s) · s  Pr(X = s) · t  s≥t  = t · Pr(X ≥ t).  9  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  For any non-negative random variable X and any t > 0:  Pr[X ≥ t · E[X]] ≤ 1 t  .  Proof:  (cid:88)  s  E[X] =  Pr(X = s) · s ≥(cid:88) ≥(cid:88)  s≥t  Pr(X = s) · s  Pr(X = s) · t  s≥t  = t · Pr(X ≥ t).  9  
markov’s inequality  The simplest concentration bound: Markov’s inequality.  For any non-negative random variable X and any t > 0:  Pr[X ≥ t · E[X]] ≤ 1 t  .  Proof:  (cid:88)  s  E[X] =  Pr(X = s) · s ≥(cid:88) ≥(cid:88)  s≥t  Pr(X = s) · s  Pr(X = s) · t  s≥t  = t · Pr(X ≥ t).  The larger the deviation t, the smaller the probability.  9  
back to our application  Expected number of duplicate CAPTCHAS: E[D] = m(m−1)  2n = .4995.  You see D = 10 duplicates.  n: number of CAPTCHAS in database (n = 1000000 claimed) , m: number of random CAPTCHAS drawn to check database size (m = 1000 in this example), D: number of pairwise duplicates in m random CAPTCHAS.  10  
back to our application  Expected number of duplicate CAPTCHAS: E[D] = m(m−1)  2n = .4995.  You see D = 10 duplicates.  Applying Markov’s inequality, if the real database size is n = 1000000 the probability of this happening is:  Pr[D ≥ 10] ≤ E[D]  =  .4995  10  ≈ .05  10  n: number of CAPTCHAS in database (n = 1000000 claimed) , m: number of random CAPTCHAS drawn to check database size (m = 1000 in this example), D: number of pairwise duplicates in m random CAPTCHAS.  10  
back to our application  Expected number of duplicate CAPTCHAS: E[D] = m(m−1)  2n = .4995.  You see D = 10 duplicates.  Applying Markov’s inequality, if the real database size is n = 1000000 the probability of this happening is:  Pr[D ≥ 10] ≤ E[D]  =  .4995  10  ≈ .05  10  This is pretty small and you feel pretty sure the number of unique CAPTCHAS is much less than 1000000.  n: number of CAPTCHAS in database (n = 1000000 claimed) , m: number of random CAPTCHAS drawn to check database size (m = 1000 in this example), D: number of pairwise duplicates in m random CAPTCHAS.  10  
hash tables  Want to store a set of items from some ﬁnite but massive universe of items (e.g., images of a certain size, text documents, 128-bit IP addresses).  11  
hash tables  Want to store a set of items from some ﬁnite but massive universe of items (e.g., images of a certain size, text documents, 128-bit IP addresses).  Goal: support query (x) to check if x is in the set in O(1) time.  11  
hash tables  Want to store a set of items from some ﬁnite but massive universe of items (e.g., images of a certain size, text documents, 128-bit IP addresses).  Goal: support query (x) to check if x is in the set in O(1) time.  Classic Solution:  11  
hash tables  Want to store a set of items from some ﬁnite but massive universe of items (e.g., images of a certain size, text documents, 128-bit IP addresses).  Goal: support query (x) to check if x is in the set in O(1) time.  Classic Solution: Hash tables  11  
hash tables  Want to store a set of items from some ﬁnite but massive universe of items (e.g., images of a certain size, text documents, 128-bit IP addresses).  Goal: support query (x) to check if x is in the set in O(1) time.  Classic Solution: Hash tables • Static hashing since we won’t worry about insertion and deletion  today.  11  
hash tables  • hash function h : U → [n] maps elements from the universe to  indices 1,··· , n of an array.  12  
hash tables  • hash function h : U → [n] maps elements from the universe to  indices 1,··· , n of an array.  • Typically |U| (cid:29) n. Many elements map to the same index.  12  
hash tables  • hash function h : U → [n] maps elements from the universe to  indices 1,··· , n of an array.  • Typically |U| (cid:29) n. Many elements map to the same index. • Collisions: when we insert m items into the hash table we may have to store multiple items in the same location (typically as a linked list).  12  
collisions  Query runtime: O(c) when the maximum number of collisions in a table entry is c (i.e., must traverse a linked list of size c).  13  
collisions  Query runtime: O(c) when the maximum number of collisions in a table entry is c (i.e., must traverse a linked list of size c).  How Can We Bound c?  13  
collisions  Query runtime: O(c) when the maximum number of collisions in a table entry is c (i.e., must traverse a linked list of size c).  How Can We Bound c? • In the worst case, could have c = m (all items hash to the same  location). In the best case, c ≈ m/n.  13  
random hash function  Let h : U → [n] be a random hash function. • I.e., for x ∈ U, Pr(h(x) = i) = 1  h(x), h(y ) are independent for any two items x (cid:54)= y .  n for all i = 1, . . . , n and  14  
random hash function  Let h : U → [n] be a random hash function. • I.e., for x ∈ U, Pr(h(x) = i) = 1  h(x), h(y ) are independent for any two items x (cid:54)= y .  n for all i = 1, . . . , n and  • Caveat 1: It is very expensive to represent and compute such a random function. We will see how a hash function computable in O(1) time function can be used instead.  • Caveat 2: In practice, often suﬃces to use hash functions like  MD5, SHA-2, etc. that ‘look random enough’.  14  
linearity of expectation  Let Ci,j = 1 if items i and j collide (h(xi ) = h(xj )), and 0 otherwise. The number of pairwise duplicates is:  C =  Ci,j .  (cid:88)  i,j∈[m],i(cid:54)=j  xi , xj : pair of stored items, m: total number of stored items, n: hash table size, C: total pairwise collisions in table, h: random hash function.  15  
linearity of expectation  Let Ci,j = 1 if items i and j collide (h(xi ) = h(xj )), and 0 otherwise. The number of pairwise duplicates is:  E[C] =  E[Ci,j ].  (linearity of expectation)  (cid:88)  i,j∈[m],i(cid:54)=j  xi , xj : pair of stored items, m: total number of stored items, n: hash table size, C: total pairwise collisions in table, h: random hash function.  15  
linearity of expectation  Let Ci,j = 1 if items i and j collide (h(xi ) = h(xj )), and 0 otherwise. The number of pairwise duplicates is:  E[C] =  E[Ci,j ].  (linearity of expectation)  (cid:88)  i,j∈[m],i(cid:54)=j  For any pair i, j, i (cid:54)= j: E[Ci,j ] = Pr[Ci,j = 1] = Pr[h(xi ) = h(xj )]  xi , xj : pair of stored items, m: total number of stored items, n: hash table size, C: total pairwise collisions in table, h: random hash function.  15  
linearity of expectation  Let Ci,j = 1 if items i and j collide (h(xi ) = h(xj )), and 0 otherwise. The number of pairwise duplicates is:  E[C] =  E[Ci,j ].  (linearity of expectation)  (cid:88)  i,j∈[m],i(cid:54)=j  For any pair i, j, i (cid:54)= j: E[Ci,j ] = Pr[Ci,j = 1] = Pr[h(xi ) = h(xj )] = 1 n .  xi , xj : pair of stored items, m: total number of stored items, n: hash table size, C: total pairwise collisions in table, h: random hash function.  15  
linearity of expectation  Let Ci,j = 1 if items i and j collide (h(xi ) = h(xj )), and 0 otherwise. The number of pairwise duplicates is:  E[C] =  E[Ci,j ].  (linearity of expectation)  (cid:88)  i,j∈[m],i(cid:54)=j  (cid:88)  For any pair i, j, i (cid:54)= j: E[Ci,j ] = Pr[Ci,j = 1] = Pr[h(xi ) = h(xj )] = 1 n .  (cid:0)m  (cid:1)  E[C] =  i,j∈[m],i(cid:54)=j  1 n  =  2 n  =  m(m − 1)  .  2n  xi , xj : pair of stored items, m: total number of stored items, n: hash table size, C: total pairwise collisions in table, h: random hash function.  15  
linearity of expectation  Let Ci,j = 1 if items i and j collide (h(xi ) = h(xj )), and 0 otherwise. The number of pairwise duplicates is:  E[C] =  E[Ci,j ].  (linearity of expectation)  (cid:88)  i,j∈[m],i(cid:54)=j  (cid:88)  For any pair i, j, i (cid:54)= j: E[Ci,j ] = Pr[Ci,j = 1] = Pr[h(xi ) = h(xj )] = 1 n .  (cid:0)m  (cid:1)  E[C] =  i,j∈[m],i(cid:54)=j  1 n  =  2 n  =  m(m − 1)  .  2n  Identical to the CAPTCHA analysis!  xi , xj : pair of stored items, m: total number of stored items, n: hash table size, C: total pairwise collisions in table, h: random hash function.  15  
collision free hashing  E[C] =  m(m − 1)  2n  .  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1)  8m2 ≤ 1 8 .  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1)  8m2 ≤ 1 8 .  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1)  8m2 ≤ 1 8 .  Apply Markov’s Inequality:  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1) 8m2 ≤ 1 8 . Apply Markov’s Inequality: Pr[C ≥ 1] ≤ E[C]  1  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1) 8m2 ≤ 1 8 . Apply Markov’s Inequality: Pr[C ≥ 1] ≤ E[C] 1 = 1 8 .  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1) 8m2 ≤ 1 8 . Apply Markov’s Inequality: Pr[C ≥ 1] ≤ E[C] 1 = 1 8 .  Pr[C = 0] = 1 − Pr[C ≥ 1]  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1) 8m2 ≤ 1 8 . Apply Markov’s Inequality: Pr[C ≥ 1] ≤ E[C] 1 = 1 8 .  Pr[C = 0] = 1 − Pr[C ≥ 1] ≥ 1 − 1 8  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1) 8m2 ≤ 1 8 . Apply Markov’s Inequality: Pr[C ≥ 1] ≤ E[C] 1 = 1 8 . 7 8  Pr[C = 0] = 1 − Pr[C ≥ 1] ≥ 1 − 1 8  =  .  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
collision free hashing  E[C] =  m(m − 1)  2n  .  • For n = 4m2 we have: E[C] = m(m−1) 8m2 ≤ 1 8 . Apply Markov’s Inequality: Pr[C ≥ 1] ≤ E[C] 1 = 1 8 . 7 8  Pr[C = 0] = 1 − Pr[C ≥ 1] ≥ 1 − 1 8  =  .  Pretty good but we are using O(m2) space to store m items.  m: total number of stored items, n: hash table size, C: total pairwise collisions in table.  16  
two level hashing  Want to preserve O(1) query time while using O(m) space.  17  
two level hashing  Want to preserve O(1) query time while using O(m) space.  Two-Level Hashing:  17  
two level hashing  Want to preserve O(1) query time while using O(m) space.  Two-Level Hashing:  • For each bucket with si values, pick a collision free hash function  mapping [si ] → [s 2 i ].  17  
two level hashing  Want to preserve O(1) query time while using O(m) space.  Two-Level Hashing:  • For each bucket with si values, pick a collision free hash function  mapping [si ] → [s 2 i ].  • Just Showed: A random function is collision free with probability ≥ 7 8 so only requires checking O(1) random functions in expectation to ﬁnd a collision free one.  17  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions.  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: S = n +(cid:80)n  i=1 s2 i  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  E[s2 i ]  i=1  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  E[s2 i ]  i=1  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  E[s2 i ]  i=1  E[s2  i ] = E  Ih(xj )=i    m(cid:88)  j=1  2  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  E[s2 i ]  i=1  2  Ih(xj )=i    m(cid:88)  (cid:88)  j=1  j,k∈[m]  E[s2  i ] = E  = E    Ih(xj )=i · Ih(xk )=i  Collisions again!  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  =    m(cid:88)  (cid:88)  Ih(xj )=i · Ih(xk )=i  2  E[s2  i ] = E  = E  Ih(xj )=i  j=1  j,k∈[m]  E[s2 i ]  i=1  (cid:88)  j,k∈[m]  E(cid:2)Ih(xj )=i · Ih(xk )=i  (cid:3) .  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  =    m(cid:88)  (cid:88)  Ih(xj )=i · Ih(xk )=i  2  E[s2  i ] = E  = E  Ih(xj )=i  j=1  j,k∈[m]  E[s2 i ]  i=1  (cid:88)  j,k∈[m]  E(cid:2)Ih(xj )=i · Ih(xk )=i  (cid:3) .  • For j = k,  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  E[s2 i ]  i=1  E[s2  i ] = E  Ih(xj )=i  2  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  = (cid:88) E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:1)2(cid:105) (cid:3) = E(cid:104)(cid:0)Ih(xj )=i  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i    m(cid:88)  (cid:88)  j=1  Ih(xj )=i · Ih(xk )=i  = E  j,k∈[m]  j,k∈[m]  (cid:3) .  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  E[s2  i ] = E  Ih(xj )=i  2  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  = (cid:88) E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:1)2(cid:105) (cid:3) = E(cid:104)(cid:0)Ih(xj )=i  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i    m(cid:88)  (cid:88)  j=1  Ih(xj )=i · Ih(xk )=i  = Pr[h(xj ) = i]  = E  j,k∈[m]  j,k∈[m]  E[s2 i ]  i=1  (cid:3) .  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  E[s2 i ]  i=1  E[s2  i ] = E  Ih(xj )=i  2  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  = (cid:88) E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:1)2(cid:105) (cid:3) = E(cid:104)(cid:0)Ih(xj )=i  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i    m(cid:88)  (cid:88)  j=1  Ih(xj )=i · Ih(xk )=i  = E  j,k∈[m]  j,k∈[m]  (cid:3) .  = Pr[h(xj ) = i] = 1 n .  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  E[s2 i ]  i=1  E[s2  i ] = E  Ih(xj )=i  2  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  = (cid:88) E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:1)2(cid:105) (cid:3) = E(cid:104)(cid:0)Ih(xj )=i  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i    m(cid:88)  (cid:88)  j=1  Ih(xj )=i · Ih(xk )=i  = E  j,k∈[m]  j,k∈[m]  (cid:3) .  = Pr[h(xj ) = i] = 1 n .  • For j (cid:54)= k,  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  E[s2  i ] = E  Ih(xj )=i  E[s2 i ]  i=1  2    m(cid:88)  (cid:88)  j=1  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  = (cid:88) E(cid:2)Ih(xj )=i · Ih(xk )=i Ih(xj )=i · Ih(xk )=i (cid:3) = E(cid:104)(cid:0)Ih(xj )=i (cid:1)2(cid:105) (cid:3)  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  (cid:3) .  = E  j,k∈[m]  j,k∈[m]  = Pr[h(xj ) = i] = 1 n .  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  E[s2 i ]  i=1  E[s2  i ] = E  Ih(xj )=i  2    m(cid:88)  (cid:88)  j=1  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  = (cid:88) E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:3) = E(cid:104)(cid:0)Ih(xj )=i (cid:1)2(cid:105) (cid:3) = Pr[h(xj ) = i ∩ h(xk ) = i]  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  Ih(xj )=i · Ih(xk )=i  = E  j,k∈[m]  j,k∈[m]  (cid:3) .  = Pr[h(xj ) = i] = 1 n .  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  E[s2  i ] = E  Ih(xj )=i  2    m(cid:88)  (cid:88)  j=1  Query time for two level hashing is O(1): requires evaluating two hash functions. What is the expected space usage?  Up to constants, space used is: E[S] = n +(cid:80)n  = (cid:88) E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:3) = E(cid:104)(cid:0)Ih(xj )=i (cid:1)2(cid:105) (cid:3) = Pr[h(xj ) = i ∩ h(xk ) = i] = 1  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  Ih(xj )=i · Ih(xk )=i  = E  j,k∈[m]  (cid:3) .  n2 .  E[s2 i ]  i=1  j,k∈[m]  = Pr[h(xj ) = i] = 1 n .  xj , xk : stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored in hash table at position i.  18  
space usage  E[s2  i ] =  (cid:88)  j,k∈[m]  E(cid:2)Ih(xj )=i · Ih(xk )=i  (cid:3)  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  (cid:3) = 1 (cid:3) = 1  n . n2 .  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
space usage  (cid:88)  E[s2  i ] =  j,k∈[m] = m · 1 n  + 2 ·  2  (cid:3)  (cid:19)  E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:18)m (cid:3) = 1 (cid:3) = 1  · 1 n2  n . n2 .  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
space usage  (cid:88)  E[s2  i ] =  j,k∈[m] = m · 1 n  + 2 ·  2  (cid:3)  (cid:19)  E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:18)m (cid:3) = 1 (cid:3) = 1  · 1 n2  n . n2 .  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
space usage  (cid:88)  E[s2  i ] =  j,k∈[m] = m · 1 n  + 2 ·  2  (cid:3)  (cid:19)  E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:18)m (cid:3) = 1 (cid:3) = 1  · 1 n2  n . n2 .  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
space usage  E[s2  i ] =  (cid:88)  j,k∈[m] = m · 1 n  (cid:3)  (cid:19)  E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:18)m (cid:3) = 1 (cid:3) = 1  + 2 · 2 m(m − 1)  · 1 n2  n . n2 .  n2  m n  +  =  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
space usage  m n  +  =  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i  ≤ 2 (If we set n = m.)  E[s2  i ] =  (cid:88)  j,k∈[m] = m · 1 n  (cid:3)  (cid:19)  E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:18)m (cid:3) = 1 (cid:3) = 1  + 2 · 2 m(m − 1)  · 1 n2  n . n2 .  n2  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
space usage  E[s2  i ] =  (cid:88)  j,k∈[m] = m · 1 n  (cid:3)  (cid:19)  E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:18)m (cid:3) = 1 (cid:3) = 1  + 2 · 2 m(m − 1)  · 1 n2  n . n2 .  n2  ≤ 2 (If we set n = m.)  m n  +  =  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i n(cid:88)  E[S] = n +  E[s2 i ]  Total Expected Space Usage: (if we set n = m)  i=1  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
space usage  E[s2  i ] =  (cid:88)  j,k∈[m] = m · 1 n  (cid:3)  (cid:19)  E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:18)m (cid:3) = 1 (cid:3) = 1  + 2 · 2 m(m − 1)  · 1 n2  n . n2 .  n2  m n  +  =  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i n(cid:88)  E[S] = n +  ≤ 2 (If we set n = m.)  Total Expected Space Usage: (if we set n = m)  E[s2  i ] ≤ n + n · 2 = 3n = 3m.  i=1  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
space usage  E[s2  i ] =  (cid:88)  j,k∈[m] = m · 1 n  (cid:3)  (cid:19)  E(cid:2)Ih(xj )=i · Ih(xk )=i (cid:18)m (cid:3) = 1 (cid:3) = 1  + 2 · 2 m(m − 1)  · 1 n2  n . n2 .  n2  m n  +  =  • For j = k, E(cid:2)Ih(xj )=i · Ih(xk )=i • For j (cid:54)= k, E(cid:2)Ih(xj )=i · Ih(xk )=i n(cid:88)  E[S] = n +  ≤ 2 (If we set n = m.)  Total Expected Space Usage: (if we set n = m)  E[s2  i ] ≤ n + n · 2 = 3n = 3m.  Near optimal space with O(1) query time!  i=1  xj , xk : stored items, m: # stored items, n: hash table size, h: random hash function, S: space usage of two level hashing, si : # items stored at pos i.  19  
something to think about  What if we want to store a set and answer membership queries in O(1) time. But we allow a small probability of a false positive: query (x) says that x is in the set when in fact it isn’t.  20  
something to think about  What if we want to store a set and answer membership queries in O(1) time. But we allow a small probability of a false positive: query (x) says that x is in the set when in fact it isn’t.  Can we use even smaller space?  20  
something to think about  What if we want to store a set and answer membership queries in O(1) time. But we allow a small probability of a false positive: query (x) says that x is in the set when in fact it isn’t.  Can we use even smaller space?  Many Applications: • Filter spam email addresses, phone numbers, suspect IPs,  duplicate Tweets.  • Quickly check if an item has been stored in a cache or is new. • Counting distinct elements (e.g., unique search queries.)  20  
efficiently computable hash functions  What properties did we use of the randomly chosen hash function?  21  
efficiently computable hash functions  What properties did we use of the randomly chosen hash function?  2-Universal Hash Function (low collision probability). A random hash function from h : U → [n] is two universal if:  Pr[h(x) = h(y )] ≤ 1 n  .  21  
efficiently computable hash functions  What properties did we use of the randomly chosen hash function?  2-Universal Hash Function (low collision probability). A random hash function from h : U → [n] is two universal if:  Pr[h(x) = h(y )] ≤ 1 n  .  Exercise: Rework the two level hashing proof to show that this property is really all that is needed.  21  
efficiently computable hash functions  What properties did we use of the randomly chosen hash function?  2-Universal Hash Function (low collision probability). A random hash function from h : U → [n] is two universal if:  Pr[h(x) = h(y )] ≤ 1 n  .  Exercise: Rework the two level hashing proof to show that this property is really all that is needed.  When h(x) and h(y ) are chosen independently at random from [n], Pr[h(x) = h(y )] = 1 n (so a fully random hash function is 2-universal)  21  
efficiently computable hash functions  What properties did we use of the randomly chosen hash function?  2-Universal Hash Function (low collision probability). A random hash function from h : U → [n] is two universal if:  Pr[h(x) = h(y )] ≤ 1 n  .  Exercise: Rework the two level hashing proof to show that this property is really all that is needed.  When h(x) and h(y ) are chosen independently at random from [n], Pr[h(x) = h(y )] = 1 n (so a fully random hash function is 2-universal) Eﬃcient Alternative: Let p be a prime with p ≥ |U|. Choose random a, b ∈ [p] with a (cid:54)= 0. Let:  h(x) = (ax + b mod p) mod n.  21  
pairwise independence  Another common requirement for a hash function:  22  
pairwise independence  Another common requirement for a hash function:  Pairwise Independent Hash Function. A random hash function from h : U → [n] is pairwise independent if for all i ∈ [n]:  Pr[h(x) = h(y ) = i] =  1 n2 .  22  
pairwise independence  Another common requirement for a hash function:  Pairwise Independent Hash Function. A random hash function from h : U → [n] is pairwise independent if for all i ∈ [n]:  Pr[h(x) = h(y ) = i] =  1 n2 .  Which is a more stringent requirement? 2-universal or pairwise independent?  22  
pairwise independence  Another common requirement for a hash function:  Pairwise Independent Hash Function. A random hash function from h : U → [n] is pairwise independent if for all i ∈ [n]:  Pr[h(x) = h(y ) = i] =  1 n2 .  Which is a more stringent requirement? 2-universal or pairwise independent?  Pr[h(x) = h(y ) = i] = n · 1  n2 =  1 n  .  22  Pr[h(x) = h(y )] =  n(cid:88)  i=1  
pairwise independence  Another common requirement for a hash function:  Pairwise Independent Hash Function. A random hash function from h : U → [n] is pairwise independent if for all i ∈ [n]:  Pr[h(x) = h(y ) = i] =  1 n2 .  Which is a more stringent requirement? 2-universal or pairwise independent?  Pr[h(x) = h(y )] =  Pr[h(x) = h(y ) = i] = n · 1  n2 =  1 n  .  n(cid:88)  i=1  A closely related (ax + b) mod p construction gives pairwise  independence on top of 2-universality.  22  
pairwise independence  Another common requirement for a hash function:  k-wise Independent Hash Function. A random hash function from h : U → [n] is k-wise independent if for all i ∈ [n]:  Pr[h(x1) = h(x2) = . . . = h(xk ) = i] =  1 nk .  Which is a more stringent requirement? 2-universal or pairwise independent?  Pr[h(x) = h(y )] =  Pr[h(x) = h(y ) = i] = n · 1  n2 =  1 n  .  n(cid:88)  i=1  A closely related (ax + b) mod p construction gives pairwise  independence on top of 2-universality.  22  
