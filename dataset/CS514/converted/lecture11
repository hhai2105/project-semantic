compsci 514: algorithms for data science  Andrew McGregor  Lecture 11  0  
high dimensional data  ‘Big Data’ means not just many data points, but many measurements per data point. I.e., very high dimensional data.  1  
high dimensional data  ‘Big Data’ means not just many data points, but many measurements per data point. I.e., very high dimensional data. • Twitter has 321 million active monthly users. Records (tens of)  thousands of measurements per user: who they follow, who follows them, when they last visited the site, timestamps for speciﬁc interactions, how many tweets they have sent, the text of those tweets, etc.  1  
high dimensional data  ‘Big Data’ means not just many data points, but many measurements per data point. I.e., very high dimensional data. • Twitter has 321 million active monthly users. Records (tens of)  thousands of measurements per user: who they follow, who follows them, when they last visited the site, timestamps for speciﬁc interactions, how many tweets they have sent, the text of those tweets, etc.  • A 3 minute Youtube clip with a resolution of 500 × 500 pixels at 15 frames/second with 3 color channels is a recording of ≥ 2 billion pixel values. Even a 500 × 500 pixel color image has 750, 000 pixel values.  1  
high dimensional data  ‘Big Data’ means not just many data points, but many measurements per data point. I.e., very high dimensional data. • Twitter has 321 million active monthly users. Records (tens of)  thousands of measurements per user: who they follow, who follows them, when they last visited the site, timestamps for speciﬁc interactions, how many tweets they have sent, the text of those tweets, etc.  • A 3 minute Youtube clip with a resolution of 500 × 500 pixels at 15 frames/second with 3 color channels is a recording of ≥ 2 billion pixel values. Even a 500 × 500 pixel color image has 750, 000 pixel values. • The human genome contains 3 billion+ base pairs. Genetic datasets  often contain information on 100s of thousands+ mutations and genetic markers.  1  
data as vectors and matrices  In data analysis and machine learning, data points with many attributes are often stored, processed, and interpreted as high dimensional vectors, with real valued entries.  2  
data as vectors and matrices  In data analysis and machine learning, data points with many attributes are often stored, processed, and interpreted as high dimensional vectors, with real valued entries.  2  
data as vectors and matrices  In data analysis and machine learning, data points with many attributes are often stored, processed, and interpreted as high dimensional vectors, with real valued entries.  Similarities/distances between vectors (e.g., (cid:104)x, y(cid:105), (cid:107)x − y(cid:107)2) have meaning for underlying data points.  2  
datasets as vectors and matrices  Data points are interpreted as high dimensional vectors, with real valued entries. Data set is interpreted as a matrix. Data Points: (cid:126)x1, (cid:126)x2, . . . , (cid:126)xn ∈ Rd . Data Set: X ∈ Rn×d with i th rows equal to (cid:126)xi .  3  
datasets as vectors and matrices  Data points are interpreted as high dimensional vectors, with real valued entries. Data set is interpreted as a matrix. Data Points: (cid:126)x1, (cid:126)x2, . . . , (cid:126)xn ∈ Rd . Data Set: X ∈ Rn×d with i th rows equal to (cid:126)xi .  3  
datasets as vectors and matrices  Data points are interpreted as high dimensional vectors, with real valued entries. Data set is interpreted as a matrix. Data Points: (cid:126)x1, (cid:126)x2, . . . , (cid:126)xn ∈ Rd . Data Set: X ∈ Rn×d with i th rows equal to (cid:126)xi .  Many data points n =⇒ tall. Many dimensions d =⇒ wide.  3  
dimensionality reduction  Dimensionality Reduction: Compress data points so that they lie in many fewer dimensions.  4  
dimensionality reduction  Dimensionality Reduction: Compress data points so that they lie in many fewer dimensions.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd → ˜x1, . . . , ˜xn ∈ Rm for m (cid:28) d.  4  
dimensionality reduction  Dimensionality Reduction: Compress data points so that they lie in many fewer dimensions.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd → ˜x1, . . . , ˜xn ∈ Rm for m (cid:28) d.  ‘Lossy compression’ that still preserves important information about the relationships between (cid:126)x1, . . . , (cid:126)xn.  4  
dimensionality reduction  Dimensionality Reduction: Compress data points so that they lie in many fewer dimensions.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd → ˜x1, . . . , ˜xn ∈ Rm for m (cid:28) d.  ‘Lossy compression’ that still preserves important information about the relationships between (cid:126)x1, . . . , (cid:126)xn.  Generally will not consider directly how well ˜xi approximates (cid:126)xi .  4  
low distortion embedding  Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd , distance function D, and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) and distance function ˜D such that for all i, j ∈ [n]:  (1 − )D((cid:126)xi , (cid:126)xj ) ≤ ˜D(˜xi , ˜xj ) ≤ (1 + )D((cid:126)xi , (cid:126)xj ).  5  
low distortion embedding  Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd , distance function D, and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) and distance function ˜D such that for all i, j ∈ [n]:  (1 − )D((cid:126)xi , (cid:126)xj ) ≤ ˜D(˜xi , ˜xj ) ≤ (1 + )D((cid:126)xi , (cid:126)xj ).  Have already seen one example in class: MinHash.  5  
low distortion embedding  Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd , distance function D, and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) and distance function ˜D such that for all i, j ∈ [n]:  (1 − )D((cid:126)xi , (cid:126)xj ) ≤ ˜D(˜xi , ˜xj ) ≤ (1 + )D((cid:126)xi , (cid:126)xj ).  Have already seen one example in class: MinHash.  5  
low distortion embedding  Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd , distance function D, and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) and distance function ˜D such that for all i, j ∈ [n]:  (1 − )D((cid:126)xi , (cid:126)xj ) ≤ ˜D(˜xi , ˜xj ) ≤ (1 + )D((cid:126)xi , (cid:126)xj ).  Have already seen one example in class: MinHash.  5  
low distortion embedding  Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd , distance function D, and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) and distance function ˜D such that for all i, j ∈ [n]:  (1 − )D((cid:126)xi , (cid:126)xj ) ≤ ˜D(˜xi , ˜xj ) ≤ (1 + )D((cid:126)xi , (cid:126)xj ).  Have already seen one example in class: MinHash.  With large enough signature size r , # matching entries in ˜xA, ˜xB  r  ≈ J((cid:126)xA, (cid:126)xB ).  5  
low distortion embedding  Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd , distance function D, and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) and distance function ˜D such that for all i, j ∈ [n]:  (1 − )D((cid:126)xi , (cid:126)xj ) ≤ ˜D(˜xi , ˜xj ) ≤ (1 + )D((cid:126)xi , (cid:126)xj ).  Have already seen one example in class: MinHash.  ≈ J((cid:126)xA, (cid:126)xB ). With large enough signature size r , # matching entries in ˜xA, ˜xB • Note: here J((cid:126)xA, (cid:126)xB ) is a similarity rather than a distance. So this is  r  not quite low distortion embedding, but is closely related.  5  
embeddings for euclidean space  Euclidean Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) such that for all i, j ∈ [n]:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  Recall that for (cid:126)z ∈ Rn, (cid:107)(cid:126)z(cid:107)2 =  i=1 (cid:126)z(i)2.  (cid:113)(cid:80)n  6  
embeddings for euclidean space  Euclidean Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) such that for all i, j ∈ [n]:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  Recall that for (cid:126)z ∈ Rn, (cid:107)(cid:126)z(cid:107)2 =  i=1 (cid:126)z(i)2.  (cid:113)(cid:80)n  6  
embeddings for euclidean space  Euclidean Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) such that for all i, j ∈ [n]:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  6  
embeddings for euclidean space  Euclidean Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) such that for all i, j ∈ [n]:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  Can use ˜x1, . . . , ˜xn in place of (cid:126)x1, . . . , (cid:126)xn in clustering, SVM, linear classiﬁcation, near neighbor search, etc.  6  
embedding with assumptions  A very easy case: Assume that (cid:126)x1, . . . , (cid:126)xn all lie on the 1st axis in Rd .  7  
embedding with assumptions  A very easy case: Assume that (cid:126)x1, . . . , (cid:126)xn all lie on the 1st axis in Rd .  Set m = 1 and ˜xi = [(cid:126)xi (1)] (i.e., ˜xi contains just a single number).  • (cid:107)˜xi − ˜xj(cid:107)2 =(cid:112)[(cid:126)xi (1) − (cid:126)xj (1)]2 = |(cid:126)xi (1) − (cid:126)xj (1)| = (cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  7  
embedding with assumptions  A very easy case: Assume that (cid:126)x1, . . . , (cid:126)xn all lie on the 1st axis in Rd .  Set m = 1 and ˜xi = [(cid:126)xi (1)] (i.e., ˜xi contains just a single number).  • (cid:107)˜xi − ˜xj(cid:107)2 =(cid:112)[(cid:126)xi (1) − (cid:126)xj (1)]2 = |(cid:126)xi (1) − (cid:126)xj (1)| = (cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  • An embedding with no distortion from any d into m = 1.  7  
embedding with assumptions  A very easy case: Assume that (cid:126)x1, . . . , (cid:126)xn all lie on the 1st axis in Rd .  Set m = 1 and ˜xi = [(cid:126)xi (1)] (i.e., ˜xi contains just a single number).  • (cid:107)˜xi − ˜xj(cid:107)2 =(cid:112)[(cid:126)xi (1) − (cid:126)xj (1)]2 = |(cid:126)xi (1) − (cid:126)xj (1)| = (cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  • An embedding with no distortion from any d into m = 1. • More generally. there’s a no distortion embedding into m = D  dimensions if all the points lie is a D dimensional space.  7  
embedding with no assumptions  What about when we don’t make any assumptions on (cid:126)x1, . . . , (cid:126)xn. I.e., they can be scattered arbitrarily around d-dimensional space? • Can we ﬁnd a no-distortion embedding into m (cid:28) d dimensions?  8  
embedding with no assumptions  What about when we don’t make any assumptions on (cid:126)x1, . . . , (cid:126)xn. I.e., they can be scattered arbitrarily around d-dimensional space? • Can we ﬁnd a no-distortion embedding into m (cid:28) d dimensions?  No. Require m = d.  8  
embedding with no assumptions  What about when we don’t make any assumptions on (cid:126)x1, . . . , (cid:126)xn. I.e., they can be scattered arbitrarily around d-dimensional space? • Can we ﬁnd a no-distortion embedding into m (cid:28) d dimensions?  No. Require m = d.  • Can we ﬁnd an -distortion embedding into m (cid:28) d dimensions  for  > 0? For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  8  
embedding with no assumptions  What about when we don’t make any assumptions on (cid:126)x1, . . . , (cid:126)xn. I.e., they can be scattered arbitrarily around d-dimensional space? • Can we ﬁnd a no-distortion embedding into m (cid:28) d dimensions?  No. Require m = d.  • Can we ﬁnd an -distortion embedding into m (cid:28) d dimensions  for  > 0? Yes! Always, with m depending on . For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  8  
the johnson-lindenstrauss lemma  Johnson-Lindenstrauss Lemma: set of points (cid:126)x1, . . . , (cid:126)xn ∈ Rd and  > 0 there exists a linear map Π : Rd → Rm such that m = O  and letting ˜xi = Π(cid:126)xi :  For any  (cid:16) log n  (cid:17)  2  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2. Further, if Π ∈ Rm×d has each entry chosen i.i.d. from N (0, 1/m), it satisﬁes the guarantee with high probability.  9  
the johnson-lindenstrauss lemma  Johnson-Lindenstrauss Lemma: set of points (cid:126)x1, . . . , (cid:126)xn ∈ Rd and  > 0 there exists a linear map Π : Rd → Rm such that m = O  and letting ˜xi = Π(cid:126)xi :  For any  (cid:16) log n  (cid:17)  2  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2. Further, if Π ∈ Rm×d has each entry chosen i.i.d. from N (0, 1/m), it satisﬁes the guarantee with high probability.  For d = 1 trillion,  = .05, and n = 100, 000, m ≈ 6600.  9  
the johnson-lindenstrauss lemma  Johnson-Lindenstrauss Lemma: set of points (cid:126)x1, . . . , (cid:126)xn ∈ Rd and  > 0 there exists a linear map Π : Rd → Rm such that m = O  and letting ˜xi = Π(cid:126)xi :  For any  (cid:16) log n  (cid:17)  2  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2. Further, if Π ∈ Rm×d has each entry chosen i.i.d. from N (0, 1/m), it satisﬁes the guarantee with high probability.  For d = 1 trillion,  = .05, and n = 100, 000, m ≈ 6600.  Very surprising! Powerful result with a simple construction: applying a random linear transformation to a set of points preserves distances between all those points with high probability.  9  
random projection  For any (cid:126)x1, . . . , (cid:126)xn and Π ∈ Rm×d with each entry chosen i.i.d. from N (0, 1/m), with high probability, letting xi = Π(cid:126)xi :  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  10  
random projection  For any (cid:126)x1, . . . , (cid:126)xn and Π ∈ Rm×d with each entry chosen i.i.d. from N (0, 1/m), with high probability, letting xi = Π(cid:126)xi :  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  • Π is known as a random projection. It is a random linear function,  mapping length d vectors to length m vectors.  10  
random projection  For any (cid:126)x1, . . . , (cid:126)xn and Π ∈ Rm×d with each entry chosen i.i.d. from N (0, 1/m), with high probability, letting xi = Π(cid:126)xi :  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  • Π is known as a random projection. It is a random linear function,  mapping length d vectors to length m vectors.  • Π is data oblivious. Stark contrast to methods like PCA.  10  
algorithmic considerations  • Many alternative constructions: ±1 entries, sparse (most entries 0), Fourier structured, etc. =⇒ more eﬃcient computation of xi = Π(cid:126)xi .  11  
algorithmic considerations  • Many alternative constructions: ±1 entries, sparse (most entries 0), Fourier structured, etc. =⇒ more eﬃcient computation of xi = Π(cid:126)xi .  • Data oblivious property means that once Π is chosen, x1, . . . , xn  can be computed in a stream with little memory.  • Storage is just O(nm) rather than O(nd).  11  
algorithmic considerations  • Many alternative constructions: ±1 entries, sparse (most entries 0), Fourier structured, etc. =⇒ more eﬃcient computation of xi = Π(cid:126)xi .  • Data oblivious property means that once Π is chosen, x1, . . . , xn  can be computed in a stream with little memory.  • Storage is just O(nm) rather than O(nd). • Compression can be performed in parallel on diﬀerent servers.  11  
algorithmic considerations  • Many alternative constructions: ±1 entries, sparse (most entries 0), Fourier structured, etc. =⇒ more eﬃcient computation of xi = Π(cid:126)xi .  • Data oblivious property means that once Π is chosen, x1, . . . , xn  can be computed in a stream with little memory.  • Storage is just O(nm) rather than O(nd). • Compression can be performed in parallel on diﬀerent servers. • When new data points are added, can be easily compressed,  without updating existing points.  11  
distributional jl  The Johnson-Lindenstrauss Lemma is a direct consequence of a closely related lemma:  (cid:16) log(1/δ)  (cid:17)  Distributional JL Lemma: Let Π ∈ Rm×d have each entry chosen i.i.d. as N (0, 1/m). , then for any (cid:126)y ∈ Rd , with probability ≥ 1 − δ  If we set m = O  2  (1 − )(cid:107)(cid:126)y(cid:107)2 ≤ (cid:107)Π(cid:126)y(cid:107)2 ≤ (1 + )(cid:107)(cid:126)y(cid:107)2  Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : embedding error, δ: embedding failure prob.  12  
distributional jl  The Johnson-Lindenstrauss Lemma is a direct consequence of a closely related lemma:  (cid:16) log(1/δ)  (cid:17)  Distributional JL Lemma: Let Π ∈ Rm×d have each entry chosen i.i.d. as N (0, 1/m). , then for any (cid:126)y ∈ Rd , with probability ≥ 1 − δ  If we set m = O  2  (1 − )(cid:107)(cid:126)y(cid:107)2 ≤ (cid:107)Π(cid:126)y(cid:107)2 ≤ (1 + )(cid:107)(cid:126)y(cid:107)2  Applying a random matrix Π to any vector (cid:126)y preserves (cid:126)y ’s norm with high probability.  Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : embedding error, δ: embedding failure prob.  12  
distributional jl  The Johnson-Lindenstrauss Lemma is a direct consequence of a closely related lemma:  (cid:16) log(1/δ)  (cid:17)  Distributional JL Lemma: Let Π ∈ Rm×d have each entry chosen i.i.d. as N (0, 1/m). , then for any (cid:126)y ∈ Rd , with probability ≥ 1 − δ  If we set m = O  2  (1 − )(cid:107)(cid:126)y(cid:107)2 ≤ (cid:107)Π(cid:126)y(cid:107)2 ≤ (1 + )(cid:107)(cid:126)y(cid:107)2  Applying a random matrix Π to any vector (cid:126)y preserves (cid:126)y ’s norm with high probability. • Like a low-distortion embedding, but for the length of a compressed  vector rather than distances between vectors.  Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : embedding error, δ: embedding failure prob.  12  
distributional jl =⇒ jl  Distributional JL Lemma =⇒ JL Lemma: Distributional JL show that a random projection Π preserves the norm of any y . The main JL Lemma says that Π preserves distances between vectors.  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  13  
distributional jl =⇒ jl  Distributional JL Lemma =⇒ JL Lemma: Distributional JL show that a random projection Π preserves the norm of any y . The main JL Lemma says that Π preserves distances between vectors.  Since Π is linear these are the same thing!  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  13  
distributional jl =⇒ jl  Distributional JL Lemma =⇒ JL Lemma: Distributional JL show that a random projection Π preserves the norm of any y . The main JL Lemma says that Π preserves distances between vectors.  Since Π is linear these are the same thing!  Proof: Given (cid:126)x1, . . . , (cid:126)xn, deﬁne(cid:0)n  (cid:1) vectors (cid:126)yij where (cid:126)yij = (cid:126)xi − (cid:126)xj .  2  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  13  
distributional jl =⇒ jl  Distributional JL Lemma =⇒ JL Lemma: Distributional JL show that a random projection Π preserves the norm of any y . The main JL Lemma says that Π preserves distances between vectors.  Since Π is linear these are the same thing!  Proof: Given (cid:126)x1, . . . , (cid:126)xn, deﬁne(cid:0)n  (cid:1) vectors (cid:126)yij where (cid:126)yij = (cid:126)xi − (cid:126)xj .  2  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  13  
distributional jl =⇒ jl  Distributional JL Lemma =⇒ JL Lemma: Distributional JL show that a random projection Π preserves the norm of any y . The main JL Lemma says that Π preserves distances between vectors.  Since Π is linear these are the same thing!  Proof: Given (cid:126)x1, . . . , (cid:126)xn, deﬁne(cid:0)n (cid:1) vectors (cid:126)yij where (cid:126)yij = (cid:126)xi − (cid:126)xj . (cid:17) (cid:16) log 1/δ(cid:48)  • If we choose Π with m = O  2  2  , for each (cid:126)yij with probability  ≥ 1 − δ(cid:48) we have:  (1 − )(cid:107)(cid:126)yij(cid:107)2 ≤ (cid:107)Π(cid:126)yij(cid:107)2 ≤ (1 + )(cid:107)(cid:126)yij(cid:107)2  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  13  
distributional jl =⇒ jl  Distributional JL Lemma =⇒ JL Lemma: Distributional JL show that a random projection Π preserves the norm of any y . The main JL Lemma says that Π preserves distances between vectors.  Since Π is linear these are the same thing!  Proof: Given (cid:126)x1, . . . , (cid:126)xn, deﬁne(cid:0)n (cid:1) vectors (cid:126)yij where (cid:126)yij = (cid:126)xi − (cid:126)xj . (cid:17) (cid:16) log 1/δ(cid:48)  • If we choose Π with m = O  2  2  , for each (cid:126)yij with probability  ≥ 1 − δ(cid:48) we have:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)Π((cid:126)xi − (cid:126)xj )(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  13  
distributional jl =⇒ jl  Distributional JL Lemma =⇒ JL Lemma: Distributional JL show that a random projection Π preserves the norm of any y . The main JL Lemma says that Π preserves distances between vectors.  Since Π is linear these are the same thing!  Proof: Given (cid:126)x1, . . . , (cid:126)xn, deﬁne(cid:0)n (cid:1) vectors (cid:126)yij where (cid:126)yij = (cid:126)xi − (cid:126)xj . (cid:17) (cid:16) log 1/δ(cid:48)  • If we choose Π with m = O  2  2  , for each (cid:126)yij with probability  ≥ 1 − δ(cid:48) we have:  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  13  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  With what probability are all pairwise distances preserved?  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  With what probability are all pairwise distances preserved?  Union bound: With probability ≥ 1 −(cid:0)n  (cid:1) · δ(cid:48) all pairwise distances are  2  preserved.  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  With what probability are all pairwise distances preserved?  Union bound: With probability ≥ 1 −(cid:0)n Apply the claim with δ(cid:48) = δ/(cid:0)n  preserved.  (cid:1).  2  2  (cid:1) · δ(cid:48) all pairwise distances are  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  With what probability are all pairwise distances preserved?  Union bound: With probability ≥ 1 −(cid:0)n Apply the claim with δ(cid:48) = δ/(cid:0)n  (cid:1). =⇒ for m = O  distances are preserved with probability ≥ 1 − δ.  preserved.  2  2  (cid:16) log(1/δ(cid:48))  (cid:17)  2  (cid:1) · δ(cid:48) all pairwise distances are  , all pairwise  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  With what probability are all pairwise distances preserved?  Union bound: With probability ≥ 1 −(cid:0)n Apply the claim with δ(cid:48) = δ/(cid:0)n  (cid:1). =⇒ for m = O  distances are preserved with probability ≥ 1 − δ.  preserved.  2  2  (cid:16) log(1/δ(cid:48))  (cid:17)  2  (cid:1) · δ(cid:48) all pairwise distances are  , all pairwise  (cid:18) log(1/δ(cid:48))  (cid:19)  2  m = O  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  (cid:1) · δ(cid:48) all pairwise distances are  With what probability are all pairwise distances preserved?  preserved.  Union bound: With probability ≥ 1 −(cid:0)n Apply the claim with δ(cid:48) = δ/(cid:0)n (cid:1). =⇒ for m = O (cid:32) (cid:1)/δ) log((cid:0)n  distances are preserved with probability ≥ 1 − δ.  (cid:18) log(1/δ(cid:48))  (cid:33)  (cid:19)  2  2  (cid:16) log(1/δ(cid:48))  (cid:17)  2  , all pairwise  m = O  = O  2  2 2  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  (cid:1) · δ(cid:48) all pairwise distances are  2  With what probability are all pairwise distances preserved?  preserved.  Union bound: With probability ≥ 1 −(cid:0)n Apply the claim with δ(cid:48) = δ/(cid:0)n (cid:1). =⇒ for m = O (cid:32) (cid:1)/δ) log((cid:0)n  distances are preserved with probability ≥ 1 − δ.  (cid:18) log(1/δ(cid:48))  (cid:33)  (cid:19)  2  (cid:16) log(1/δ(cid:48)) (cid:17) (cid:19) (cid:18) log(n2/δ)  2  , all pairwise  m = O  = O  2  2 2  = O  2  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  (cid:1) · δ(cid:48) all pairwise distances are  With what probability are all pairwise distances preserved?  preserved.  Union bound: With probability ≥ 1 −(cid:0)n Apply the claim with δ(cid:48) = δ/(cid:0)n (cid:1). =⇒ for m = O (cid:32) (cid:1)/δ) log((cid:0)n  distances are preserved with probability ≥ 1 − δ.  (cid:18) log(1/δ(cid:48))  (cid:33)  (cid:19)  2  2  (cid:16) log(1/δ(cid:48)) (cid:17) (cid:19) (cid:18) log(n2/δ)  2  m = O  = O  2  2 2  = O  2  (cid:126)x1, . . . , (cid:126)xn: original points, x1, . . . , xn: compressed points, Π ∈ Rm×d : random projection matrix. d: original dimension. m: compressed dimension, : em- bedding error, δ: embedding failure prob.  , all pairwise  (cid:18) log(n/δ)  (cid:19)  = O  2  14  
distributional jl =⇒ jl  (cid:16) log(1/δ(cid:48))  (cid:17)  Claim: If we choose Π with i.i.d. N (0, 1/m) entries and m = O ≥ 1 − δ(cid:48) we have:  2  , letting xi = Π(cid:126)xi , for each pair (cid:126)xi , (cid:126)xj with probability  (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)xi − xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  (cid:1) · δ(cid:48) all pairwise distances are  With what probability are all pairwise distances preserved?  preserved.  Union bound: With probability ≥ 1 −(cid:0)n Apply the claim with δ(cid:48) = δ/(cid:0)n (cid:1). =⇒ for m = O (cid:32) (cid:1)/δ) log((cid:0)n  distances are preserved with probability ≥ 1 − δ.  (cid:18) log(1/δ(cid:48))  (cid:33)  (cid:19)  2  2  (cid:16) log(1/δ(cid:48)) (cid:17) (cid:19) (cid:18) log(n2/δ)  2  m = O  = O  2  2 2  = O  2  Yields the JL lemma.  , all pairwise  (cid:18) log(n/δ)  (cid:19)  = O  2  14  
