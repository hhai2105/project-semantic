compsci 514: algorithms for data science  Andrew McGregor  Lecture 10  0  
(, k)-frequent items problem  Given stream of n items x1, . . . , xn where each xi ∈ U. Return a set F , such that for every x ∈ U: 1. If f (x) ≥ n/k then x ∈ F 2. If f (x) < (1 − )n/k then x (cid:54)∈ F  where f (x) is the number of times x appears in the stream.  1  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  2  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  2  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  2  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  2  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  2  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  2  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  2  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  Use A[h(x)] to estimate f (x), the frequency of x in the stream. • Claim: A[h(x)] ≥ f (x). • Claim: A[h(x)] ≤ f (x) + 2n/m with probability at least 1/2.  2  
frequent elements with count-min sketch  Count-Min Sketch: A random hashing based method closely related to bloom ﬁlters.  Use A[h(x)] to estimate f (x), the frequency of x in the stream. • Claim: A[h(x)] ≥ f (x). • Claim: A[h(x)] ≤ f (x) + 2n/m with probability at least 1/2. How can we increase this probability to 1 − δ for arbitrary δ > 0?  2  
count-min sketch accuracy  3  
count-min sketch accuracy  3  
count-min sketch accuracy  3  
count-min sketch accuracy  3  
count-min sketch accuracy  3  
count-min sketch accuracy  3  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)].  3  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)]. • What is Pr[f (x) ≤ ˜f (x) ≤ f (x) + 2n/m]?  3  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)]. • What is Pr[f (x) ≤ ˜f (x) ≤ f (x) + 2n/m]? Answer: ≥ 1 − 1/2t.  3  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)]. • What is Pr[f (x) ≤ ˜f (x) ≤ f (x) + 2n/m]? Answer: ≥ 1 − 1/2t. • Setting t = log(1/δ) ensures probability is at least 1 − δ.  3  
count-min sketch accuracy  • Estimate f (x) with ˜f (x) = mini∈[t] Ai [hi (x)]. • What is Pr[f (x) ≤ ˜f (x) ≤ f (x) + 2n/m]? Answer: ≥ 1 − 1/2t. • Setting t = log(1/δ) ensures probability is at least 1 − δ. • Setting m = 2k/ ensures the error 2n/m is n/k and this is enough to determine whether we need to output the element.  3  
count-min sketch  Upshot: Count-min sketch lets us estimate the frequency of every item in a stream up to error n O (log(1/δ) · k/) space.  k with probability ≥ 1 − δ in  4  
count-min sketch  k with probability ≥ 1 − δ in  Upshot: Count-min sketch lets us estimate the frequency of every item in a stream up to error n O (log(1/δ) · k/) space. • Accurate enough to solve the (, k)-Frequent elements problem: k and those with  Can distinguish between items with frequency n frequency < (1 − ) n k .  4  
count-min sketch  k with probability ≥ 1 − δ in  Upshot: Count-min sketch lets us estimate the frequency of every item in a stream up to error n O (log(1/δ) · k/) space. • Accurate enough to solve the (, k)-Frequent elements problem: k and those with • How should we set δ if we want a good estimate for all items at  Can distinguish between items with frequency n frequency < (1 − ) n k .  once, with 99% probability?  4  
count-min sketch  k with probability ≥ 1 − δ in  Upshot: Count-min sketch lets us estimate the frequency of every item in a stream up to error n O (log(1/δ) · k/) space. • Accurate enough to solve the (, k)-Frequent elements problem: k and those with • How should we set δ if we want a good estimate for all items at  Can distinguish between items with frequency n frequency < (1 − ) n k .  once, with 99% probability? δ = 0.01/|U| ensures  Pr[there exists x ∈ U with a bad estimate]  Pr[estimate for x is bad] ≤(cid:88)  ≤(cid:88)  x∈U  x∈U  0.01/|U| = 0.01  4  
identifying frequent elements  Count-min sketch gives an accurate frequency estimate for every item in the stream. But how do we identify the frequent items without having to look up the estimated frequency for x ∈ U?  5  
identifying frequent elements  Count-min sketch gives an accurate frequency estimate for every item in the stream. But how do we identify the frequent items without having to look up the estimated frequency for x ∈ U?  One approach: • Maintain a set F while processing the stream: • At step i:  and it isn’t already in F .  • Add ith stream element to F if it’s estimated frequency is ≥ i/k • Remove any element from F whose estimated frequency is < i/k. • Store at most k items at once and have all items with frequency  ≥ n/k stored at the end of the stream.  5  
Questions on Frequent Elements?  6  
high dimensional data  ‘Big Data’ means not just many data points, but many measurements per data point. I.e., very high dimensional data.  7  
high dimensional data  ‘Big Data’ means not just many data points, but many measurements per data point. I.e., very high dimensional data. • Twitter has 321 million active monthly users. Records (tens of)  thousands of measurements per user: who they follow, who follows them, when they last visited the site, timestamps for speciﬁc interactions, how many tweets they have sent, the text of those tweets, etc.  7  
high dimensional data  ‘Big Data’ means not just many data points, but many measurements per data point. I.e., very high dimensional data. • Twitter has 321 million active monthly users. Records (tens of)  thousands of measurements per user: who they follow, who follows them, when they last visited the site, timestamps for speciﬁc interactions, how many tweets they have sent, the text of those tweets, etc.  • A 3 minute Youtube clip with a resolution of 500 × 500 pixels at 15 frames/second with 3 color channels is a recording of ≥ 2 billion pixel values. Even a 500 × 500 pixel color image has 750, 000 pixel values.  7  
high dimensional data  ‘Big Data’ means not just many data points, but many measurements per data point. I.e., very high dimensional data. • Twitter has 321 million active monthly users. Records (tens of)  thousands of measurements per user: who they follow, who follows them, when they last visited the site, timestamps for speciﬁc interactions, how many tweets they have sent, the text of those tweets, etc.  • A 3 minute Youtube clip with a resolution of 500 × 500 pixels at 15 frames/second with 3 color channels is a recording of ≥ 2 billion pixel values. Even a 500 × 500 pixel color image has 750, 000 pixel values. • The human genome contains 3 billion+ base pairs. Genetic datasets  often contain information on 100s of thousands+ mutations and genetic markers.  7  
data as vectors and matrices  In data analysis and machine learning, data points with many attributes are often stored, processed, and interpreted as high dimensional vectors, with real valued entries.  8  
data as vectors and matrices  In data analysis and machine learning, data points with many attributes are often stored, processed, and interpreted as high dimensional vectors, with real valued entries.  8  
data as vectors and matrices  In data analysis and machine learning, data points with many attributes are often stored, processed, and interpreted as high dimensional vectors, with real valued entries.  Similarities/distances between vectors (e.g., (cid:104)x, y(cid:105), (cid:107)x − y(cid:107)2) have meaning for underlying data points.  8  
datasets as vectors and matrices  Data points are interpreted as high dimensional vectors, with real valued entries. Data set is interpreted as a matrix. Data Points: (cid:126)x1, (cid:126)x2, . . . , (cid:126)xn ∈ Rd . Data Set: X ∈ Rn×d with i th rows equal to (cid:126)xi .  9  
datasets as vectors and matrices  Data points are interpreted as high dimensional vectors, with real valued entries. Data set is interpreted as a matrix. Data Points: (cid:126)x1, (cid:126)x2, . . . , (cid:126)xn ∈ Rd . Data Set: X ∈ Rn×d with i th rows equal to (cid:126)xi .  9  
datasets as vectors and matrices  Data points are interpreted as high dimensional vectors, with real valued entries. Data set is interpreted as a matrix. Data Points: (cid:126)x1, (cid:126)x2, . . . , (cid:126)xn ∈ Rd . Data Set: X ∈ Rn×d with i th rows equal to (cid:126)xi .  Many data points n =⇒ tall. Many dimensions d =⇒ wide.  9  
dimensionality reduction  Dimensionality Reduction: Compress data points so that they lie in many fewer dimensions.  10  
dimensionality reduction  Dimensionality Reduction: Compress data points so that they lie in many fewer dimensions.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd → ˜x1, . . . , ˜xn ∈ Rm for m (cid:28) d.  10  
dimensionality reduction  Dimensionality Reduction: Compress data points so that they lie in many fewer dimensions.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd → ˜x1, . . . , ˜xn ∈ Rm for m (cid:28) d.  ‘Lossy compression’ that still preserves important information about the relationships between (cid:126)x1, . . . , (cid:126)xn.  10  
dimensionality reduction  Dimensionality Reduction: Compress data points so that they lie in many fewer dimensions.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd → ˜x1, . . . , ˜xn ∈ Rm for m (cid:28) d.  ‘Lossy compression’ that still preserves important information about the relationships between (cid:126)x1, . . . , (cid:126)xn.  Generally will not consider directly how well ˜xi approximates (cid:126)xi .  10  
low distortion embedding  Low Distortion Embedding: Given (cid:126)x1, . . . , (cid:126)xn ∈ Rd , distance function D, and error parameter  ≥ 0, ﬁnd ˜x1, . . . , ˜xn ∈ Rm (where m (cid:28) d) and distance function ˜D such that for all i, j ∈ [n]:  (1 − )D((cid:126)xi , (cid:126)xj ) ≤ ˜D(˜xi , ˜xj ) ≤ (1 + )D((cid:126)xi , (cid:126)xj ).  We’ll focus on the case where D and ˜D are Euclidean distances. I.e., the distance between two vectors x and y is deﬁned as  (cid:115)(cid:88)  (cid:107)(cid:126)x − (cid:126)y(cid:107)2 =  ((cid:126)x(i) − (cid:126)y (i))2  This is related to the Euclidean norm, (cid:107)(cid:126)z(cid:107)2 =  i  (cid:113)(cid:80)n  i=1 (cid:126)z(i)2.  11  
the johnson-lindenstrauss lemma  Johnson-Lindenstrauss Lemma: set of points (cid:126)x1, . . . , (cid:126)xn ∈ Rd and  > 0 there exists a linear map M : Rd → Rm such that m = O  and letting ˜xi = M(cid:126)xi :  For any  (cid:16) log n  (cid:17)  2  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  12  
the johnson-lindenstrauss lemma  Johnson-Lindenstrauss Lemma: set of points (cid:126)x1, . . . , (cid:126)xn ∈ Rd and  > 0 there exists a linear map M : Rd → Rm such that m = O  and letting ˜xi = M(cid:126)xi :  For any  (cid:16) log n  (cid:17)  2  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  For d = 1 trillion,  = .05, and n = 100, 000, m ≈ 6600.  12  
the johnson-lindenstrauss lemma  Johnson-Lindenstrauss Lemma: set of points (cid:126)x1, . . . , (cid:126)xn ∈ Rd and  > 0 there exists a linear map M : Rd → Rm such that m = O  and letting ˜xi = M(cid:126)xi :  For any  (cid:16) log n  (cid:17)  2  For all i, j : (1 − )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2 ≤ (cid:107)˜xi − ˜xj(cid:107)2 ≤ (1 + )(cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  For d = 1 trillion,  = .05, and n = 100, 000, m ≈ 6600.  Very surprising! Powerful result with a simple construction: applying a random linear transformation to a set of points preserves distances between all those points with high probability.  12  
