compsci 514: algorithms for data science  Andrew McGregor  Lecture 22  0  
summary  Last Class: Fast computation of the SVD/eigendecomposition. • Power method for computing the top singular vector of a matrix. • Power method is a simple iterative algorithm for solving the  non-convex optimization problem max(cid:126)v :(cid:107)(cid:126)v(cid:107)2  2=1 |(cid:126)v T A(cid:126)v|  Final Two Weeks of Class: • More general iterative algorithms for optimization, speciﬁcally gradient  descent and its variants.  • What are these methods, when are they applied, and how do you  analyze their performance?  • Small taste of what you can ﬁnd in COMPSCI 590OP or 690OP.  1  
discrete vs. continuous optimization  Discrete (Combinatorial) Optimization: • Graph Problems: min-cut, max-cut, max ﬂow, shortest path,  (traditional CS algorithms)  matchings, maximum independent set, traveling salesman problem  • Problems with discrete constraints or outputs: bin-packing, scheduling,  sequence alignment, submodular maximization  • Generally searching over a ﬁnite but exponentially large set of possible  solutions. Many of these problems are NP-Hard.  (maybe seen in ML/advanced algorithms)  Continuous Optimization: • Unconstrained convex and non-convex optimization. • Linear programming, quadratic programming, semideﬁnite  programming  2  
continuous optimization examples  3  
mathematical setup  Given some function f : Rd → R, ﬁnd (cid:126)θ(cid:63) with:  f ((cid:126)θ(cid:63)) = min (cid:126)θ∈R d  f ((cid:126)θ)  4  
mathematical setup  Given some function f : Rd → R, ﬁnd (cid:126)θ(cid:63) with:  f ((cid:126)θ(cid:63)) = min (cid:126)θ∈R d  f ((cid:126)θ) +   Typically up to some small approximation factor.  4  
mathematical setup  Given some function f : Rd → R, ﬁnd (cid:126)θ(cid:63) with:  f ((cid:126)θ(cid:63)) = min (cid:126)θ∈R d  f ((cid:126)θ) +   Typically up to some small approximation factor.  Often under some constraints: • (cid:107)(cid:126)θ(cid:107)2 ≤ 1, (cid:107)(cid:126)θ(cid:107)1 ≤ 1. • A(cid:126)θ ≤ (cid:126)b, (cid:126)θT A(cid:126)θ ≥ 0.  • (cid:80)d  (cid:126)θ(i) ≤ c.  i=1  4  
why continuous optimization?  Modern machine learning centers around continuous optimization.  Typical Set Up: (supervised machine learning) • Have a model, which is a function mapping inputs to predictions  (neural network, linear function, low-degree polynomial etc).  • The model is parameterized by a parameter vector (weights in a neural  network, coeﬃcients in a linear function or polynomial)  • Want to train this model on input data, by picking a parameter vector such that the model does a good job mapping inputs to predictions on your training data.  This training step is typically formulated as a continuous optimization problem.  5  
optimization in ml  Example 1: Linear Regression, e.g., predicting house prices based on d features (sq. footage, average price of houses in neighborhood. . . )  6  
optimization in ml  Example 1: Linear Regression, e.g., predicting house prices based on d features (sq. footage, average price of houses in neighborhood. . . )  Model: M(cid:126)θ : Rd → R with M(cid:126)θ((cid:126)x) def= (cid:126)θ(1) · (cid:126)x(1) + . . . + (cid:126)θ(d) · (cid:126)x(d).  6  
optimization in ml  Example 1: Linear Regression, e.g., predicting house prices based on d features (sq. footage, average price of houses in neighborhood. . . )  Model: M(cid:126)θ : Rd → R with M(cid:126)θ((cid:126)x) def= (cid:126)θ(1) · (cid:126)x(1) + . . . + (cid:126)θ(d) · (cid:126)x(d).  6  
optimization in ml  Example 1: Linear Regression, e.g., predicting house prices based on d features (sq. footage, average price of houses in neighborhood. . . )  Model: M(cid:126)θ : Rd → R with M(cid:126)θ((cid:126)x) def= (cid:126)θ(1) · (cid:126)x(1) + . . . + (cid:126)θ(d) · (cid:126)x(d). Parameter Vector: (cid:126)θ ∈ Rd (the regression coeﬃcients)  6  
optimization in ml  Example 1: Linear Regression, e.g., predicting house prices based on d features (sq. footage, average price of houses in neighborhood. . . )  Model: M(cid:126)θ : Rd → R with M(cid:126)θ((cid:126)x) def= (cid:126)θ(1) · (cid:126)x(1) + . . . + (cid:126)θ(d) · (cid:126)x(d). Parameter Vector: (cid:126)θ ∈ Rd (the regression coeﬃcients)  Optimization Problem: Given data points (training points) (cid:126)x1, . . . , (cid:126)xn (the rows of data matrix X ∈ Rn×d ) and labels y1, . . . , yn ∈ R, ﬁnd (cid:126)θ∗ minimizing the loss function:  n(cid:88)  L((cid:126)θ, X, (cid:126)y ) =  (cid:96)(M(cid:126)θ((cid:126)xi ), yi )  where (cid:96) is some measurement of how far M(cid:126)θ((cid:126)xi ) is from yi .  i=1  6  
optimization in ml  Example 1: Linear Regression, e.g., predicting house prices based on d features (sq. footage, average price of houses in neighborhood. . . )  Model: M(cid:126)θ : Rd → R with M(cid:126)θ((cid:126)x) def= (cid:126)θ(1) · (cid:126)x(1) + . . . + (cid:126)θ(d) · (cid:126)x(d). Parameter Vector: (cid:126)θ ∈ Rd (the regression coeﬃcients)  Optimization Problem: Given data points (training points) (cid:126)x1, . . . , (cid:126)xn (the rows of data matrix X ∈ Rn×d ) and labels y1, . . . , yn ∈ R, ﬁnd (cid:126)θ∗ minimizing the loss function:  where (cid:96) is some measurement of how far M(cid:126)θ((cid:126)xi ) is from yi .  • (cid:96)(M(cid:126)θ((cid:126)xi ), yi ) =(cid:0)M(cid:126)θ((cid:126)xi ) − yi • yi ∈ {−1, 1} and (cid:96)(M(cid:126)θ((cid:126)xi ), yi ) = ln(cid:0)1 + exp(−yi M(cid:126)θ((cid:126)xi ))(cid:1) (logistic  (least squares regression)  (cid:1)2  regression)  6  n(cid:88)  i=1  L((cid:126)θ, X, (cid:126)y ) =  (cid:96)(M(cid:126)θ((cid:126)xi ), yi )  
optimization in ml  Example 1: Linear Regression, e.g., predicting house prices based on d features (sq. footage, average price of houses in neighborhood. . . )  Model: M(cid:126)θ : Rd → R with M(cid:126)θ((cid:126)x) def= (cid:126)θ(1) · (cid:126)x(1) + . . . + (cid:126)θ(d) · (cid:126)x(d). Parameter Vector: (cid:126)θ ∈ Rd (the regression coeﬃcients)  Optimization Problem: Given data points (training points) (cid:126)x1, . . . , (cid:126)xn (the rows of data matrix X ∈ Rn×d ) and labels y1, . . . , yn ∈ R, ﬁnd (cid:126)θ∗ minimizing the loss function:  where (cid:96) is some measurement of how far M(cid:126)θ((cid:126)xi ) is from yi .  • (cid:96)(M(cid:126)θ((cid:126)xi ), yi ) =(cid:0)M(cid:126)θ((cid:126)xi ) − yi • yi ∈ {−1, 1} and (cid:96)(M(cid:126)θ((cid:126)xi ), yi ) = ln(cid:0)1 + exp(−yi M(cid:126)θ((cid:126)xi ))(cid:1) (logistic  (least squares regression)  (cid:1)2  regression)  6  LX,y ((cid:126)θ) = L((cid:126)θ, X, (cid:126)y ) =  (cid:96)(M(cid:126)θ((cid:126)xi ), yi )  n(cid:88)  i=1  
optimization in ml  Example 2: Neural Networks  Model: M(cid:126)θ : Rd → R. M(cid:126)θ((cid:126)x) = (cid:104)(cid:126)wout, σ(W2σ(W1(cid:126)x))(cid:105). Parameter Vector: (cid:126)θ ∈ R(# edges) (the weights on every edge)  Optimization Problem: Given data points (cid:126)x1, . . . , (cid:126)xn and labels z1, . . . , zn ∈ R, ﬁnd (cid:126)θ∗ minimizing the loss function:  n(cid:88)  LX,(cid:126)y ((cid:126)θ) =  (cid:96)(M(cid:126)θ((cid:126)xi ), zi )  i=1  7  
optimization in ml  n(cid:88)  i=1  LX,(cid:126)y ((cid:126)θ) =  (cid:96)(M(cid:126)θ((cid:126)xi ), yi )  • Supervised means we have labels y1, . . . , yn for the training points. • Solving the ﬁnal optimization problem has many diﬀerent names: likelihood maximization, empirical risk minimization, minimizing training loss, etc.  • Continuous optimization is also very common in unsupervised learning.  (PCA, spectral clustering, etc.)  • Generalization tries to explain why minimizing the loss LX,(cid:126)y ((cid:126)θ) on the training points minimizes the loss on future test points. I.e., makes us have good predictions on future inputs.  8  
optimization algorithms  Choice of optimization algorithm for minimizing f ((cid:126)θ) will depend on many things: • The form of f (in ML, depends on the model & loss function). • Any constraints on (cid:126)θ (e.g., (cid:107)(cid:126)θ(cid:107) < c). • Computational constraints, such as memory constraints.  n(cid:88)  LX,(cid:126)y ((cid:126)θ) =  (cid:96)(M(cid:126)θ((cid:126)xi ), yi )  i=1  9  
gradient descent  Next few classes: Gradient descent (and some important variants) • An extremely simple greedy iterative method, that can be applied to  almost any continuous function we care about optimizing.  • Often not the ‘best’ choice for any given function, but it is the  approach of choice in ML since it is simple, general, and often works very well.  • At each step, tries to move towards the lowest nearby point in the  function that is can – in the opposite direction of the gradient.  10  
multivariate calculus review  (cid:124)  (cid:126)ei = [0, 0, 1, 0, 0, . . . , 0]  1 at position i  Let (cid:126)ei ∈ Rd denote the i th standard basis vector,  (cid:123)(cid:122)  (cid:125)  .  11  
multivariate calculus review  Let (cid:126)ei ∈ Rd denote the i th standard basis vector,  (cid:126)ei = [0, 0, 1, 0, 0, . . . , 0]  .  1 at position i  (cid:124)  (cid:123)(cid:122)  (cid:125)  Partial Derivative:  ∂f ∂(cid:126)θ(i)  = lim →0  f ((cid:126)θ +  · (cid:126)ei ) − f ((cid:126)θ)    .  11  
multivariate calculus review  Let (cid:126)ei ∈ Rd denote the i th standard basis vector,  (cid:126)ei = [0, 0, 1, 0, 0, . . . , 0]  .  1 at position i  (cid:124)  (cid:123)(cid:122)  (cid:125)  Partial Derivative:  ∂f ∂(cid:126)θ(i)  = lim →0  f ((cid:126)θ +  · (cid:126)ei ) − f ((cid:126)θ)    Directional Derivative:  D(cid:126)v f ((cid:126)θ) = lim →0  f ((cid:126)θ + (cid:126)v ) − f ((cid:126)θ)    .  .  11  
multivariate calculus review  Gradient: Just a ‘list’ of the partial derivatives.    (cid:126)∇f ((cid:126)θ) =    ∂f  ∂(cid:126)θ(1)  ∂f  ∂(cid:126)θ(2)  ...  ∂f  ∂(cid:126)θ(d)  12  
multivariate calculus review  Gradient: Just a ‘list’ of the partial derivatives.    (cid:126)∇f ((cid:126)θ) =    ∂f  ∂(cid:126)θ(1)  ∂f  ∂(cid:126)θ(2)  ...  ∂f  ∂(cid:126)θ(d)  Directional Derivative in Terms of the Gradient:  D(cid:126)v f ((cid:126)θ) = (cid:104)(cid:126)v , (cid:126)∇f ((cid:126)θ)(cid:105).  12  
function access  Often the functions we are trying to optimize are very complex (e.g., a neural network). We will assume access to:  Function Evaluation: Can compute f ((cid:126)θ) for any (cid:126)θ. Gradient Evaluation: Can compute (cid:126)∇f ((cid:126)θ) for any (cid:126)θ.  13  
function access  Often the functions we are trying to optimize are very complex (e.g., a neural network). We will assume access to:  Function Evaluation: Can compute f ((cid:126)θ) for any (cid:126)θ. Gradient Evaluation: Can compute (cid:126)∇f ((cid:126)θ) for any (cid:126)θ.  In neural networks: • Function evaluation is called a forward pass (propogate an input  through the network).  • Gradient evaluation is called a backward pass (compute the  gradient via chain rule, using backpropagation).  13  
gradient descent greedy approach  Gradient descent is a greedy iterative optimization algorithm: Starting at (cid:126)θ(0), in each iteration let (cid:126)θ(i) = (cid:126)θ(i−1) + η(cid:126)v , where η is a (small) ‘step size’ and (cid:126)v is a direction chosen to minimize f ((cid:126)θ(i−1) + η(cid:126)v ).  14  
gradient descent greedy approach  Gradient descent is a greedy iterative optimization algorithm: Starting at (cid:126)θ(0), in each iteration let (cid:126)θ(i) = (cid:126)θ(i−1) + η(cid:126)v , where η is a (small) ‘step size’ and (cid:126)v is a direction chosen to minimize f ((cid:126)θ(i−1) + η(cid:126)v ).  D(cid:126)v f ((cid:126)θ) = lim →0  f ((cid:126)θ + (cid:126)v ) − f ((cid:126)θ)    .  14  
gradient descent greedy approach  Gradient descent is a greedy iterative optimization algorithm: Starting at (cid:126)θ(0), in each iteration let (cid:126)θ(i) = (cid:126)θ(i−1) + η(cid:126)v , where η is a (small) ‘step size’ and (cid:126)v is a direction chosen to minimize f ((cid:126)θ(i−1) + η(cid:126)v ). f ((cid:126)θ(i−1) + (cid:126)v ) − f ((cid:126)θ(i−1))  D(cid:126)v f ((cid:126)θ(i−1)) = lim →0    .  14  
gradient descent greedy approach  Gradient descent is a greedy iterative optimization algorithm: Starting at (cid:126)θ(0), in each iteration let (cid:126)θ(i) = (cid:126)θ(i−1) + η(cid:126)v , where η is a (small) ‘step size’ and (cid:126)v is a direction chosen to minimize f ((cid:126)θ(i−1) + η(cid:126)v ). f ((cid:126)θ(i−1) + (cid:126)v ) − f ((cid:126)θ(i−1))  D(cid:126)v f ((cid:126)θ(i−1)) = lim →0    .  So for small η: f ((cid:126)θ(i)) − f ((cid:126)θ(i−1)) = f ((cid:126)θ(i−1) + η(cid:126)v ) − f ((cid:126)θ(i−1))  14  
gradient descent greedy approach  Gradient descent is a greedy iterative optimization algorithm: Starting at (cid:126)θ(0), in each iteration let (cid:126)θ(i) = (cid:126)θ(i−1) + η(cid:126)v , where η is a (small) ‘step size’ and (cid:126)v is a direction chosen to minimize f ((cid:126)θ(i−1) + η(cid:126)v ). f ((cid:126)θ(i−1) + (cid:126)v ) − f ((cid:126)θ(i−1))  D(cid:126)v f ((cid:126)θ(i−1)) = lim →0    .  So for small η: f ((cid:126)θ(i)) − f ((cid:126)θ(i−1)) = f ((cid:126)θ(i−1) + η(cid:126)v ) − f ((cid:126)θ(i−1)) ≈ η · D(cid:126)v f ((cid:126)θ(i−1))  14  
gradient descent greedy approach  Gradient descent is a greedy iterative optimization algorithm: Starting at (cid:126)θ(0), in each iteration let (cid:126)θ(i) = (cid:126)θ(i−1) + η(cid:126)v , where η is a (small) ‘step size’ and (cid:126)v is a direction chosen to minimize f ((cid:126)θ(i−1) + η(cid:126)v ). f ((cid:126)θ(i−1) + (cid:126)v ) − f ((cid:126)θ(i−1))  D(cid:126)v f ((cid:126)θ(i−1)) = lim →0    .  So for small η: f ((cid:126)θ(i)) − f ((cid:126)θ(i−1)) = f ((cid:126)θ(i−1) + η(cid:126)v ) − f ((cid:126)θ(i−1)) ≈ η · D(cid:126)v f ((cid:126)θ(i−1))  = η · (cid:104)(cid:126)v , (cid:126)∇f ((cid:126)θ(i−1))(cid:105).  14  
gradient descent greedy approach  Gradient descent is a greedy iterative optimization algorithm: Starting at (cid:126)θ(0), in each iteration let (cid:126)θ(i) = (cid:126)θ(i−1) + η(cid:126)v , where η is a (small) ‘step size’ and (cid:126)v is a direction chosen to minimize f ((cid:126)θ(i−1) + η(cid:126)v ). f ((cid:126)θ(i−1) + (cid:126)v ) − f ((cid:126)θ(i−1))  D(cid:126)v f ((cid:126)θ(i−1)) = lim →0    .  So for small η: f ((cid:126)θ(i)) − f ((cid:126)θ(i−1)) = f ((cid:126)θ(i−1) + η(cid:126)v ) − f ((cid:126)θ(i−1)) ≈ η · D(cid:126)v f ((cid:126)θ(i−1))  = η · (cid:104)(cid:126)v , (cid:126)∇f ((cid:126)θ(i−1))(cid:105). We want to choose (cid:126)v minimizing (cid:104)(cid:126)v , (cid:126)∇f ((cid:126)θ(i−1))(cid:105) – i.e., pointing in the direction of (cid:126)∇f ((cid:126)θ(i−1)) but with the opposite sign.  14  
gradient descent pseudocode  Gradient Descent • Choose some initialization (cid:126)θ(0). • For i = 1, . . . , t  • (cid:126)θ(i) = (cid:126)θ(i−1) − η∇f ((cid:126)θ(i−1))  • Return (cid:126)θ(t), as an approximate minimizer of f ((cid:126)θ).  Step size η is chosen ahead of time or adapted during the algorithm (details to come.)  15  
gradient descent pseudocode  Gradient Descent • Choose some initialization (cid:126)θ(0). • For i = 1, . . . , t  • (cid:126)θ(i) = (cid:126)θ(i−1) − η∇f ((cid:126)θ(i−1))  • Return (cid:126)θ(t), as an approximate minimizer of f ((cid:126)θ).  Step size η is chosen ahead of time or adapted during the algorithm (details to come.) • For now assume η stays the same in each iteration.  15  
when does gradient descent work?  Gradient Descent Update: (cid:126)θi+1 = (cid:126)θi − η∇f ((cid:126)θi )  16  
