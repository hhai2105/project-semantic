compsci 514: algorithms for data science  Andrew McGregor  Lecture 4  0  
last time  Last Class: • 2-Level Hashing Analysis (linearity of expectation and Markov’s  inequality)  • 2-universal and pairwise independent hash functions • Chebyshev: Pr(|X − E[X]| ≥ t) ≤ Var[X]/t2  1  
last time  Last Class: • 2-Level Hashing Analysis (linearity of expectation and Markov’s  inequality)  • 2-universal and pairwise independent hash functions • Chebyshev: Pr(|X − E[X]| ≥ t) ≤ Var[X]/t2  This Time: • Random hashing for load balancing. Motivating:  • Stronger concentration inequalities: Chebyshev’s inequality,  exponential tail bounds, and their connections to the law of large numbers and central limit theorem.  • The union bound.  1  
randomized load balancing  Randomized Load Balancing:  • n requests randomly assigned to k servers.  2  
randomized load balancing  Randomized Load Balancing:  • n requests randomly assigned to k servers. • Expected load and variance for server i is  E[Ri ] = n/k  and Var[Ri ] = n(1 − 1/k)/k .  2  
randomized load balancing  Randomized Load Balancing:  • n requests randomly assigned to k servers. • Expected load and variance for server i is  E[Ri ] = n/k  and Var[Ri ] = n(1 − 1/k)/k .  • Suppose each server can handle at most E[Ri ] = n/k requests  2  
randomized load balancing  Randomized Load Balancing:  • n requests randomly assigned to k servers. • Expected load and variance for server i is  E[Ri ] = n/k  and Var[Ri ] = n(1 − 1/k)/k .  • Suppose each server can handle at most E[Ri ] = n/k requests • By Markov’s inequality, Pr[Ri ≥ 2E[Ri ]] ≤ 1/2.  2  
randomized load balancing  Randomized Load Balancing:  • n requests randomly assigned to k servers. • Expected load and variance for server i is  E[Ri ] = n/k  and Var[Ri ] = n(1 − 1/k)/k .  • Suppose each server can handle at most E[Ri ] = n/k requests • By Markov’s inequality, Pr[Ri ≥ 2E[Ri ]] ≤ 1/2. • By Chebyshev’s inequality, Pr[Ri ≥ 2E[Ri ]] ≤ Var[Ri ]  E[Ri ]2 < k n .  2  
maximum server load  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n 2n k capacity?  k . I.e., that some server is overloaded if we give each  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n/k.  3  
maximum server load  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n (cid:18) 2n k capacity?  k . I.e., that some server is overloaded if we give each  (cid:19)  Pr  (Ri ) ≥ 2n k  max  i  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n/k.  3  
maximum server load  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n (cid:18) 2n k capacity?  k . I.e., that some server is overloaded if we give each  (cid:18)(cid:20)  (cid:19)  (cid:21)  (cid:20)  (cid:21)  (cid:20)  R1 ≥ 2n k  ∪  R2 ≥ 2n k  ∪ . . . ∪  Rk ≥ 2n k  Pr  (Ri ) ≥ 2n k  max  i  = Pr  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n/k.  (cid:21)(cid:19)  3  
maximum server load  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n (cid:18) 2n k capacity?  k . I.e., that some server is overloaded if we give each  (cid:18)(cid:20)  (cid:19)  (cid:21)  (cid:20)  (cid:21)  (cid:20)  Pr  (Ri ) ≥ 2n k  max  i  = Pr  R1 ≥ 2n k  or  R2 ≥ 2n k  or . . . or  Rk ≥ 2n k  (cid:21)(cid:19)  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n/k.  3  
maximum server load  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n 2n k capacity?  k . I.e., that some server is overloaded if we give each  (cid:18)  (cid:19)  (cid:32) k(cid:91)  (cid:20)  i=1  (cid:21)(cid:33)  Pr  (Ri ) ≥ 2n k  max  i  = Pr  Ri ≥ 2n k  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n/k.  3  
maximum server load  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n 2n k capacity?  k . I.e., that some server is overloaded if we give each  (cid:18)  Pr  max  i  (cid:21)(cid:33)  Ri ≥ 2n k  (cid:20)  (cid:32) k(cid:91) (cid:19) (cid:3)(cid:17) (cid:2)Ri ≥ 2n  = Pr  i=1  k  (Ri ) ≥ 2n k  (cid:16)(cid:83)k  i=1  We want to show that Pr  is small.  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n/k.  3  
maximum server load  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n 2n k capacity?  k . I.e., that some server is overloaded if we give each  (cid:21)(cid:33)  Ri ≥ 2n k  (cid:18)  Pr  max  i  (cid:20)  (cid:32) k(cid:91) (cid:19) (cid:3)(cid:17) (cid:2)Ri ≥ 2n  = Pr  i=1  k  (Ri ) ≥ 2n k  (cid:16)(cid:83)k  i=1  We want to show that Pr  is small.  How do we do this? Note that R1, . . . , Rk are correlated in a somewhat complex way.  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n/k.  3  
the union bound  Union Bound: For any random events A1, A2, ..., Ak , Pr (A1 ∪ A2 ∪ . . . ∪ Ak ) ≤ Pr(A1) + Pr(A2) + . . . + Pr (Ak ).  4  
the union bound  Union Bound: For any random events A1, A2, ..., Ak , Pr (A1 ∪ A2 ∪ . . . ∪ Ak ) ≤ Pr(A1) + Pr(A2) + . . . + Pr (Ak ).  4  
the union bound  Union Bound: For any random events A1, A2, ..., Ak , Pr (A1 ∪ A2 ∪ . . . ∪ Ak ) ≤ Pr(A1) + Pr(A2) + . . . + Pr (Ak ).  When is the union bound tight?  4  
the union bound  Union Bound: For any random events A1, A2, ..., Ak , Pr (A1 ∪ A2 ∪ . . . ∪ Ak ) ≤ Pr(A1) + Pr(A2) + . . . + Pr (Ak ).  When is the union bound tight? When A1, ..., Ak are all disjoint.  4  
the union bound  Union Bound: For any random events A1, A2, ..., Ak , Pr (A1 ∪ A2 ∪ . . . ∪ Ak ) ≤ Pr(A1) + Pr(A2) + . . . + Pr (Ak ).  When is the union bound tight? When A1, ..., Ak are all disjoint.  4  
the union bound  Union Bound: For any random events A1, A2, ..., Ak , Pr (A1 ∪ A2 ∪ . . . ∪ Ak ) ≤ Pr(A1) + Pr(A2) + . . . + Pr (Ak ).  When is the union bound tight? When A1, ..., Ak are all disjoint.  On the ﬁrst problem set, you will prove the union bound, as a consequence of Markov’s inquality.  4  
applying the union bound  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n capacity?  k . I.e., that some server is overloaded if we give each 2n  k  (cid:21)(cid:33)  (cid:32) k(cid:91)  (cid:20)  i=1  Ri ≥ 2n k  (cid:18)  Pr  (Ri ) ≥ 2n k  max  i  (cid:19)  = Pr  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n k .  5  
applying the union bound  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n capacity?  k . I.e., that some server is overloaded if we give each 2n  k  (cid:18)  Pr  (Ri ) ≥ 2n k  max  i  (cid:19)  (cid:32) k(cid:91) (cid:20) (cid:18)(cid:20) ≤ k(cid:88)  = Pr  Pr  i=1  i=1  (cid:21)(cid:33) (cid:21)(cid:19)  Ri ≥ 2n k  Ri ≥ 2n k  (Union Bound)  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n k .  5  
applying the union bound  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n capacity?  k . I.e., that some server is overloaded if we give each 2n  k  (cid:18)  Pr  (Ri ) ≥ 2n k  max  i  (cid:21)(cid:33) (cid:21)(cid:19)  Ri ≥ 2n k  Ri ≥ 2n k  (cid:19)  = Pr  (cid:32) k(cid:91) (cid:20) (cid:18)(cid:20) ≤ k(cid:88) ≤ k(cid:88)  Pr  i=1  i=1  k n  i=1  (Union Bound)  (Bound from Chebyshev’s)  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n k .  5  
applying the union bound  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n capacity?  k . I.e., that some server is overloaded if we give each 2n  k  (cid:18)  Pr  (Ri ) ≥ 2n k  max  i  (cid:19)  = Pr  (cid:32) k(cid:91) (cid:20) (cid:18)(cid:20) ≤ k(cid:88) ≤ k(cid:88)  Pr  i=1  i=1  k n  =  i=1  (cid:21)(cid:33) (cid:21)(cid:19)  Ri ≥ 2n k  Ri ≥ 2n k  k 2 n  (Union Bound)  (Bound from Chebyshev’s)  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n k .  5  
applying the union bound  What is the probability that the maximum server load exceeds 2 · E[Ri ] = 2n capacity?  k . I.e., that some server is overloaded if we give each 2n  k  (cid:18)  Pr  (Ri ) ≥ 2n k  max  i  (cid:19)  = Pr  (cid:32) k(cid:91) (cid:20) (cid:18)(cid:20) ≤ k(cid:88) ≤ k(cid:88)  Pr  i=1  i=1  k n  =  i=1  (cid:21)(cid:33) (cid:21)(cid:19)  Ri ≥ 2n k  Ri ≥ 2n k  k 2 n  (Union Bound)  (Bound from Chebyshev’s)  As long as k (cid:28) √  n, the maximum server load will be small (compared to  the expected load) with good probability.  n: total number of requests, k: number of servers randomly assigned requests, Ri : number of requests assigned to server i. E[Ri ] = n  k . Var[Ri ] = n k .  5  
back to chebyshev’s inequality  Pr(|X − E[X]| ≥ t) ≤ Var[X]  t2  X: any random variable, t, s: any ﬁxed numbers.  6  
back to chebyshev’s inequality  Pr(|X − E[X]| ≥ t) ≤ Var[X]  t2  What is the probability that X falls s standard deviations from it’s mean?  X: any random variable, t, s: any ﬁxed numbers.  6  
back to chebyshev’s inequality  Pr(|X − E[X]| ≥ t) ≤ Var[X]  t2  What is the probability that X falls s standard deviations from it’s mean?  Pr(|X − E[X]| ≥ s ·(cid:112)Var[X]) ≤ Var[X]  s 2 · Var[X]  =  1 s 2 .  X: any random variable, t, s: any ﬁxed numbers.  6  
back to chebyshev’s inequality  Pr(|X − E[X]| ≥ t) ≤ Var[X]  t2  What is the probability that X falls s standard deviations from it’s mean?  Pr(|X − E[X]| ≥ s ·(cid:112)Var[X]) ≤ Var[X]  s 2 · Var[X]  =  1 s 2 .  Why is this so powerful?  X: any random variable, t, s: any ﬁxed numbers.  6  
law of large numbers  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  7  
law of large numbers  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  (cid:80)n  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  7  
law of large numbers  (cid:35)  (cid:34)  1 n  n(cid:88)  Var[S] = Var  Xi  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  (cid:80)n  i=1  7  
law of large numbers  (cid:35)  (cid:34)  1 n  n(cid:88)  i=1  n(cid:88)  1 n2  Var[S] = Var  Xi  =  Var [Xi ]  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  (cid:80)n  i=1  7  
law of large numbers  (cid:35)  (cid:34)  1 n  n(cid:88)  i=1  n(cid:88)  i=1  1 n2  Var[S] = Var  Xi  =  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  (cid:80)n  Var [Xi ] =  1  n2 · n · σ2  7  
law of large numbers  (cid:35)  (cid:34)  1 n  n(cid:88)  i=1  n(cid:88)  i=1  1 n2  Var[S] = Var  Xi  =  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  (cid:80)n  Var [Xi ] =  1  n2 · n · σ2 =  σ2 n  .  7  
law of large numbers  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  (cid:80)n  (cid:35)  (cid:34)  1 n  n(cid:88)  i=1  n(cid:88)  i=1  1 n2  Var[S] = Var  Xi  =  Var [Xi ] =  1  n2 · n · σ2 =  σ2 n  .  By Chebyshev’s Inequality: for any ﬁxed value  > 0,  Pr(|S − E[S]| ≥ ) ≤ Var[S]  2 =  σ2 n2 .  7  
law of large numbers  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  (cid:80)n  (cid:35)  (cid:34)  1 n  n(cid:88)  i=1  n(cid:88)  i=1  1 n2  Var[S] = Var  Xi  =  Var [Xi ] =  1  n2 · n · σ2 =  σ2 n  .  By Chebyshev’s Inequality: for any ﬁxed value  > 0,  Pr(|S − µ| ≥ ) ≤ Var[S]  2 =  σ2 n2 .  7  
law of large numbers  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  (cid:80)n  (cid:35)  (cid:34)  1 n  n(cid:88)  i=1  n(cid:88)  i=1  1 n2  Var[S] = Var  Xi  =  Var [Xi ] =  1  n2 · n · σ2 =  σ2 n  .  By Chebyshev’s Inequality: for any ﬁxed value  > 0,  Pr(|S − µ| ≥ ) ≤ Var[S]  2 =  σ2 n2 .  Law of Large Numbers: with enough samples n, the sample average will always concentrate to the mean.  7  
law of large numbers  Consider drawing independent identically distributed (i.i.d.) random variables X1, . . . , Xn with mean µ and variance σ2.  How well does the sample average S = 1 n true mean µ?  i=1 Xi approximate the  (cid:80)n  (cid:35)  (cid:34)  1 n  n(cid:88)  i=1  n(cid:88)  i=1  1 n2  Var[S] = Var  Xi  =  Var [Xi ] =  1  n2 · n · σ2 =  σ2 n  .  By Chebyshev’s Inequality: for any ﬁxed value  > 0,  Pr(|S − µ| ≥ ) ≤ Var[S]  2 =  σ2 n2 .  Law of Large Numbers: with enough samples n, the sample average will always concentrate to the mean. • Cannot show from vanilla Markov’s inequality.  7  
server load and law of large numbers  √ The number of servers must be small compared to the number of requests (k = O( comparison to the expected load with good probability.  n)) for the maximum load to be bounded in  n: total number of requests, k: number of servers randomly assigned requests.  8  
server load and law of large numbers  √ The number of servers must be small compared to the number of requests (k = O( comparison to the expected load with good probability. • There are many requests routed to a relatively small number of  n)) for the maximum load to be bounded in  servers so the load seen on each server is close to what is expected via law of large numbers.  n: total number of requests, k: number of servers randomly assigned requests.  8  
Questions on union bound, Chebyshev’s inequality, random  hashing?  9  
flipping coins  We ﬂip n = 100 independent coins, each are heads with probability 1/2 and tails with probability 1/2. Let H be the number of heads.  10  
flipping coins  We ﬂip n = 100 independent coins, each are heads with probability 1/2 and tails with probability 1/2. Let H be the number of heads.  E[H] =  n 2  = 50 and Var[H] =  10  
flipping coins  We ﬂip n = 100 independent coins, each are heads with probability 1/2 and tails with probability 1/2. Let H be the number of heads.  E[H] =  n 2  = 50 and Var[H] =  n 4  = 25  10  
flipping coins  We ﬂip n = 100 independent coins, each are heads with probability 1/2 and tails with probability 1/2. Let H be the number of heads.  E[H] =  n 2  = 50 and Var[H] =  n 4  = 25  Markov’s:  Pr(H ≥ 60) ≤ .833 Pr(H ≥ 70) ≤ .714 Pr(H ≥ 80) ≤ .625  10  
flipping coins  We ﬂip n = 100 independent coins, each are heads with probability 1/2 and tails with probability 1/2. Let H be the number of heads.  E[H] =  n 2  = 50 and Var[H] =  = 25 → s.d. = 5  n 4  Markov’s:  Pr(H ≥ 60) ≤ .833 Pr(H ≥ 70) ≤ .714 Pr(H ≥ 80) ≤ .625  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .0278  10  
flipping coins  We ﬂip n = 100 independent coins, each are heads with probability 1/2 and tails with probability 1/2. Let H be the number of heads.  E[H] =  n 2  = 50 and Var[H] =  = 25 → s.d. = 5  n 4  Markov’s:  Pr(H ≥ 60) ≤ .833 Pr(H ≥ 70) ≤ .714 Pr(H ≥ 80) ≤ .625  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .0278  In Reality:  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 Pr(H ≥ 80) < 10−9  H has a simple Binomial distribution, so can compute these probabilities exactly.  10  
tighter concentration bounds  To be fair.... Markov and Chebyshev’s inequalities apply much more generally than to Binomial random variables like coin ﬂips.  11  
tighter concentration bounds  To be fair.... Markov and Chebyshev’s inequalities apply much more generally than to Binomial random variables like coin ﬂips.  Can we obtain tighter concentration bounds that still apply to very general distributions?  11  
tighter concentration bounds  To be fair.... Markov and Chebyshev’s inequalities apply much more generally than to Binomial random variables like coin ﬂips.  Can we obtain tighter concentration bounds that still apply to very general distributions? • Markov’s: Pr(X ≥ t) ≤ E[X]  . First Moment.  t  11  
tighter concentration bounds  To be fair.... Markov and Chebyshev’s inequalities apply much more generally than to Binomial random variables like coin ﬂips.  Can we obtain tighter concentration bounds that still apply to very general distributions? • Markov’s: Pr(X ≥ t) ≤ E[X] • Chebyshev’s: Pr(|X − E[X]| ≥ t) = Pr(|X − E[X]|2 ≥ t 2) ≤ Var[X]  . First Moment.  t  Second Moment.  t2  .  11  
tighter concentration bounds  To be fair.... Markov and Chebyshev’s inequalities apply much more generally than to Binomial random variables like coin ﬂips.  Can we obtain tighter concentration bounds that still apply to very general distributions? • Markov’s: Pr(X ≥ t) ≤ E[X] • Chebyshev’s: Pr(|X − E[X]| ≥ t) = Pr(|X − E[X]|2 ≥ t 2) ≤ Var[X]  . First Moment.  t  .  t2  Second Moment.  • What if we just apply Markov’s inequality to even higher moments?  11  
a fourth moment bound  Consider any random variable X:  Pr(|X − E[X]| ≥ t) = Pr  (cid:16)  (X − E[X])4 ≥ t4(cid:17)  12  
a fourth moment bound  Consider any random variable X:  Pr(|X − E[X]| ≥ t) = Pr  (cid:16)  (X − E[X])4 ≥ t4(cid:17) ≤  E(cid:104) (X − E[X])4(cid:105)  t4  .  12  
a fourth moment bound  Consider any random variable X:  Pr(|X − E[X]| ≥ t) = Pr  (cid:16)  (X − E[X])4 ≥ t4(cid:17) ≤  E(cid:104) (X − E[X])4(cid:105)  t4  .  12  
a fourth moment bound  Consider any random variable X:  Pr(|X − E[X]| ≥ t) = Pr  (cid:16)  (X − E[X])4 ≥ t4(cid:17) ≤  E(cid:104) (X − E[X])4(cid:105)  t4  .  Application to Coin Flips: Recall: n = 100 independent fair coins, H is the number of heads. • Bound the fourth moment:  12  
a fourth moment bound  Consider any random variable X:  Pr(|X − E[X]| ≥ t) = Pr  (cid:16)  (X − E[X])4 ≥ t4(cid:17) ≤  E(cid:104) (X − E[X])4(cid:105)  t4  .  Application to Coin Flips: Recall: n = 100 independent fair coins, H is the number of heads. • Bound the fourth moment:  E(cid:104)  (H − E[H])4(cid:105)  = E  (cid:32) 100(cid:88)  (cid:33)4  Hi − 50  i=1  where Hi = 1 if coin ﬂip i is heads and 0 otherwise.  12  
a fourth moment bound  Consider any random variable X:  Pr(|X − E[X]| ≥ t) = Pr  (cid:16)  (X − E[X])4 ≥ t4(cid:17) ≤  E(cid:104) (X − E[X])4(cid:105)  t4  .  Application to Coin Flips: Recall: n = 100 independent fair coins, H is the number of heads. • Bound the fourth moment:  E(cid:104)  (H − E[H])4(cid:105)  = E  (cid:32) 100(cid:88)  (cid:33)4 =  (cid:88)  Hi − 50  cijk(cid:96)E[Hi Hj Hk H(cid:96)]  i=1  i,j,k,(cid:96)  where Hi = 1 if coin ﬂip i is heads and 0 otherwise. Then apply some messy calculations...  12  
a fourth moment bound  Consider any random variable X:  Pr(|X − E[X]| ≥ t) = Pr  (cid:16)  (X − E[X])4 ≥ t4(cid:17) ≤  E(cid:104) (X − E[X])4(cid:105)  t4  .  Application to Coin Flips: Recall: n = 100 independent fair coins, H is the number of heads. • Bound the fourth moment:  E(cid:104)  (H − E[H])4(cid:105)  = E  (cid:32) 100(cid:88)  (cid:33)4 =  (cid:88)  Hi − 50  cijk(cid:96)E[Hi Hj Hk H(cid:96)] = 1862.5  i=1  i,j,k,(cid:96)  where Hi = 1 if coin ﬂip i is heads and 0 otherwise. Then apply some messy calculations...  12  
a fourth moment bound  Consider any random variable X:  Pr(|X − E[X]| ≥ t) = Pr  (cid:16)  (X − E[X])4 ≥ t4(cid:17) ≤  E(cid:104) (X − E[X])4(cid:105)  t4  .  Application to Coin Flips: Recall: n = 100 independent fair coins, H is the number of heads. • Bound the fourth moment:  E(cid:104)  (H − E[H])4(cid:105)  = E  (cid:32) 100(cid:88)  (cid:33)4 =  (cid:88)  Hi − 50  cijk(cid:96)E[Hi Hj Hk H(cid:96)] = 1862.5  i=1  i,j,k,(cid:96)  where Hi = 1 if coin ﬂip i is heads and 0 otherwise. Then apply some messy calculations...  • Apply Fourth Moment Bound: Pr (|H − E[H]| ≥ t) ≤ 1862.5  t4  .  12  
tighter bounds  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  In Reality:  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 −9  Pr(H ≥ 80) < 10  13  
tighter bounds  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  4th Moment:  In Reality:  Pr(H ≥ 60) ≤ .186 Pr(H ≥ 70) ≤ .0116 Pr(H ≥ 80) ≤ .0023  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 −9  Pr(H ≥ 80) < 10  13  
tighter bounds  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  4th Moment:  In Reality:  Pr(H ≥ 60) ≤ .186 Pr(H ≥ 70) ≤ .0116 Pr(H ≥ 80) ≤ .0023  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 −9  Pr(H ≥ 80) < 10  Can we just keep applying Markov’s inequality to higher and higher moments and getting tighter bounds?  13  
tighter bounds  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  4th Moment:  In Reality:  Pr(H ≥ 60) ≤ .186 Pr(H ≥ 70) ≤ .0116 Pr(H ≥ 80) ≤ .0023  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 −9  Pr(H ≥ 80) < 10  Can we just keep applying Markov’s inequality to higher and higher moments and getting tighter bounds? • Yes! To a point.  13  
tighter bounds  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  4th Moment:  In Reality:  Pr(H ≥ 60) ≤ .186 Pr(H ≥ 70) ≤ .0116 Pr(H ≥ 80) ≤ .0023  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 −9  Pr(H ≥ 80) < 10  Can we just keep applying Markov’s inequality to higher and higher moments and getting tighter bounds? • Yes! To a point. • In fact – don’t need to just apply Markov’s to |X − E[X]|k for some k. Can apply to any monotonic function f (|X − E[X]|).  13  
tighter bounds  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  4th Moment:  In Reality:  Pr(H ≥ 60) ≤ .186 Pr(H ≥ 70) ≤ .0116 Pr(H ≥ 80) ≤ .0023  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 −9  Pr(H ≥ 80) < 10  Can we just keep applying Markov’s inequality to higher and higher moments and getting tighter bounds? • Yes! To a point. • In fact – don’t need to just apply Markov’s to |X − E[X]|k for some k. Can apply to any monotonic function f (|X − E[X]|).  • Why monotonic?  13  
tighter bounds  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  4th Moment:  In Reality:  Pr(H ≥ 60) ≤ .186 Pr(H ≥ 70) ≤ .0116 Pr(H ≥ 80) ≤ .0023  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 −9  Pr(H ≥ 80) < 10  Can we just keep applying Markov’s inequality to higher and higher moments and getting tighter bounds? • Yes! To a point. • In fact – don’t need to just apply Markov’s to |X − E[X]|k for some k. Can apply to any monotonic function f (|X − E[X]|).  • Why monotonic?  Pr (|X − E[X]| > t) = Pr (f (|X − E[X]|) > f (t)).  13  
exponential concentration bounds  Moment Generating Function: Consider for any t > 0:  Mt(X) = et·(X−E[X])  14  
exponential concentration bounds  Moment Generating Function: Consider for any t > 0:  Mt(X) = et·(X−E[X]) =  ∞(cid:88)  tk (X − E[X])k  k=0  k!  14  
exponential concentration bounds  Mt(X) = et·(X−E[X]) =  ∞(cid:88)  Moment Generating Function: Consider for any t > 0:  tk (X − E[X])k  k=0  k!  • Mt(X) is monotonic for any t > 0.  14  
exponential concentration bounds  Mt(X) = et·(X−E[X]) =  ∞(cid:88)  Moment Generating Function: Consider for any t > 0:  tk (X − E[X])k  k=0  k!  • Mt(X) is monotonic for any t > 0. • Weighted sum of all moments, with t controlling how slowly the  weights fall oﬀ (larger t = slower falloﬀ).  14  
exponential concentration bounds  Mt(X) = et·(X−E[X]) =  ∞(cid:88)  Moment Generating Function: Consider for any t > 0:  tk (X − E[X])k  k=0  k!  • Mt(X) is monotonic for any t > 0. • Weighted sum of all moments, with t controlling how slowly the  weights fall oﬀ (larger t = slower falloﬀ).  • Choosing t appropriately lets one prove a number of very  powerful exponential concentration bounds (exponential tail bounds).  14  
exponential concentration bounds  Mt(X) = et·(X−E[X]) =  ∞(cid:88)  Moment Generating Function: Consider for any t > 0:  tk (X − E[X])k  k=0  k!  • Mt(X) is monotonic for any t > 0. • Weighted sum of all moments, with t controlling how slowly the  weights fall oﬀ (larger t = slower falloﬀ).  • Choosing t appropriately lets one prove a number of very  powerful exponential concentration bounds (exponential tail bounds).  • Chernoﬀ bound, Bernstein inequalities, Hoeﬀding’s inequality,  Azuma’s inequality, Berry-Esseen theorem, etc.  14  
exponential concentration bounds  Mt(X) = et·(X−E[X]) =  ∞(cid:88)  Moment Generating Function: Consider for any t > 0:  tk (X − E[X])k  k=0  k!  • Mt(X) is monotonic for any t > 0. • Weighted sum of all moments, with t controlling how slowly the  weights fall oﬀ (larger t = slower falloﬀ).  • Choosing t appropriately lets one prove a number of very  powerful exponential concentration bounds (exponential tail bounds).  • Chernoﬀ bound, Bernstein inequalities, Hoeﬀding’s inequality,  Azuma’s inequality, Berry-Esseen theorem, etc.  • We will explore the basic proof approach in homework.  14  
bernstein inequality  Bernstein Inequality: Consider independent random variables X1, . . . , Xn all falling in [−M, M]. i=1 Xi ] and  i=1 Var[Xi ]. For any t ≥ 0:  i=1 Xi ] =(cid:80)n σ2 = Var[(cid:80)n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ t (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Xi − µ  Pr  i=1  (cid:33)  Let µ = E[(cid:80)n (cid:33) (cid:32)  −  t 2 2σ2 + 4  3 Mt  ≤ 2 exp  .  15  
bernstein inequality  Bernstein Inequality: Consider independent random variables X1, . . . , Xn all falling in [−M, M]. i=1 Xi ] and  i=1 Var[Xi ]. For any t ≥ 0:  i=1 Xi ] =(cid:80)n σ2 = Var[(cid:80)n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ t (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Xi − µ  Pr  i=1  (cid:33)  Assume that M = 1 and plug in t = s · σ for s ≤ σ.  Let µ = E[(cid:80)n (cid:33) (cid:32)  −  t 2 2σ2 + 4  3 Mt  ≤ 2 exp  .  15  
bernstein inequality  Bernstein Inequality: Consider independent random variables i=1 Xi ] and σ2 =  X1, . . . , Xn all falling in [-1,1]. Let µ = E[(cid:80)n Var[(cid:80)n i=1 Var[Xi ]. For any s ≥ 0: (cid:18)  (cid:33)  (cid:19)  i=1 Xi ] =(cid:80)n (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Pr  i=1  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ  Xi − µ  ≤ 2 exp  − s 2 4  Assume that M = 1 and plug in t = s · σ for s ≤ σ.  .  15  
bernstein inequality  Bernstein Inequality: Consider independent random variables i=1 Xi ] and σ2 =  i=1 Xi ] =(cid:80)n (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Pr  i=1  X1, . . . , Xn all falling in [-1,1]. Let µ = E[(cid:80)n Var[(cid:80)n i=1 Var[Xi ]. For any s ≥ 0: (cid:18)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ Compare to Chebyshev’s: Pr(cid:0)(cid:12)(cid:12)(cid:80)n i=1 Xi − µ(cid:12)(cid:12) ≥ sσ(cid:1) ≤ 1  Assume that M = 1 and plug in t = s · σ for s ≤ σ.  ≤ 2 exp  Xi − µ  (cid:33)  (cid:19)  .  − s 2 4  s 2 .  15  
bernstein inequality  Bernstein Inequality: Consider independent random variables i=1 Xi ] and σ2 =  i=1 Xi ] =(cid:80)n (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  Pr  i=1  X1, . . . , Xn all falling in [-1,1]. Let µ = E[(cid:80)n Var[(cid:80)n i=1 Var[Xi ]. For any s ≥ 0: (cid:18)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ sσ Compare to Chebyshev’s: Pr(cid:0)(cid:12)(cid:12)(cid:80)n i=1 Xi − µ(cid:12)(cid:12) ≥ sσ(cid:1) ≤ 1  Assume that M = 1 and plug in t = s · σ for s ≤ σ.  ≤ 2 exp  Xi − µ  (cid:33)  (cid:19)  .  − s 2 4  s 2 .  • An exponentially stronger dependence on s!  15  
comparision to chebyshev’s  Consider again bounding the number of heads H in n = 100 independent coin ﬂips.  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  Bernstein:  In Reality:  Pr(H ≥ 60) ≤ .15 Pr(H ≥ 70) ≤ .00086 Pr(H ≥ 80) ≤ 3−7  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 Pr(H ≥ 80) < 10−9  H: total number heads in 100 random coin ﬂips. E[H] = 50.  16  
comparision to chebyshev’s  Consider again bounding the number of heads H in n = 100 independent coin ﬂips.  Chebyshev’s: Pr(H ≥ 60) ≤ .25 Pr(H ≥ 70) ≤ .0625 Pr(H ≥ 80) ≤ .04  Bernstein:  In Reality:  Pr(H ≥ 60) ≤ .15 Pr(H ≥ 70) ≤ .00086 Pr(H ≥ 80) ≤ 3−7  Pr(H ≥ 60) = 0.0284 Pr(H ≥ 70) = .000039 Pr(H ≥ 80) < 10−9  Getting much closer to the true probability.  H: total number heads in 100 random coin ﬂips. E[H] = 50.  16  
the chernoff bound  A useful variation of the Bernstein inequality for binary (indicator) random variables is:  Chernoﬀ Bound (simpliﬁed version): Consider independent random variables X1, . . . , Xn taking values in {0, 1}. Let µ =  .  17  E[(cid:80)n  i=1 Xi ]. For any δ ≥ 0  (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  i=1  Pr  Xi − µ  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ δµ (cid:33)  (cid:18)  (cid:19)  ≤ 2 exp  − δ2µ 2 + δ  
the chernoff bound  A useful variation of the Bernstein inequality for binary (indicator) random variables is:  Chernoﬀ Bound (simpliﬁed version): Consider independent random variables X1, . . . , Xn taking values in {0, 1}. Let µ =  E[(cid:80)n  i=1 Xi ]. For any δ ≥ 0  (cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)  i=1  Pr  Xi − µ  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ δµ (cid:33)  (cid:18)  (cid:19)  .  ≤ 2 exp  − δ2µ 2 + δ  As δ gets larger and larger, the bound falls of exponentially fast.  17  
