compsci 514: algorithms for data science  Andrew McGregor  Lecture 16  0  
summary  Last Class: Low-Rank Approximation • When data lies in a k-dimensional subspace V, we can perfectly embed into k dimensions using an orthonormal span V ∈ Rd×k . • When data lies close to V, the optimal embedding in that space  is given by projecting onto that space.  XVVT = arg min  B with rows in V  (cid:107)X − B(cid:107)2 F .  1  
summary  Last Class: Low-Rank Approximation • When data lies in a k-dimensional subspace V, we can perfectly embed into k dimensions using an orthonormal span V ∈ Rd×k . • When data lies close to V, the optimal embedding in that space  is given by projecting onto that space.  XVVT = arg min  B with rows in V  (cid:107)X − B(cid:107)2 F .  This Class: • The best subspace V is the subspace spanned by the top k  eigenvectors of XT X. How good is this approximation?  1  
recap: basic set up  Reminder of Set Up: Assume that (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd . Let X ∈ Rn×d be the data matrix.  Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. • VVT ∈ Rd×d is the projection matrix onto V. • X ≈ X(VVT ). Gives the closest approximation to X with rows in V. (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  2  
recap: basic set up  Reminder of Set Up: Assume that (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd . Let X ∈ Rn×d be the data matrix.  Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. • VVT ∈ Rd×d is the projection matrix onto V. • X ≈ X(VVT ). Gives the closest approximation to X with rows in V. (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  2  
recap: best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT . XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  3  
recap: best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT . XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  arg min  orthonormal V∈Rd×k  (cid:107)X − XVVT(cid:107)2  F =  arg max  orthonormal V∈Rd×k  (cid:107)XV(cid:107)2 F .  3  
recap: best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT . XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  arg min  (cid:107)X − XVVT(cid:107)2  F =  orthonormal V∈Rd×k  orthonormal V∈Rd×k Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  arg max  (cid:107)XV(cid:107)2 F .  3  
recap: best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT . XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  arg min  (cid:107)X − XVVT(cid:107)2  F =  orthonormal V∈Rd×k  orthonormal V∈Rd×k Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  arg max  (cid:107)XV(cid:107)2 F .  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T XT X(cid:126)v .  3  
recap: best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT . XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  arg min  (cid:107)X − XVVT(cid:107)2  F =  orthonormal V∈Rd×k  orthonormal V∈Rd×k Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  arg max  (cid:107)XV(cid:107)2 F .  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T XT X(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T XT X(cid:126)v .  3  
recap: best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT . XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  arg min  (cid:107)X − XVVT(cid:107)2  F =  orthonormal V∈Rd×k  orthonormal V∈Rd×k Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  arg max  (cid:107)XV(cid:107)2 F .  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T XT X(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T XT X(cid:126)v .  . . .  (cid:126)vk =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)vj(cid:105)=0 ∀j<k  (cid:126)v T XT X(cid:126)v .  3  
recap: best fit subspace  If (cid:126)x1, . . . , (cid:126)xn are close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as XVVT . XV gives optimal embedding of X in V. How do we ﬁnd V (equivalently V)?  arg min  (cid:107)X − XVVT(cid:107)2  F =  orthonormal V∈Rd×k  orthonormal V∈Rd×k Surprisingly, can ﬁnd the columns of V, (cid:126)v1, . . . , (cid:126)vk greedily.  arg max  (cid:107)XV(cid:107)2 F .  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T XT X(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T XT X(cid:126)v .  . . .  (cid:126)vk =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)vj(cid:105)=0 ∀j<k  (cid:126)v T XT X(cid:126)v .  These are exactly the top k eigenvectors of XT X.  3  
review: eigenvectors and eigendecomposition  Eigenvector: (cid:126)x ∈ Rd is an eigenvector of a matrix A ∈ Rd×d if A(cid:126)x = λ(cid:126)x for some scalar λ (the eigenvalue corresponding to (cid:126)x).  4  
review: eigenvectors and eigendecomposition  Eigenvector: (cid:126)x ∈ Rd is an eigenvector of a matrix A ∈ Rd×d if A(cid:126)x = λ(cid:126)x for some scalar λ (the eigenvalue corresponding to (cid:126)x). • That is, A just ‘stretches’ x.  4  
review: eigenvectors and eigendecomposition  Eigenvector: (cid:126)x ∈ Rd is an eigenvector of a matrix A ∈ Rd×d if A(cid:126)x = λ(cid:126)x for some scalar λ (the eigenvalue corresponding to (cid:126)x). • That is, A just ‘stretches’ x. • If A is symmetric, can ﬁnd d orthonormal eigenvectors  (cid:126)v1, . . . , (cid:126)vd . Let V ∈ Rd×d have these vectors as columns and Λ be the diagonal matrix with the corresponding eigenvalues on the diagonal.  4  
review: eigenvectors and eigendecomposition  Eigenvector: (cid:126)x ∈ Rd is an eigenvector of a matrix A ∈ Rd×d if A(cid:126)x = λ(cid:126)x for some scalar λ (the eigenvalue corresponding to (cid:126)x). • That is, A just ‘stretches’ x. • If A is symmetric, can ﬁnd d orthonormal eigenvectors  (cid:126)v1, . . . , (cid:126)vd . Let V ∈ Rd×d have these vectors as columns and Λ be the diagonal matrix with the corresponding eigenvalues on the diagonal.   |  AV =    | A(cid:126)v1 A(cid:126)v2 | |  | | ··· A(cid:126)vd | |  4  
review: eigenvectors and eigendecomposition  Eigenvector: (cid:126)x ∈ Rd is an eigenvector of a matrix A ∈ Rd×d if A(cid:126)x = λ(cid:126)x for some scalar λ (the eigenvalue corresponding to (cid:126)x). • That is, A just ‘stretches’ x. • If A is symmetric, can ﬁnd d orthonormal eigenvectors  (cid:126)v1, . . . , (cid:126)vd . Let V ∈ Rd×d have these vectors as columns and Λ be the diagonal matrix with the corresponding eigenvalues on the diagonal.   |  AV =  | A(cid:126)v1 A(cid:126)v2 | |  | | ··· A(cid:126)vd | |   =   |  | λ1(cid:126)v1 λ2(cid:126)v2 | |  | | ··· λ(cid:126)vd | |    4  
review: eigenvectors and eigendecomposition  Eigenvector: (cid:126)x ∈ Rd is an eigenvector of a matrix A ∈ Rd×d if A(cid:126)x = λ(cid:126)x for some scalar λ (the eigenvalue corresponding to (cid:126)x). • That is, A just ‘stretches’ x. • If A is symmetric, can ﬁnd d orthonormal eigenvectors  (cid:126)v1, . . . , (cid:126)vd . Let V ∈ Rd×d have these vectors as columns and Λ be the diagonal matrix with the corresponding eigenvalues on the diagonal.   |  AV =  | A(cid:126)v1 A(cid:126)v2 | |  | | ··· A(cid:126)vd | |   =   |  | λ1(cid:126)v1 λ2(cid:126)v2 | |  | | ··· λ(cid:126)vd | |   = VΛ  4  
review: eigenvectors and eigendecomposition  Eigenvector: (cid:126)x ∈ Rd is an eigenvector of a matrix A ∈ Rd×d if A(cid:126)x = λ(cid:126)x for some scalar λ (the eigenvalue corresponding to (cid:126)x). • That is, A just ‘stretches’ x. • If A is symmetric, can ﬁnd d orthonormal eigenvectors  (cid:126)v1, . . . , (cid:126)vd . Let V ∈ Rd×d have these vectors as columns and Λ be the diagonal matrix with the corresponding eigenvalues on the diagonal.   |  AV =  | A(cid:126)v1 A(cid:126)v2 | |  | | ··· A(cid:126)vd | |   =   |  | λ1(cid:126)v1 λ2(cid:126)v2 | |  | | ··· λ(cid:126)vd | |   = VΛ  Yields eigendecomposition: AVVT = A = VΛVT .  4  
review: eigenvectors and eigendecomposition  Typically order the eigenvectors in decreasing order:  λ1 ≥ λ2 ≥ . . . ≥ λd  5  
courant-fischer principal  Courant-Fischer Principal: For symmetric A, the eigenvectors are given via the greedy optimization:  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T A(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T A(cid:126)v .  . . .  (cid:126)vd =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)vj(cid:105)=0 ∀j<d  (cid:126)v T A(cid:126)v .  6  
courant-fischer principal  Courant-Fischer Principal: For symmetric A, the eigenvectors are given via the greedy optimization:  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T A(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T A(cid:126)v .  . . .  (cid:126)vd =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)vj(cid:105)=0 ∀j<d  (cid:126)v T A(cid:126)v .  • (cid:126)v T  j A(cid:126)vj = λj · (cid:126)v T  j (cid:126)vj = λj , the j th largest eigenvalue.  6  
courant-fischer principal  Courant-Fischer Principal: For symmetric A, the eigenvectors are given via the greedy optimization:  (cid:126)v1 = arg max  (cid:126)v with (cid:107)v(cid:107)2=1  (cid:126)v T A(cid:126)v .  (cid:126)v2 =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)v1(cid:105)=0  (cid:126)v T A(cid:126)v .  . . .  (cid:126)vd =  arg max  (cid:126)v with (cid:107)v(cid:107)2=1, (cid:104)(cid:126)v ,(cid:126)vj(cid:105)=0 ∀j<d  (cid:126)v T A(cid:126)v .  j A(cid:126)vj = λj · (cid:126)v T  • (cid:126)v T • The ﬁrst k eigenvectors of XT X (corresponding to the largest k  j (cid:126)vj = λj , the j th largest eigenvalue.  eigenvalues) are exactly the directions of greatest variance in X that we use for low-rank approximation.  6  
low-rank approx via eigendecomposition  7  
low-rank approx via eigendecomposition  Upshot: Letting Vk have columns (cid:126)v1, . . . , (cid:126)vk corresponding to the top k eigenvectors of the covariance matrix XT X, Vk is the orthogonal basis minimizing  (cid:107)X − XVk VT  k (cid:107)2 F ,  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
low-rank approx via eigendecomposition  Upshot: Letting Vk have columns (cid:126)v1, . . . , (cid:126)vk corresponding to the top k eigenvectors of the covariance matrix XT X, Vk is the orthogonal basis minimizing  (cid:107)X − XVk VT  k (cid:107)2 F ,  This is principal component analysis (PCA).  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
low-rank approx via eigendecomposition  Upshot: Letting Vk have columns (cid:126)v1, . . . , (cid:126)vk corresponding to the top k eigenvectors of the covariance matrix XT X, Vk is the orthogonal basis minimizing  (cid:107)X − XVk VT  k (cid:107)2 F ,  This is principal component analysis (PCA). How accurate is this low-rank approximation?  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
low-rank approx via eigendecomposition  Upshot: Letting Vk have columns (cid:126)v1, . . . , (cid:126)vk corresponding to the top k eigenvectors of the covariance matrix XT X, Vk is the orthogonal basis minimizing  (cid:107)X − XVk VT  k (cid:107)2 F ,  This is principal component analysis (PCA). How accurate is this low-rank approximation? Can understand via eigenvalues of XT X.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
low-rank approx via eigendecomposition  Upshot: Letting Vk have columns (cid:126)v1, . . . , (cid:126)vk corresponding to the top k eigenvectors of the covariance matrix XT X, Vk is the orthogonal basis minimizing  (cid:107)X − XVk VT  k (cid:107)2 F ,  This is principal component analysis (PCA). How accurate is this low-rank approximation? Can understand via eigenvalues of XT X.  • In Homework 3 we show matrix form of Pythagorus Theorem:  (cid:107)X(cid:107)2  F = (cid:107)X − XVk VT  k (cid:107)2  F + (cid:107)XVk VT  k (cid:107)2  F  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
spectrum analysis  Let (cid:126)v1, . . . , (cid:126)vk be the top k eigenvectors of XT X (the top k principal components). Approximation error is: (cid:107)X − XVk VT  k (cid:107)2  F  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
spectrum analysis  Let (cid:126)v1, . . . , (cid:126)vk be the top k eigenvectors of XT X (the top k principal components). Approximation error is: (cid:107)X − XVk VT  F − (cid:107)XVk VT  k (cid:107)2  k (cid:107)2  F = (cid:107)X(cid:107)2  F  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
spectrum analysis  Let (cid:126)v1, . . . , (cid:126)vk be the top k eigenvectors of XT X (the top k principal components). Approximation error is: (cid:107)X − XVk VT  F − (cid:107)XVk(cid:107)2  F  k (cid:107)2  F = (cid:107)X(cid:107)2  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
spectrum analysis  Let (cid:126)v1, . . . , (cid:126)vk be the top k eigenvectors of XT X (the top k principal components). Approximation error is: (cid:107)X − XVk VT  F − (cid:107)XVk(cid:107)2  F  k (cid:107)2  F = (cid:107)X(cid:107)2  • For any matrix A, (cid:107)A(cid:107)2  F =(cid:80)d  i=1 (cid:107)(cid:126)ai(cid:107)2  2 = tr(AT A) = sum of  diagonal entries = sum eigenvalues.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
spectrum analysis  Let (cid:126)v1, . . . , (cid:126)vk be the top k eigenvectors of XT X (the top k principal components). Approximation error is: (cid:107)X − XVk VT k XT XVk )  F = tr(XT X) − tr(VT  k (cid:107)2  • For any matrix A, (cid:107)A(cid:107)2  F =(cid:80)d  i=1 (cid:107)(cid:126)ai(cid:107)2  2 = tr(AT A) = sum of  diagonal entries = sum eigenvalues.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
spectrum analysis  Let (cid:126)v1, . . . , (cid:126)vk be the top k eigenvectors of XT X (the top k principal components). Approximation error is: (cid:107)X − XVk VT k XT XVk )  F = tr(XT X) − tr(VT  k (cid:107)2  d(cid:88)  λi (XT X) − k(cid:88)  i=1  i=1  =  (cid:126)v T i XT X(cid:126)vi  • For any matrix A, (cid:107)A(cid:107)2  i=1 (cid:107)(cid:126)ai(cid:107)2  2 = tr(AT A) = sum of  F =(cid:80)d  diagonal entries = sum eigenvalues.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
spectrum analysis  Let (cid:126)v1, . . . , (cid:126)vk be the top k eigenvectors of XT X (the top k principal components). Approximation error is: (cid:107)X − XVk VT k XT XVk )  F = tr(XT X) − tr(VT  k (cid:107)2  d(cid:88) d(cid:88)  i=1  i=1  =  =  i=1  λi (XT X) − k(cid:88) λi (XT X) − k(cid:88) F =(cid:80)d  i=1  i=1 (cid:107)(cid:126)ai(cid:107)2  (cid:126)v T i XT X(cid:126)vi  λi (XT X)  • For any matrix A, (cid:107)A(cid:107)2  2 = tr(AT A) = sum of  diagonal entries = sum eigenvalues.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
spectrum analysis  Let (cid:126)v1, . . . , (cid:126)vk be the top k eigenvectors of XT X (the top k principal components). Approximation error is: (cid:107)X − XVk VT k XT XVk )  F = tr(XT X) − tr(VT  k (cid:107)2  d(cid:88) d(cid:88)  i=1  i=1  =  =  i=1  λi (XT X) − k(cid:88) λi (XT X) − k(cid:88) F =(cid:80)d  i=1  i=1 (cid:107)(cid:126)ai(cid:107)2  (cid:126)v T i XT X(cid:126)vi  λi (XT X) =  d(cid:88)  i=k+1  λi (XT X)  2 = tr(AT A) = sum of  • For any matrix A, (cid:107)A(cid:107)2  diagonal entries = sum eigenvalues.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
spectrum analysis  Claim: The error in approximating X with the best rank k approximation (projecting onto the top k eigenvectors of XT X) is:  d(cid:88)  λi (XT X)  (cid:107)X − XVk VT  k (cid:107)2  F =  i=k+1  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
spectrum analysis  Claim: The error in approximating X with the best rank k approximation (projecting onto the top k eigenvectors of XT X) is:  d(cid:88)  λi (XT X)  (cid:107)X − XVk VT  k (cid:107)2  F =  i=k+1  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
spectrum analysis  Claim: The error in approximating X with the best rank k approximation (projecting onto the top k eigenvectors of XT X) is:  d(cid:88)  λi (XT X)  (cid:107)X − XVk VT  k (cid:107)2  F =  i=k+1  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
spectrum analysis  Claim: The error in approximating X with the best rank k approximation (projecting onto the top k eigenvectors of XT X) is:  d(cid:88)  λi (XT X)  (cid:107)X − XVk VT  k (cid:107)2  F =  i=k+1  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
spectrum analysis  Claim: The error in approximating X with the best rank k approximation (projecting onto the top k eigenvectors of XT X) is:  d(cid:88)  λi (XT X)  (cid:107)X − XVk VT  k (cid:107)2  F =  i=k+1  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
spectrum analysis  Plotting the spectrum of the covariance matrix XT X (its eigenvalues) shows how compressible X is using low-rank approximation (i.e., how close (cid:126)x1, . . . , (cid:126)xn are to a low-dimensional subspace).  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  11  
spectrum analysis  Plotting the spectrum of the covariance matrix XT X (its eigenvalues) shows how compressible X is using low-rank approximation (i.e., how close (cid:126)x1, . . . , (cid:126)xn are to a low-dimensional subspace).  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  11  
spectrum analysis  Plotting the spectrum of the covariance matrix XT X (its eigenvalues) shows how compressible X is using low-rank approximation (i.e., how close (cid:126)x1, . . . , (cid:126)xn are to a low-dimensional subspace).  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  11  
spectrum analysis  Plotting the spectrum of the covariance matrix XT X (its eigenvalues) shows how compressible X is using low-rank approximation (i.e., how close (cid:126)x1, . . . , (cid:126)xn are to a low-dimensional subspace).  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : top eigenvectors of XT X, Vk ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  11  
spectrum analysis  Exercise: Show that the eigenvalues of XT X are always positive. Hint: Use that λj = (cid:126)v T  j XT X(cid:126)vj .  12  
summary  • Many (most) datasets can be approximated via projection onto a  low-dimensional subspace.  • Find this subspace via a maximization problem:  max  orthonormal V  (cid:107)XV(cid:107)2 F .  • Greedy solution via eigendecomposition of XT X. • Columns of V are the top eigenvectors of XT X. • Error of best low-rank approximation is determined by the tail of  XT X’s eigenvalue spectrum.  13  
summary  • Many (most) datasets can be approximated via projection onto a  low-dimensional subspace.  • Find this subspace via a maximization problem:  max  orthonormal V  (cid:107)XV(cid:107)2 F .  • Greedy solution via eigendecomposition of XT X. • Columns of V are the top eigenvectors of XT X. • Error of best low-rank approximation is determined by the tail of  XT X’s eigenvalue spectrum.  • We’ll return to the problem how to quickly compute the top  eigenvectors of XT X.  13  
14  
