compsci 514: algorithms for data science  Andrew McGregor  Lecture 23  0  
summary  Last Class: • Multivariable calculus review and gradient computation. • Introduction to gradient descent. Motivation as a greedy algorithm.  1  
summary  Last Class: • Multivariable calculus review and gradient computation. • Introduction to gradient descent. Motivation as a greedy algorithm.  This Class: • Analysis of gradient descent for Lipschitz, convex functions. • Extension to projected gradient descent for constrained optimization.  1  
function minimization via gradient descent  Goal: Find (cid:126)θ ∈ Rd that (nearly) minimizes convex function f .  Gradient Descent Algorithm: • Choose some initialization (cid:126)θ(0). • For i = 1, . . . , t − 1  • (cid:126)θ(i) = (cid:126)θ(i−1) − η∇f ((cid:126)θ(i−1)) • Return ˆθ = arg min(cid:126)θ1,...(cid:126)θt  f ((cid:126)θi ).  Step size η is chosen ahead of time or adapted during the algorithm. For now assume η stays the same in each iteration.  2  
when does gradient descent work?  Gradient Descent Update in 1D: θi+1 = θi − ηf (cid:48)(θi ), i.e., increase θ if derivative is negative and decrease θ if derivative is positive.  3  
convexity  Deﬁnition – Convex Function: A function f : Rd → R is convex iﬀ, for any (cid:126)θ1, (cid:126)θ2 ∈ Rd and λ ∈ [0, 1]: (1 − λ) · f ((cid:126)θ1) + λ · f ((cid:126)θ2) ≥ f  (1 − λ) · (cid:126)θ1 + λ · (cid:126)θ2  (cid:16)  (cid:17)  4  
convexity  Corollary: A function f : R → R is convex iﬀ, for any θ1, θ2 ∈ R:  “slope between f (θ1) and f (θ2)” =  f (θ2) − f (θ1)  θ2 − θ1  ≥ f (cid:48)(θ1)  
convexity  Corollary: A function f : R → R is convex iﬀ, for any θ1, θ2 ∈ R:  “slope between f (θ1) and f (θ2)” =  More generally, a function f : Rd → R is convex if and only if, for  any (cid:126)θ1, (cid:126)θ2 ∈ Rd : f ((cid:126)θ2) − f ((cid:126)θ1) ≥ (cid:126)∇f ((cid:126)θ1)T(cid:16)(cid:126)θ2 − (cid:126)θ1  (cid:17)  f (θ2) − f (θ1)  θ2 − θ1  ≥ f (cid:48)(θ1)  5  
lipschitz functions  Gradient Descent Update: (cid:126)θi+1 = (cid:126)θi − η∇f ((cid:126)θi )  6  
lipschitz functions  Gradient Descent Update: (cid:126)θi+1 = (cid:126)θi − η∇f ((cid:126)θi )  For fast convergence, need to assume that the function is Lipschitz, i.e., size of gradient (cid:107)(cid:126)∇f ((cid:126)θ)(cid:107)2 is bounded. We’ll assume  ∀(cid:126)θ1, (cid:126)θ2 :  |f ((cid:126)θ1) − f ((cid:126)θ2)| ≤ G · (cid:107)(cid:126)θ1 − (cid:126)θ2(cid:107)2  6  
Gradient Descent analysis for convex, Lipschitz functions.  7  
gd analysis – convex functions  Assume that: • f is convex. • f is G Lipschitz, i.e., (cid:107)(cid:126)∇f ((cid:126)θ)(cid:107)2 ≤ G for all (cid:126)θ. • (cid:107)(cid:126)θ1 − (cid:126)θ∗(cid:107)2 ≤ R where (cid:126)θ1 is the initialization point.  Gradient Descent • Choose some initialization (cid:126)θ1 and set η = R √ • For i = 1, . . . , t − 1 • (cid:126)θi+1 = (cid:126)θi − η∇f ((cid:126)θi ) • Return ˆθ = arg min(cid:126)θ1,...(cid:126)θt  f ((cid:126)θi ).  G  .  t  8  
gd analysis proof  Theorem: For convex G -Lipschitz function f : R → R, GD run with t ≥ R 2G 2 , and starting point within R of θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f (θ∗) + .  √ iterations, η = R G  2  t  9  
gd analysis proof  Theorem: For convex G -Lipschitz function f : R → R, GD run with t ≥ R 2G 2 , and starting point within R of θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f (θ∗) + .  √ iterations, η = R G  2  t  • Substituting θi+1 = θi − ηf (cid:48)(θi ) and letting ai = θi − θ∗ gives:  i+1 = (θi+1 − θ∗)2 = (ai − ηf (cid:48)(θi ))2 = a2 a2  i − 2ηf (cid:48)(θi )ai + (ηf (cid:48)(θi ))2  9  
gd analysis proof  Theorem: For convex G -Lipschitz function f : R → R, GD run with t ≥ R 2G 2 , and starting point within R of θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f (θ∗) + .  √ iterations, η = R G  2  t  • Substituting θi+1 = θi − ηf (cid:48)(θi ) and letting ai = θi − θ∗ gives:  i+1 = (θi+1 − θ∗)2 = (ai − ηf (cid:48)(θi ))2 = a2 a2  i − 2ηf (cid:48)(θi )ai + (ηf (cid:48)(θi ))2  • Rearrange and use convexity to show: f (θi ) − f (θ∗) ≤ f (cid:48)(θi )ai = 1  2η  (cid:0)a2  i − a2  i+1  (cid:1) + η(f (cid:48)(θi ))2/2  9  
gd analysis proof  Theorem: For convex G -Lipschitz function f : R → R, GD run with t ≥ R 2G 2 , and starting point within R of θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f (θ∗) + .  √ iterations, η = R G  2  t  • Substituting θi+1 = θi − ηf (cid:48)(θi ) and letting ai = θi − θ∗ gives:  i+1 = (θi+1 − θ∗)2 = (ai − ηf (cid:48)(θi ))2 = a2 a2  i − 2ηf (cid:48)(θi )ai + (ηf (cid:48)(θi ))2  • Rearrange and use convexity to show: f (θi ) − f (θ∗) ≤ f (cid:48)(θi )ai = 1  (cid:1) + η(f (cid:48)(θi ))2/2 (cid:17) i=1 (f (θi ) − f (θ∗)) ≤(cid:16) 1 (cid:80)t • Summing over i and using the fact |f (cid:48)(θi )| ≤ G , i+1)  (cid:0)a2  2 ≤ a2  (cid:80)t  i − a2  i − a2  i=1(a2  2η  i+1  1 t  2tη  + ηG 2  1  2tη + ηG 2  2  9  
gd analysis proof  Theorem: For convex G -Lipschitz function f : R → R, GD run with t ≥ R 2G 2 , and starting point within R of θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f (θ∗) + .  √ iterations, η = R G  2  t  • Substituting θi+1 = θi − ηf (cid:48)(θi ) and letting ai = θi − θ∗ gives:  i+1 = (θi+1 − θ∗)2 = (ai − ηf (cid:48)(θi ))2 = a2 a2  i − 2ηf (cid:48)(θi )ai + (ηf (cid:48)(θi ))2  1  2tη + ηG 2  2  9  • Rearrange and use convexity to show: f (θi ) − f (θ∗) ≤ f (cid:48)(θi )ai = 1  (cid:1) + η(f (cid:48)(θi ))2/2 (cid:17) i=1 (f (θi ) − f (θ∗)) ≤(cid:16) 1 (cid:80)t • Summing over i and using the fact |f (cid:48)(θi )| ≤ G , i+1)  (cid:0)a2  2 ≤ a2  i − a2  i=1(a2  i+1  2η  1 t  (cid:80)t 1 ≤ R 2 and f (ˆθ) − f (θ∗) ≤ 1 f (ˆθ) ≤ f (θ∗) + R 2  2tη  t  + ηG 2  i − a2 (cid:80)t i=1 (f (θi ) − f (θ∗)) 2 ≤ f (θ∗) +   2tη + ηG 2  • Using a2  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−θ∗(cid:107)2  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2 2 .  10  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 Step 1.1: (cid:126)∇f ((cid:126)θi )T ((cid:126)θi − (cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2 2 .  + ηG 2 2 .  11  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 Step 1.1: (cid:126)∇f ((cid:126)θi )T ((cid:126)θi − (cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 via Convexity.  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2 2 .  + ηG 2  2 . Implies Step 1  11  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 Step 1.1: (cid:126)∇f ((cid:126)θi )T ((cid:126)θi − (cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 via Convexity. Proof of Step 1.1:  (cid:107)(cid:126)θi+1 − (cid:126)θ∗(cid:107)2  2 = (cid:107)(cid:126)θi − η (cid:126)∇f ((cid:126)θi ) − (cid:126)θ∗(cid:107)2  2  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2 2 .  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2  2 . Implies Step 1  using fact (cid:107)a + b(cid:107)2  = (cid:107)(cid:126)θi − (cid:126)θ∗(cid:107)2 2 = (cid:107)a(cid:107)2  2 + 2aT b + (cid:107)b(cid:107)2 2.  2 − 2η (cid:126)∇f ((cid:126)θi )T ((cid:126)θi − (cid:126)θ∗) + (cid:107)η (cid:126)∇f (θi )(cid:107)2  2  11  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 Step 1.1: (cid:126)∇f ((cid:126)θi )T ((cid:126)θi − (cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 via Convexity. Proof of Step 1.1:  (cid:107)(cid:126)θi+1 − (cid:126)θ∗(cid:107)2  2 = (cid:107)(cid:126)θi − η (cid:126)∇f ((cid:126)θi ) − (cid:126)θ∗(cid:107)2  2  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2 2 .  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2  2 . Implies Step 1  using fact (cid:107)a + b(cid:107)2  2 + 2aT b + (cid:107)b(cid:107)2 (cid:126)∇f ((cid:126)θi )T ((cid:126)θi − (cid:126)θ∗) ≤ (cid:107)(cid:126)θi − (cid:126)θ∗(cid:107)2  2 − 2η (cid:126)∇f ((cid:126)θi )T ((cid:126)θi − (cid:126)θ∗) + (cid:107)η (cid:126)∇f (θi )(cid:107)2 2. Since (cid:107)η (cid:126)∇f ((cid:126)θi )(cid:107)2 2 ≤ η2G 2, ηG 2  2 − (cid:107)(cid:126)θi+1 − (cid:126)θ∗(cid:107)2  2  2  +  = (cid:107)(cid:126)θi − (cid:126)θ∗(cid:107)2 2 = (cid:107)a(cid:107)2  2η  2  11  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying: f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2 2  12  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying: f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 2η·t + ηG 2 2 .  (cid:80)t i=1 f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ R 2  Step 2: 1 t  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2  2 =⇒  12  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying: f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 2η·t + ηG 2 2 .  (cid:80)t i=1 f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ R 2  Step 2: 1 t  2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2  + ηG 2  2 =⇒  t(cid:88)  i=1  Proof of Step 2:  f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ tηG 2 2  =  tηG 2  2  +  +  1 2η  1 2η  (cid:16)(cid:107)(cid:126)θi − (cid:126)θ∗(cid:107)2  2 − (cid:107)(cid:126)θi+1 − (cid:126)θ∗(cid:107)2  2  t−1(cid:88)  i=0  (cid:107)(cid:126)θ0 − (cid:126)θ∗(cid:107)2  2 ≤ tηG 2 2  +  R 2 2η  (cid:17)  12  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  13  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  (cid:80)t i=1 f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ R 2  2η·t + ηG 2  2  • Step 2: 1  t  13  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  (cid:80)t i=1 f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ R 2  2η·t + ηG 2  2 ≤ .  • Step 2: 1  t  13  
gd analysis proof  Theorem: For convex G -Lipschitz function f : Rd → R, GD run with t ≥ R 2G 2 , and starting point within radius R of (cid:126)θ∗, outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) + .  √ iterations, η = R G  2  t  • Step 2: 1 • Result follows since 1  t  t  (cid:80)t 2η·t + ηG 2 i=1 f ((cid:126)θi ) ≥ f (ˆθ).  (cid:80)t i=1 f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ R 2  2 ≤ .  13  
constrained convex optimization  Often want to perform convex optimization with convex constraints.  (cid:126)θ∗ = arg min (cid:126)θ∈S  f ((cid:126)θ),  where S is a convex set.  14  
constrained convex optimization  Often want to perform convex optimization with convex constraints.  (cid:126)θ∗ = arg min (cid:126)θ∈S  f ((cid:126)θ),  where S is a convex set.  Deﬁnition – Convex Set: A set S ⊆ Rd is convex if and only if, for any (cid:126)θ1, (cid:126)θ2 ∈ S and λ ∈ [0, 1]: (1 − λ)(cid:126)θ1 + λ · (cid:126)θ2 ∈ S  14  
constrained convex optimization  Often want to perform convex optimization with convex constraints.  (cid:126)θ∗ = arg min (cid:126)θ∈S  f ((cid:126)θ),  where S is a convex set.  Deﬁnition – Convex Set: A set S ⊆ Rd is convex if and only if, for any (cid:126)θ1, (cid:126)θ2 ∈ S and λ ∈ [0, 1]: (1 − λ)(cid:126)θ1 + λ · (cid:126)θ2 ∈ S  For any convex set let PS(·) denote the projection function onto S:  PS((cid:126)y ) = arg min  (cid:126)θ∈S  (cid:107)(cid:126)θ − (cid:126)y(cid:107)2  14  
constrained convex optimization  Often want to perform convex optimization with convex constraints.  (cid:126)θ∗ = arg min (cid:126)θ∈S  f ((cid:126)θ),  where S is a convex set.  Deﬁnition – Convex Set: A set S ⊆ Rd is convex if and only if, for any (cid:126)θ1, (cid:126)θ2 ∈ S and λ ∈ [0, 1]: (1 − λ)(cid:126)θ1 + λ · (cid:126)θ2 ∈ S  For any convex set let PS(·) denote the projection function onto S:  PS((cid:126)y ) = arg min  (cid:126)θ∈S  (cid:107)(cid:126)θ − (cid:126)y(cid:107)2  • For S = {(cid:126)θ ∈ Rd : (cid:107)(cid:126)θ(cid:107)2 ≤ 1} what is PS((cid:126)y )?  14  
constrained convex optimization  Often want to perform convex optimization with convex constraints.  (cid:126)θ∗ = arg min (cid:126)θ∈S  f ((cid:126)θ),  where S is a convex set.  Deﬁnition – Convex Set: A set S ⊆ Rd is convex if and only if, for any (cid:126)θ1, (cid:126)θ2 ∈ S and λ ∈ [0, 1]: (1 − λ)(cid:126)θ1 + λ · (cid:126)θ2 ∈ S  For any convex set let PS(·) denote the projection function onto S:  PS((cid:126)y ) = arg min  (cid:126)θ∈S  (cid:107)(cid:126)θ − (cid:126)y(cid:107)2  • For S = {(cid:126)θ ∈ Rd : (cid:107)(cid:126)θ(cid:107)2 ≤ 1} what is PS((cid:126)y )? • For S being a k dimensional subspace of Rd , what is PS((cid:126)y )?  14  
projected gradient descent  Projected Gradient Descent • Choose some initialization (cid:126)θ1 and set η = R √ • For i = 1, . . . , t − 1  G  i+1 = (cid:126)θi − η · (cid:126)∇f ((cid:126)θi )  • (cid:126)θ(out) • (cid:126)θi+1 = PS((cid:126)θ(out) i+1 ). • Return ˆθ = arg min(cid:126)θi  f ((cid:126)θi ).  .  t  15  
convex projections  Analysis of projected gradient descent is almost identifcal to gradient descent analysis!  16  
convex projections  Analysis of projected gradient descent is almost identifcal to gradient descent analysis! Just need to appeal to following geometric result:  Theorem – Projection to a convex set: For any convex set S ⊆ Rd , (cid:126)y ∈ Rd , and (cid:126)θ ∈ S,  (cid:107)PS((cid:126)y ) − (cid:126)θ(cid:107)2 ≤ (cid:107)(cid:126)y − (cid:126)θ(cid:107)2.  16  
projected gradient descent analysis  Theorem – Projected GD: For convex G -Lipschitz function f , and convex set S, Projected GD run with t ≥ R 2G 2 iterations, √ , and starting point within radius R of (cid:126)θ∗ = min(cid:126)θ∈S f ((cid:126)θ), η = R G outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) +   2  t  17  
projected gradient descent analysis  Theorem – Projected GD: For convex G -Lipschitz function f , and convex set S, Projected GD run with t ≥ R 2G 2 iterations, √ , and starting point within radius R of (cid:126)θ∗ = min(cid:126)θ∈S f ((cid:126)θ), η = R G outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) +   2  t  Recall: (cid:126)θ(out)  i+1 = (cid:126)θi − η · (cid:126)∇f ((cid:126)θi ) and (cid:126)θi+1 = PS((cid:126)θ(out) i+1 ).  17  
projected gradient descent analysis  Theorem – Projected GD: For convex G -Lipschitz function f , and convex set S, Projected GD run with t ≥ R 2G 2 iterations, √ , and starting point within radius R of (cid:126)θ∗ = min(cid:126)θ∈S f ((cid:126)θ), η = R G outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) +   2  t  Recall: (cid:126)θ(out) Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−θ∗(cid:107)2  i+1 = (cid:126)θi − η · (cid:126)∇f ((cid:126)θi ) and (cid:126)θi+1 = PS((cid:126)θ(out) i+1 ). i+1 −(cid:126)θ∗(cid:107)2  2−(cid:107)(cid:126)θ(out)  2  2η  + ηG 2 2 .  17  
projected gradient descent analysis  Theorem – Projected GD: For convex G -Lipschitz function f , and convex set S, Projected GD run with t ≥ R 2G 2 iterations, √ , and starting point within radius R of (cid:126)θ∗ = min(cid:126)θ∈S f ((cid:126)θ), η = R G outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) +   2  t  i+1 = (cid:126)θi − η · (cid:126)∇f ((cid:126)θi ) and (cid:126)θi+1 = PS((cid:126)θ(out) i+1 ). i+1 −(cid:126)θ∗(cid:107)2  Recall: (cid:126)θ(out) Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−θ∗(cid:107)2 Step 1.a: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2  2η 2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  2−(cid:107)(cid:126)θ(out)  2  + ηG 2 2 .  2  + ηG 2 2 .  17  
projected gradient descent analysis  Theorem – Projected GD: For convex G -Lipschitz function f , and convex set S, Projected GD run with t ≥ R 2G 2 iterations, √ , and starting point within radius R of (cid:126)θ∗ = min(cid:126)θ∈S f ((cid:126)θ), η = R G outputs ˆθ satisfying f (ˆθ) ≤ f ((cid:126)θ∗) +   2  t  i+1 = (cid:126)θi − η · (cid:126)∇f ((cid:126)θi ) and (cid:126)θi+1 = PS((cid:126)θ(out) i+1 ). i+1 −(cid:126)θ∗(cid:107)2  2−(cid:107)(cid:126)θ(out)  Recall: (cid:126)θ(out) Step 1: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−θ∗(cid:107)2 Step 1.a: For all i, f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ (cid:107)(cid:126)θi−(cid:126)θ∗(cid:107)2 2η·t + ηG 2  (cid:80)t i=1 f ((cid:126)θi ) − f ((cid:126)θ∗) ≤ R 2  Step 2: 1 t  2η 2−(cid:107)(cid:126)θi+1−(cid:126)θ∗(cid:107)2 2η  + ηG 2 2 . 2 =⇒ Theorem.  2  2  + ηG 2 2 .  17  
