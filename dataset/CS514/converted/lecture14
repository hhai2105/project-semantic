compsci 514: algorithms for data science  Andrew McGregor  Lecture 14  0  
summary  Next Few Classes: Low-rank approximation, the SVD, and principal component analysis (PCA). • Reduce d-dimensional data points to a smaller dimension m. • Like JL, compression is linear, i.e., by applying a matrix. • Chose matrix taking into account structure of dataset. • Can give better compression than random projection.  1  
summary  Next Few Classes: Low-rank approximation, the SVD, and principal component analysis (PCA). • Reduce d-dimensional data points to a smaller dimension m. • Like JL, compression is linear, i.e., by applying a matrix. • Chose matrix taking into account structure of dataset. • Can give better compression than random projection.  Will be using a fair amount of linear algebra: orthogonal basis, column/row span, eigenvectors, etc,  1  
embedding with assumptions  Assume that data points (cid:126)x1, . . . , (cid:126)xn lie in any k-dimensional subspace V of Rd .  2  
embedding with assumptions  Assume that data points (cid:126)x1, . . . , (cid:126)xn lie in any k-dimensional subspace V of Rd .  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)xi , (cid:126)xj :  (cid:107)VT (cid:126)xi − VT (cid:126)xj(cid:107)2 = (cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  2  
embedding with assumptions  Assume that data points (cid:126)x1, . . . , (cid:126)xn lie in any k-dimensional subspace V of Rd .  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)xi , (cid:126)xj :  (cid:107)VT (cid:126)xi − VT (cid:126)xj(cid:107)2 = (cid:107)(cid:126)xi − (cid:126)xj(cid:107)2.  • VT ∈ Rk×d is a linear embedding of (cid:126)x1, . . . , (cid:126)xn into k dimensions with  no distortion.  2  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  • (cid:107)(cid:126)y(cid:107)2  2 = (cid:126)y T (cid:126)y  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  • (cid:107)(cid:126)y(cid:107)2  2 = (cid:126)y T (cid:126)y = (V(cid:126)c)T (V(cid:126)c)  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  • (cid:107)(cid:126)y(cid:107)2  2 = (cid:126)y T (cid:126)y = (V(cid:126)c)T (V(cid:126)c) = (cid:126)c T VT V(cid:126)c  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  • (cid:107)(cid:126)y(cid:107)2 • (cid:107)VT (cid:126)y(cid:107)2  2 = (cid:126)y T (cid:126)y = (V(cid:126)c)T (V(cid:126)c) = (cid:126)c T VT V(cid:126)c 2 = (VT (cid:126)y )T (VT (cid:126)y ) = (cid:126)y T VVT (cid:126)y  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  • (cid:107)(cid:126)y(cid:107)2 • (cid:107)VT (cid:126)y(cid:107)2  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  2 = (cid:126)y T (cid:126)y = (V(cid:126)c)T (V(cid:126)c) = (cid:126)c T VT V(cid:126)c  2 = (VT (cid:126)y )T (VT (cid:126)y ) = (cid:126)y T VVT (cid:126)y = (cid:126)c T VT VVT V(cid:126)c  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  • (cid:107)(cid:126)y(cid:107)2 • (cid:107)VT (cid:126)y(cid:107)2 • But VT V = I  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  2 = (cid:126)y T (cid:126)y = (V(cid:126)c)T (V(cid:126)c) = (cid:126)c T VT V(cid:126)c  2 = (VT (cid:126)y )T (VT (cid:126)y ) = (cid:126)y T VVT (cid:126)y = (cid:126)c T VT VVT V(cid:126)c  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  • (cid:107)(cid:126)y(cid:107)2 • (cid:107)VT (cid:126)y(cid:107)2 • But VT V = I since  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  2 = (cid:126)y T (cid:126)y = (V(cid:126)c)T (V(cid:126)c) = (cid:126)c T VT V(cid:126)c  2 = (VT (cid:126)y )T (VT (cid:126)y ) = (cid:126)y T VVT (cid:126)y = (cid:126)c T VT VVT V(cid:126)c  1  0  [VT V]i,j = (cid:126)v T  i (cid:126)vj =  i = j i (cid:54)= j  3  
dot product transformation  Claim: Let (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns. For all (cid:126)y ∈ V:  (cid:107)VT y(cid:107)2 = (cid:107)y(cid:107)2.  Proof:  • If (cid:126)y =(cid:80)  • (cid:107)(cid:126)y(cid:107)2 • (cid:107)VT (cid:126)y(cid:107)2 • But VT V = I since  i ci (cid:126)vi then (cid:126)y = V(cid:126)c where (cid:126)c T = (c1, . . . , ck )  2 = (cid:126)y T (cid:126)y = (V(cid:126)c)T (V(cid:126)c) = (cid:126)c T VT V(cid:126)c  2 = (VT (cid:126)y )T (VT (cid:126)y ) = (cid:126)y T VVT (cid:126)y = (cid:126)c T VT VVT V(cid:126)c  1  0  [VT V]i,j = (cid:126)v T  i (cid:126)vj =  • So (cid:107)(cid:126)y(cid:107)2  2 = (cid:126)c T (cid:126)c = (cid:107)VT (cid:126)y(cid:107)2 2.  i = j i (cid:54)= j  3  
embedding with assumptions  Now assume that data points (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd .  4  
embedding with assumptions  Now assume that data points (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd .  4  
embedding with assumptions  Now assume that data points (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd .  Letting (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns, VT (cid:126)xi ∈ Rk is still a good embedding for xi ∈ Rd .  4  
embedding with assumptions  Now assume that data points (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd .  Letting (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns, VT (cid:126)xi ∈ Rk is still a good embedding for xi ∈ Rd . The key idea behind low-rank approximation and principal component analysis (PCA).  4  
embedding with assumptions  Now assume that data points (cid:126)x1, . . . , (cid:126)xn lie close to any k-dimensional subspace V of Rd .  Letting (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V and V ∈ Rd×k be the matrix with these vectors as its columns, VT (cid:126)xi ∈ Rk is still a good embedding for xi ∈ Rd . The key idea behind low-rank approximation and principal component analysis (PCA). • How do we ﬁnd V and V? • How good is the embedding?  4  
low-rank factorization  Claim: (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V ⇔ the data matrix X ∈ Rn×d has rank ≤ k.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  5  
low-rank factorization  Claim: (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V ⇔ the data matrix X ∈ Rn×d has rank ≤ k. • Letting (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V, can write any (cid:126)xi as:  (cid:126)xi = V(cid:126)ci = ci,1 · (cid:126)v1 + ci,2 · (cid:126)v2 + . . . + ci,k · (cid:126)vk .  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  5  
low-rank factorization  Claim: (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V ⇔ the data matrix X ∈ Rn×d has rank ≤ k. • Letting (cid:126)v1, . . . , (cid:126)vk be an orthonormal basis for V, can write any (cid:126)xi as:  (cid:126)xi = V(cid:126)ci = ci,1 · (cid:126)v1 + ci,2 · (cid:126)v2 + . . . + ci,k · (cid:126)vk . • So (cid:126)v1, . . . , (cid:126)vk span the rows of X and thus rank(X) ≤ k.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  5  
Claim: (cid:126)x1, . . . , (cid:126)xn ∈ Rd lie in a k-dimensional subspace V ⇔ the data matrix X ∈ Rn×d has rank ≤ k. • Every data point (cid:126)xi (row of X) can be written as  (cid:126)xi = V(cid:126)ci = ci,1 · (cid:126)v1 + . . . + ci,k · (cid:126)vk .  (cid:126)x1, . . . , (cid:126)xn: data points (in Rd ), V: k-dimensional subspace of Rd , (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  6  
Claim: (cid:126)x1, . . . , (cid:126)xn ∈ Rd lie in a k-dimensional subspace V ⇔ the data matrix X ∈ Rn×d has rank ≤ k. • Every data point (cid:126)xi (row of X) can be written as  (cid:126)xi = V(cid:126)ci = ci,1 · (cid:126)v1 + . . . + ci,k · (cid:126)vk .  (cid:126)x1, . . . , (cid:126)xn: data points (in Rd ), V: k-dimensional subspace of Rd , (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  6  
Claim: (cid:126)x1, . . . , (cid:126)xn ∈ Rd lie in a k-dimensional subspace V ⇔ the data matrix X ∈ Rn×d has rank ≤ k. • Every data point (cid:126)xi (row of X) can be written as  (cid:126)xi = V(cid:126)ci = ci,1 · (cid:126)v1 + . . . + ci,k · (cid:126)vk .  • X can be represented by (n + d) · k parameters vs. n · d.  (cid:126)x1, . . . , (cid:126)xn: data points (in Rd ), V: k-dimensional subspace of Rd , (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  6  
Claim: (cid:126)x1, . . . , (cid:126)xn ∈ Rd lie in a k-dimensional subspace V ⇔ the data matrix X ∈ Rn×d has rank ≤ k. • Every data point (cid:126)xi (row of X) can be written as  (cid:126)xi = V(cid:126)ci = ci,1 · (cid:126)v1 + . . . + ci,k · (cid:126)vk .  • X can be represented by (n + d) · k parameters vs. n · d. • The rows of X are spanned by k vectors: the columns of V =⇒ the  columns of X are spanned by k vectors: the columns of C.  (cid:126)x1, . . . , (cid:126)xn: data points (in Rd ), V: k-dimensional subspace of Rd , (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  6  
low-rank factorization  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace with orthonormal basis V ∈ Rd×k , the data matrix can be written as X = CVT .  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  7  
low-rank factorization  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace with orthonormal basis V ∈ Rd×k , the data matrix can be written as X = CVT .  Exercise: What is this coeﬃcient matrix C? Hint: Use that VT V = I.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  7  
low-rank factorization  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace with orthonormal basis V ∈ Rd×k , the data matrix can be written as X = CVT .  Exercise: What is this coeﬃcient matrix C? Hint: Use that VT V = I. • X = CVT =⇒ XV = CVT V  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  7  
low-rank factorization  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace with orthonormal basis V ∈ Rd×k , the data matrix can be written as X = CVT .  Exercise: What is this coeﬃcient matrix C? Hint: Use that VT V = I. • X = CVT =⇒ XV = CVT V =⇒ XV = C  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  7  
low-rank factorization  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace with orthonormal basis V ∈ Rd×k , the data matrix can be written as X = CVT .  Exercise: What is this coeﬃcient matrix C? Hint: Use that VT V = I. • X = CVT =⇒ XV = CVT V =⇒ XV = C  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  7  
projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = CVT .  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = XVVT .  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = XVVT .  • VVT is a projection matrix, which projects the rows of X (the data  points (cid:126)x1, . . . , (cid:126)xn) onto the subspace V.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = XVVT .  • VVT is a projection matrix, which projects the rows of X (the data  points (cid:126)x1, . . . , (cid:126)xn) onto the subspace V.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = XVVT .  • VVT is a projection matrix, which projects the rows of X (the data  points (cid:126)x1, . . . , (cid:126)xn) onto the subspace V.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
projection view  Claim: If (cid:126)x1, . . . , (cid:126)xn lie in a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be written as  X = XVVT .  • VVT is a projection matrix, which projects the rows of X (the data  points (cid:126)x1, . . . , (cid:126)xn) onto the subspace V.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  8  
low-rank approximation  Claim: If (cid:126)x1, . . . , (cid:126)xn lie close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as:  X ≈ XVVT  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
low-rank approximation  Claim: If (cid:126)x1, . . . , (cid:126)xn lie close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as:  X ≈ XVVT  Note: XVVT has rank k. It is a low-rank approximation of X.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
low-rank approximation  Claim: If (cid:126)x1, . . . , (cid:126)xn lie close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as:  X ≈ XVVT  Note: XVVT has rank k. It is a low-rank approximation of X.  (cid:88)  i,j  XVVT = arg min  B with rows in V  (cid:107)X − B(cid:107)2  F = arg min  B with rows in V  (Xi,j − Bi,j )2.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthonormal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  9  
low-rank approximation  So Far: If (cid:126)x1, . . . , (cid:126)xn lie close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as:  X ≈ XVVT .  This is the closest approximation to X with rows in V (i.e., in the column span of V).  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
low-rank approximation  So Far: If (cid:126)x1, . . . , (cid:126)xn lie close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as:  X ≈ XVVT .  This is the closest approximation to X with rows in V (i.e., in the column span of V). • Letting (XVVT )i , (XVVT )j be the i th and j th projected data points,  (cid:107)(XVVT )i−(XVVT )j(cid:107)2 = (cid:107)[(XV)i−(XV)j ]VT(cid:107)2 = (cid:107)[(XV)i−(XV)j ](cid:107)2.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
low-rank approximation  So Far: If (cid:126)x1, . . . , (cid:126)xn lie close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as:  X ≈ XVVT .  This is the closest approximation to X with rows in V (i.e., in the column span of V). • Letting (XVVT )i , (XVVT )j be the i th and j th projected data points,  (cid:107)(XVVT )i−(XVVT )j(cid:107)2 = (cid:107)[(XV)i−(XV)j ]VT(cid:107)2 = (cid:107)[(XV)i−(XV)j ](cid:107)2.  • Can use XV ∈ Rn×k as a compressed approximate data set.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
low-rank approximation  So Far: If (cid:126)x1, . . . , (cid:126)xn lie close to a k-dimensional subspace V with orthonormal basis V ∈ Rd×k , the data matrix can be approximated as:  X ≈ XVVT .  This is the closest approximation to X with rows in V (i.e., in the column span of V). • Letting (XVVT )i , (XVVT )j be the i th and j th projected data points,  (cid:107)(XVVT )i−(XVVT )j(cid:107)2 = (cid:107)[(XV)i−(XV)j ]VT(cid:107)2 = (cid:107)[(XV)i−(XV)j ](cid:107)2.  • Can use XV ∈ Rn×k as a compressed approximate data set. Key question is how to ﬁnd the subspace V and correspondingly V.  (cid:126)x1, . . . , (cid:126)xn ∈ Rd : data points, X ∈ Rn×d : data matrix, (cid:126)v1, . . . , (cid:126)vk ∈ Rd : orthogonal basis for subspace V. V ∈ Rd×k : matrix with columns (cid:126)v1, . . . , (cid:126)vk .  10  
