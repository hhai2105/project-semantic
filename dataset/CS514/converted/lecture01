compsci 514: algorithms for data science  Andrew McGregor  Lecture 1  0  
motivation for this class  1  
motivation for this class  People are increasingly interested in analyzing and learning from massive datasets.  1  
motivation for this class  People are increasingly interested in analyzing and learning from massive datasets. • Twitter receives 6,000 tweets per second, 500 million/day. Google receives 60,000 searches per second, 5.6 billion/day. • How do they process them to target advertisements? To predict  trends? To improve their products?  1  
motivation for this class  People are increasingly interested in analyzing and learning from massive datasets. • Twitter receives 6,000 tweets per second, 500 million/day. Google receives 60,000 searches per second, 5.6 billion/day. • How do they process them to target advertisements? To predict  trends? To improve their products?  • The Large Synoptic Survey Telescope will take high deﬁnition photographs of the sky, producing 15 terabytes of data/night. • How do they denoise and compress the images? How do they detect anomalies such as changing brightness or position of objects to alert researchers?  1  
a new paradigm for algorithm design  • Traditionally, algorithm design focuses on fast computation when data is stored in an eﬃciently accessible centralized manner (e.g., in RAM on a single machine).  2  
a new paradigm for algorithm design  • Traditionally, algorithm design focuses on fast computation when data is stored in an eﬃciently accessible centralized manner (e.g., in RAM on a single machine).  • Massive data sets require storage in a distributed manner or  processing in a continuous stream.  2  
a new paradigm for algorithm design  • Traditionally, algorithm design focuses on fast computation when data is stored in an eﬃciently accessible centralized manner (e.g., in RAM on a single machine).  • Massive data sets require storage in a distributed manner or  processing in a continuous stream.  • Even ‘simple’ problems become very diﬃcult in this setting.  2  
a new paradigm for algorithm design  For example:  3  
a new paradigm for algorithm design  For example: • How can Twitter rapidly detect if an incoming Tweet is an exact duplicate of another Tweet made in the last year? Given that no machine can store all Tweets made in a year.  3  
a new paradigm for algorithm design  For example: • How can Twitter rapidly detect if an incoming Tweet is an exact duplicate of another Tweet made in the last year? Given that no machine can store all Tweets made in a year.  • How can Google estimate the number of unique search queries that are made in a given week? Given that no machine can store the full list of queries.  3  
a new paradigm for algorithm design  For example: • How can Twitter rapidly detect if an incoming Tweet is an exact duplicate of another Tweet made in the last year? Given that no machine can store all Tweets made in a year.  • How can Google estimate the number of unique search queries that are made in a given week? Given that no machine can store the full list of queries.  • When you use Shazam to identify a song from a recording, how  does it provide an answer in < 10 seconds, without scanning over all ∼ 8 million audio ﬁles in its database.  3  
motivation for this class  A Second Motivation: Data Science is highly interdisciplinary.  4  
motivation for this class  A Second Motivation: Data Science is highly interdisciplinary.  4  
motivation for this class  A Second Motivation: Data Science is highly interdisciplinary.  • Many techniques that aren’t covered in the traditional CS  algorithms curriculum.  • Emphasis on building comfort with mathematical tools that  underly data science and machine learning.  4  
what we’ll cover  5  
what we’ll cover  Section 1: Randomized Methods & Sketching  5  
what we’ll cover  Section 1: Randomized Methods & Sketching  How can we eﬃciently compress large data sets in a way that lets  us answer important algorithmic questions rapidly?  5  
what we’ll cover  Section 1: Randomized Methods & Sketching  How can we eﬃciently compress large data sets in a way that lets  us answer important algorithmic questions rapidly?  • Probability tools and concentration inequalities. • Randomized hashing for eﬃcient lookup, load balancing, and  estimation. Bloom ﬁlters.  • Locality sensitive hashing and nearest neighbor search. • Streaming algorithms: identifying frequent items in a data stream,  counting distinct items, etc.  • Random compression of high-dimensional vectors: the  Johnson-Lindenstrauss lemma, applications, and connections to the weirdness of high-dimensional geometry.  5  
what we’ll cover  6  
what we’ll cover  Section 2: Spectral Methods  6  
what we’ll cover  Section 2: Spectral Methods  How do we identify the most important features of a dataset using  linear algebraic techniques?  6  
what we’ll cover  Section 2: Spectral Methods  How do we identify the most important features of a dataset using  linear algebraic techniques?  • Principal component analysis, low-rank approximation, dimensionality  reduction.  • Singular value decomposition (SVD) and its applications to PCA,  low-rank approximation, LSI, MDS, . . .  • Spectral graph theory. Spectral clustering, community detection,  network visualization.  • Computing the SVD on large datasets via iterative methods.  6  
what we’ll cover  7  
what we’ll cover  Section 3: Optimization  7  
what we’ll cover  Section 3: Optimization  Fundamental continuous optimization approaches that drive  methods in machine learning and statistics.  7  
what we’ll cover  Section 3: Optimization  Fundamental continuous optimization approaches that drive  methods in machine learning and statistics.  • Gradient descent. Analysis for convex functions. • Stochastic and online gradient descent. • Focus on convergence analysis.  7  
what we’ll cover  Section 3: Optimization  Fundamental continuous optimization approaches that drive  methods in machine learning and statistics.  • Gradient descent. Analysis for convex functions. • Stochastic and online gradient descent. • Focus on convergence analysis.  A small taste of what you can ﬁnd in COMPSCI 590OP or 690OP.  7  
important topics we won’t cover  8  
important topics we won’t cover  • Systems/Software Tools.  8  
important topics we won’t cover  • Systems/Software Tools.  • COMPSCI 532: Systems for Data Science  8  
important topics we won’t cover  • Systems/Software Tools.  • COMPSCI 532: Systems for Data Science  • Machine Learning/Data Analysis Methods and Models. • E.g., regression methods, kernel methods, random forests, SVM,  deep neural networks.  8  
important topics we won’t cover  • Systems/Software Tools.  • COMPSCI 532: Systems for Data Science  • Machine Learning/Data Analysis Methods and Models. • E.g., regression methods, kernel methods, random forests, SVM, • COMPSCI 589/689: Machine Learning  deep neural networks.  8  
style of the course  This is a theory course.  9  
style of the course  This is a theory course. • Build general mathematical tools and algorithmic strategies that can  be applied to a wide range of problems.  9  
style of the course  This is a theory course. • Build general mathematical tools and algorithmic strategies that can  be applied to a wide range of problems.  • Assignments will emphasize algorithm design, correctness proofs, and  asymptotic analysis (minimal required coding).  9  
style of the course  This is a theory course. • Build general mathematical tools and algorithmic strategies that can  be applied to a wide range of problems.  • Assignments will emphasize algorithm design, correctness proofs, and  asymptotic analysis (minimal required coding).  • The homework is designed to make you think beyond what is taught in class. You will get stuck, and not see the solutions right away. This is the best (only?) way to build mathematical and algorithm design skills.  9  
style of the course  This is a theory course. • Build general mathematical tools and algorithmic strategies that can  be applied to a wide range of problems.  • Assignments will emphasize algorithm design, correctness proofs, and  asymptotic analysis (minimal required coding).  • The homework is designed to make you think beyond what is taught in class. You will get stuck, and not see the solutions right away. This is the best (only?) way to build mathematical and algorithm design skills.  • A strong algorithms and mathematical background (particularly in  linear algebra and probability) are required.  • UMass prereqs: COMPSCI 240 and COMPSCI 311.  9  
style of the course  This is a theory course. • Build general mathematical tools and algorithmic strategies that can  be applied to a wide range of problems.  • Assignments will emphasize algorithm design, correctness proofs, and  asymptotic analysis (minimal required coding).  • The homework is designed to make you think beyond what is taught in class. You will get stuck, and not see the solutions right away. This is the best (only?) way to build mathematical and algorithm design skills.  • A strong algorithms and mathematical background (particularly in  linear algebra and probability) are required.  • UMass prereqs: COMPSCI 240 and COMPSCI 311. For example: Bayes’ rule in conditional probability. What it means for a vector x to be an eigenvector of a matrix A, projection, greedy algorithms, divide-and-conquer algorithms.  9  
course logistics  See course webpage for lecture slides and related readings:  https://people.cs.umass.edu/~mcgregor/CS514S22/  See Moodle page for this link if you lose it.  10  
personnel  Professor: Andrew McGregor • Email: mcgregor@cs.umass.edu • Oﬃce Hour: 9-10am Wednesday.  TAs: • Shiv Shankar (sshankar@umass.edu). 3-4pm Monday. • Weronika Nguyen (thuytrangngu@umass.edu). 11am-12pm Friday.  See Moodle page for Zoom links.  11  
piazza and participation  We will use Piazza for class discussion and questions. • See Moodle for link to sign up.  12  
piazza and participation  We will use Piazza for class discussion and questions. • See Moodle for link to sign up.  You’re helping yourself and others if you: • Ask good clarifying questions and answering questions during  lectures.  • Answer other students’ or instructor questions on Piazza. • Post helpful/interesting links on Piazza, e.g., resources covering  class material, research articles related to class topics.  12  
textbooks and materials  We will use material from two textbooks (links to free online versions on the course webpage): Foundations of Data Science and Mining of Massive Datasets, but will follow neither closely. • I will sometimes post optional readings a few days prior to each  class.  • Draft lecture notes will be posted before each class and  potentially updated afterwards if necessary.  13  
homework  We will have 4 problem sets, which you may complete in groups of up to 3 students.  14  
homework  We will have 4 problem sets, which you may complete in groups of up to 3 students. • We strongly encourage working in groups, as it will make  completing the problem sets much easier/more educational.  • You may not discuss homework with people outside your group (except the instructor and TAs) until the solutions are released. • Use are not allowed to consult previous solutions from the class  or search for solutions online.  • Work together on each question rather than dividing the  questions between group members.  • See Piazza for a thread to help you organize groups.  14  
homework  We will have 4 problem sets, which you may complete in groups of up to 3 students. • We strongly encourage working in groups, as it will make  completing the problem sets much easier/more educational.  • You may not discuss homework with people outside your group (except the instructor and TAs) until the solutions are released. • Use are not allowed to consult previous solutions from the class  or search for solutions online.  • Work together on each question rather than dividing the  questions between group members.  • See Piazza for a thread to help you organize groups.  Problem set submissions will be via Gradescope. • See Moodle for a link to join.  14  
weekly quizzes  Will release a Moodle quiz each Friday. It’s due Monday at 8pm.  15  
weekly quizzes  Will release a Moodle quiz each Friday. It’s due Monday at 8pm. • Designed as a check-in that you are following the material, and  to help me make adjustments as needed.  • Should take under an hour per week, open notes. • Will sometimes include free response check-in questions to get your feedback on how the course is going, what material from the past week you ﬁnd most confusing, interesting, etc.  15  
grading  Grade Breakdown: • Four Problem Sets: 30%. • Weekly Quizzes: 15%. • Midterm: 25%. (Likely to be week before Spring Break.) • Final: 25%. (During Final’s Week.) • Piazza Participation: 5%.  16  
grading  Grade Breakdown: • Four Problem Sets: 30%. • Weekly Quizzes: 15%. • Midterm: 25%. (Likely to be week before Spring Break.) • Final: 25%. (During Final’s Week.) • Piazza Participation: 5%.  Academic Honesty: • A ﬁrst violation cheating on a homework, quiz, or other  assignment will result in a 0 on that assignment.  • A second violation, or cheating on an exam will result in failing  the class.  16  
disability services and accommodations  UMass Amherst is committed to making reasonable, eﬀective, and appropriate accommodations to meet the needs to students with disabilities. • If you have a documented disability on ﬁle with Disability  Services, you may be eligible for reasonable accommodations in this course.  • If your disability requires an accommodation, please notify me by  Friday 2/4 so that we can make arrangements.  17  
disability services and accommodations  UMass Amherst is committed to making reasonable, eﬀective, and appropriate accommodations to meet the needs to students with disabilities. • If you have a documented disability on ﬁle with Disability  Services, you may be eligible for reasonable accommodations in this course.  • If your disability requires an accommodation, please notify me by  Friday 2/4 so that we can make arrangements.  I understand that people have diﬀerent learning needs, home situations, etc. If something isn’t working for you in the class, please reach out and let’s try to work it out.  17  
Questions?  18  
Section 1: Randomized Methods & Sketching  19  
some probability review  20  
some probability review  Consider a random X variable taking values in some ﬁnite set S ⊂ R. E.g., for a random dice roll, S = {1, 2, 3, 4, 5, 6}.  20  
some probability review  Consider a random X variable taking values in some ﬁnite set S ⊂ R. E.g., for a random dice roll, S = {1, 2, 3, 4, 5, 6}. • Expectation:  E[X] =(cid:80)  s∈S Pr(X = s) · s.  20  
some probability review  Consider a random X variable taking values in some ﬁnite set S ⊂ R. E.g., for a random dice roll, S = {1, 2, 3, 4, 5, 6}. • Expectation: • Variance:  Var[X] = E[(X − E[X])2].  E[X] =(cid:80)  s∈S Pr(X = s) · s.  20  
some probability review  Consider a random X variable taking values in some ﬁnite set S ⊂ R. E.g., for a random dice roll, S = {1, 2, 3, 4, 5, 6}. • Expectation: • Variance:  Var[X] = E[(X − E[X])2].  E[X] =(cid:80)  s∈S Pr(X = s) · s.  Exercise: Show that for any scalar α, E[α · X] = α · E[X] and Var[α · X] = α2 · Var[X].  20  
independence  Consider two random events A and B.  A ∩ B: event that both events A and B happen.  21  
independence  Consider two random events A and B. • Conditional Probability:  Pr(A ∩ B)  Pr(B)  Pr(A|B) =  .  A ∩ B: event that both events A and B happen.  21  
independence  Consider two random events A and B. • Conditional Probability:  Pr(A ∩ B)  Pr(B)  Pr(A|B) =  .  • Independence: A and B are independent if:  Pr(A|B) = Pr(A).  A ∩ B: event that both events A and B happen.  21  
independence  Consider two random events A and B. • Conditional Probability:  Pr(A|B) =  .  Pr(A ∩ B)  Pr(B)  • Independence: A and B are independent if:  Pr(A|B) = Pr(A).  Using the deﬁnition of conditional probability, independence means:  Pr(A ∩ B)  Pr(B)  = Pr(A) =⇒ Pr(A ∩ B) = Pr(A) · Pr(B).  A ∩ B: event that both events A and B happen.  21  
independence  For Example: What is the probability that for two independent dice rolls the ﬁrst is a 6 and the second is odd?  22  
independence  For Example: What is the probability that for two independent dice rolls the ﬁrst is a 6 and the second is odd?  Pr(D1 = 6 ∩ D2 ∈ {1, 3, 5}) = Pr(D1 = 6) · Pr(D2 ∈ {1, 3, 5})  22  
independence  For Example: What is the probability that for two independent dice rolls the ﬁrst is a 6 and the second is odd?  Pr(D1 = 6 ∩ D2 ∈ {1, 3, 5}) = Pr(D1 = 6) · Pr(D2 ∈ {1, 3, 5})  Independent Random Variables: Two random variables X, Y are independent if for all s, t, X = s and Y = t are independent events. In other words:  Pr(X = s ∩ Y = t) = Pr(X = s) · Pr(Y = t).  22  
linearity of expectation and variance  When are the expectation and variance linear?  I.e., under what conditions on X and Y do we have:  E[X + Y] = E[X] + E[Y]  and  Var[X + Y] = Var[X] + Var[Y].  X, Y: any two random variables.  23  
linearity of expectation  E[X + Y] = E[X] + E[Y]  24  
linearity of expectation  E[X + Y] = E[X] + E[Y] for any random variables X and Y.  24  
linearity of expectation  E[X + Y] = E[X] + E[Y] for any random variables X and Y.  Proof:  24  
linearity of expectation  Proof: E[X + Y] =  s∈S  t∈T  E[X + Y] = E[X] + E[Y] for any random variables X and Y.  (cid:88)  (cid:88)  Pr(X = s ∩ Y = t) · (s + t)  24  
linearity of expectation  E[X + Y] = E[X] + E[Y] for any random variables X and Y.  Proof: E[X + Y] =  =  (cid:88) (cid:88)  s∈S  (cid:88) (cid:88)  t∈T  s∈S  t∈T  Pr(X = s ∩ Y = t) · (s + t)  Pr(X = s ∩ Y = t)s +  (cid:88)  (cid:88)  s∈S  t∈T  Pr(X = s ∩ Y = t)t  24  
linearity of expectation  E[X + Y] = E[X] + E[Y] for any random variables X and Y.  Proof: E[X + Y] =  =  =  (cid:88) (cid:88) (cid:88)  s∈S  s∈S  s∈S  t∈T  (cid:88) (cid:88) (cid:88)  t∈T s  t∈T  Pr(X = s ∩ Y = t) · (s + t)  Pr(X = s ∩ Y = t)s +  Pr(X = s ∩ Y = t)t  (cid:88) (cid:88)  s∈S  (cid:88) (cid:88)  t∈T t  t∈T  s∈S  Pr(X = s ∩ Y = t) +  Pr(X = s ∩ Y = t)  24  
linearity of expectation  E[X + Y] = E[X] + E[Y] for any random variables X and Y.  Proof: E[X + Y] =  =  =  (cid:88) (cid:88) (cid:88)  s∈S  s∈S  s∈S  t∈T  (cid:88) (cid:88) (cid:88)  t∈T s  t∈T  Pr(X = s ∩ Y = t) · (s + t)  Pr(X = s ∩ Y = t)s +  Pr(X = s ∩ Y = t)t  (cid:88) (cid:88)  s∈S  (cid:88) (cid:88)  t∈T t  t∈T  s∈S  Pr(X = s ∩ Y = t) +  Pr(X = s ∩ Y = t)  24  
linearity of expectation  E[X + Y] = E[X] + E[Y] for any random variables X and Y.  Proof: E[X + Y] =  =  =  =  t∈T  (cid:88) (cid:88) (cid:88)  t∈T s  s∈S  (cid:88) (cid:88) (cid:88) (cid:88)  s∈S  s∈S  s∈S  Pr(X = s ∩ Y = t) · (s + t)  Pr(X = s ∩ Y = t)s +  Pr(X = s ∩ Y = t)t  (cid:88) (cid:88)  s∈S  (cid:88) (cid:88)  t∈T t  s∈S  Pr(X = s ∩ Y = t) +  t∈T s Pr(X = s) +  t∈T t Pr(Y = t)  (cid:88)  t∈T  Pr(X = s ∩ Y = t)  (law of total probability)  24  
linearity of expectation  E[X + Y] = E[X] + E[Y] for any random variables X and Y.  Proof: E[X + Y] =  =  =  =  t∈T  (cid:88) (cid:88) (cid:88)  t∈T s  s∈S  (cid:88) (cid:88) (cid:88) (cid:88)  s∈S  s∈S  s∈S  Pr(X = s ∩ Y = t) · (s + t)  Pr(X = s ∩ Y = t)s +  Pr(X = s ∩ Y = t)t  (cid:88) (cid:88)  s∈S  (cid:88) (cid:88)  t∈T t  s∈S  Pr(X = s ∩ Y = t) +  t∈T s Pr(X = s) +  t∈T t Pr(Y = t)  (cid:88)  t∈T  Pr(X = s ∩ Y = t)  = E[X] + E[Y].  (law of total probability)  24  
linearity of variance  Var[X + Y] = Var[X] + Var[Y]  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent.  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation)  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give:  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  (linearity of expectation)  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  = E[X2] + 2E[XY] + E[Y2] − E[X]2 − 2E[X] · E[Y] − E[Y]2  (linearity of expectation)  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  = E[X2] + 2E[XY] + E[Y2] − E[X]2 − 2E[X] · E[Y] − E[Y]2  (linearity of expectation)  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  (linearity of expectation)  = E[X2] + 2E[XY] + E[Y2] − E[X]2 − 2E[X] · E[Y] − E[Y]2 = E[X2] + E[Y2] − E[X]2 − E[Y]2  25  
linearity of variance  Var[X + Y] = Var[X] + Var[Y] when X and Y are independent. Exercise 1: Var[X] = E[X2] − E[X]2 (via linearity of expectation) Exercise 2: E[XY] = E[X] · E[Y] when X, Y are independent.  Together give: Var[X + Y] = E[(X + Y)2] − E[X + Y]2  = E[X2] + 2E[XY] + E[Y2] − (E[X] + E[Y])2  (linearity of expectation)  = E[X2] + 2E[XY] + E[Y2] − E[X]2 − 2E[X] · E[Y] − E[Y]2 = E[X2] + E[Y2] − E[X]2 − E[Y]2 = Var[X] + Var[Y].  25  
